---
title: "Dictionary_2"
output: html_document
date: "2025-09-15"
---


This notebook builds a reproducible, restartable pipeline for mining, scoring, and context-aware expansion of acronyms in subcontract descriptions. It is RDS-only, restartable at any stage, and tuned to run on a Mac M3 (\~24 GB RAM). All artifacts are written under Data/processed (auto-created). The input is a pre-loaded subcontracts data frame; by default we trim to the first 200k rows for faster iteration (remove that line for a full run). The canonical description column is auto-detected, preferring expanded\_description, then subaward\_description.

The pipeline runs in numbered stages and writes a small set of versionable RDS files at each step so you can delete from any stage and re-run cleanly.

Stage 1 normalizes text and computes flags. We standardize text (Unicode NFKC), remove control characters, collapse whitespace, and apply an ASCII-clean “mojibake fix” that also strips common CLIN/LINE markup variants. We compute lightweight flags: flag\_serial (serial/part-code–like), flag\_multilin (residual LINE/CLIN markup), flag\_junky (original had >5% non-ASCII), and flag\_short (very short lines). 

Stage 2 loads an external acronym dictionary (DAU). Data/acronymdict.RDS may be a tidy two-column (acr, exp) or a free-text single column with lines like “ACR expansion”. We split on semicolons, uppercase and trim, dedupe, and write a clean table.

Stage 3 mines high-confidence (acronym, expansion) pairs from the text. We handle “long form (ACR)”, “ACR (long form)”, “ACR: long form”, and “long form: ACR”. Precision filters use Stage-1 flags and heuristics: exclude catalog markers (NSN, CLIN, P/N, etc.), block pure numbers or shouty tokens unless allow-listed, and never treat function words as acronyms. We verify by comparing the acronym to initials from a normalized long form (connectors removed), accepting exact matches and a small prefix slack to allow trailing words.

Stage 4 aggregates counts and features. We write dict\_all\_raw\.rds with frequencies and feat\_tbl.rds with distinct-document coverage and definition-pattern hits per (acr, exp).

Stage 5 canonicalizes expansions with Jaro–Winkler. Within each acronym, we bucket expansions by short prefix and token count, cluster by similarity, and promote a canonical representative; exp\_map.rds records raw-to-canonical collapses. JW\_THRESH controls aggressiveness (about 0.95–0.97 recommended).

Stage 6 scores candidates and computes a global prior. We form the union of mined expansions and DAU entries, join document/definition features, and compute a transparent raw composite (definition hits, raw frequency, document coverage, DAU authority). We keep a per-acronym normalized score for ranking within an acronym. Separately we compute a global prior in \[0,1] with good spread using a winsorized min–max blend and a robust logistic transform (median/MAD). Singleton acronyms get a within-acronym score of 1.0 so they are not penalized. The final prior is a weighted blend of within-acronym and global components. Top-K rows per acronym are kept in acr\_candidates.rds, and a convenience lexicon is written to acr\_lexicon.rds.

Stage 7 freezes word embeddings. We train wordvector::textmodel\_word2vec once on Quanteda tokens and cache the dense matrix (w2v\_contracts\_matrix.rds) and the vocabulary. Later stages reuse this frozen semantic space. Control threads via options(wordvector\_threads=…).

Stage 8 performs context-aware expansion using DAU ∪ mined detection. First, we detect candidate acronyms directly from text using a deduped acronym universe composed of all acronyms seen in acr\_candidates.rds plus all DAU acronyms. We look for 2–5-character uppercase-ish tokens (keeping “&” and “.” so R\&D and U.S. survive), splitting on “/” and “-” but not on “&”. A curated keep list (e.g., DoD, service branches, F-35, etc.) is never expanded inline. Next, for each acronym occurrence we score candidates with a convex combination of local cosine similarity between the context window and the candidate embedding and the Stage-6 prior; alpha controls the local vs prior blend (higher alpha means more local). For windows with ≤2 tokens we lean slightly more on the prior. A small, targeted heuristic helps R\&D when paired with generic heads like “SUPPORT”. If the best score exceeds the gate tau, we rewrite in place as “Expansion (ACR)”. We never expand on serial-like lines, and we always use the frozen embeddings from Stage 7.

Tune mode runs on a stratified sample and writes expanded\_sample\_metrics.rds (replacements per 1k tokens, share of lines with replacements, median/p90 score, acronyms per 1k tokens, share of lines with ≥1 acronym, and replacements per 1k given an acronym was found) and expanded\_sample\_preview\.rds (per-row before/after, counts, top score, and reason). Full mode runs over all rows and writes subcontracts\_expanded.rds. Both modes parallelize with a Windows-safe fallback. A small helper (“Pick-a-tau”) plots the operating curve of scores and suggests tau for a target replacement rate or share of lines.

For operations and reproducibility, a simple logger appends to Data/processed/build\_log.txt; print\_stage\_sizes() shows artifact sizes. The control helpers stage\_status(), reset\_from(), and run\_from() let you delete outputs from any stage onward and re-run to a target, optionally invoking Stage 8 in tune or full mode. Convenience wrappers run\_build\_once(), run\_expand\_tune(), and run\_expand\_full() cover common flows. All RDS objects carry a small meta attribute (timestamp, input hash, parameters) for auditability.

Practical knobs: tighten canonicalization via JW\_THRESH in Stage 5. If junk expansions leak, raise tau, reduce topK\_per\_acr, or increase alpha to lean more on local context. If shouty all-caps spam appears as acronyms, extend STOP\_ACR or the catalog marker list, or increase the initials strictness in Stage 3. If memory gets tight for tokenization or embeddings, lower min\_count, reduce the working sample, or reduce workers. Everything is disk-resumable.




```{r}

  library(digest)
  library(dplyr)
  library(conflicted)
  library(readr)
  library(here)
  library(stringr)
  library(stringi)
  library(purrr)
  library(tibble)
  library(tidyr)
  library(stringdist)
  library(quanteda)
  library(quanteda.textmodels)
  library(parallel)
  library(wordvector)
  library(fs)

conflicts_prefer(dplyr::filter)


proc_dir <- here("Data", "processed")
if (!dir_exists(proc_dir)) dir_create(proc_dir, recurse = TRUE)



```

```{r}

`%||%` <- function(x, y) if (is.null(x)) y else x

.safe_apply <- function(X, FUN, workers = 6) {
  workers <- as.integer(max(1L, workers))
  if (.Platform$OS.type == "windows" || workers == 1L) {
    return(lapply(X, FUN))
  }
  ok <- TRUE
  out <- tryCatch(parallel::mclapply(X, FUN, mc.cores = workers),
                  error = function(e) { ok <<- FALSE; e })
  if (!ok || inherits(out, "error") || is.null(out)) {
    out <- lapply(X, FUN)
  }
  out
}


.save_rds_meta <- function(obj, path, meta) { attr(obj, "meta") <- meta; write_rds(obj, path); invisible(obj) }
.hash_input <- function(x, n = 1000) {
  if (is.null(x)) return(digest("NULL"))
  if (is.data.frame(x)) {
    nr <- nrow(x); if (is.null(nr) || nr == 0) return(digest("EMPTY"))
    ix <- seq(1, nr, length.out = min(n, nr)) |> round() |> unique()
    return(digest(list(names(x), x[ix, , drop = FALSE])))
  } else if (is.character(x)) {
    return(digest(paste0(head(x, n), collapse = "\n")))
  } else {
    return(digest(x))
  }
}


#### Stage 1 

stage1_clean_flags <- function(subcontracts,
                               out_path = file.path(proc_dir, "subcontracts_with_flags.rds")) {
  # ------- helpers (vectorized) -------
  clean1 <- function(x) {
    x <- stringi::stri_trans_nfkc(x)
    x <- gsub("[\\p{Cc}\\p{Cf}]+", " ", x, perl = TRUE)
    x <- gsub("\\s+", " ", x, perl = TRUE)
    stringr::str_squish(x)
  }

  # ASCII-clean: drop non-ASCII (do NOT convert), remove CLIN/LINE markup
  fix_mojibake <- function(x) {
    x0 <- x
    x  <- gsub("[\\p{Cc}\\p{Cf}]+", " ", x, perl = TRUE)
    x  <- gsub("[^\\p{ASCII}]", " ", x, perl = TRUE)


    x <- gsub("(?<![A-Za-z])LINE(?:\\s+ITEM)?\\s*[:;\\-]?\\s*\\d+[A-Z0-9-]*\\b", " ", x, perl = TRUE)
    x <- gsub("(?<![A-Za-z])CLIN\\s*[:;\\-]?\\s*\\d+[A-Z0-9-]*\\b",              " ", x, perl = TRUE)
    x <- gsub("(?<![A-Za-z])LINE(?:\\s+ITEM)?\\s*[:;\\-]\\s*",                   " ", x, perl = TRUE)
    x <- gsub("(?<![A-Za-z])CLIN\\s*[:;\\-]\\s*",                               " ", x, perl = TRUE)
    x <- gsub("(?<![A-Za-z])LINE(?:\\s+ITEM)?\\b",                              " ", x, perl = TRUE)
    x <- gsub("(?<![A-Za-z])CLIN\\b",                                           " ", x, perl = TRUE)

    x  <- gsub("\\s+", " ", x, perl = TRUE)
    x  <- stringr::str_squish(x)

    # fallback: if we blanked content but original had ASCII letters, keep cleaned original
    empty_now   <- !nzchar(x)
    had_letters <- stringi::stri_detect_regex(x0, "[A-Za-z]")
    x[empty_now & had_letters] <- x0[empty_now & had_letters]
    x
  }

 
  is_serial_like_majority <- function(x_fix) {
    ifelse(!nzchar(x_fix), FALSE, {
      n_all    <- nchar(x_fix)
      n_space  <- stringi::stri_count_fixed(x_fix, " ")
      n_code   <- stringi::stri_count_regex(x_fix, "[0-9/_-]")
      n_alpha  <- stringi::stri_count_regex(x_fix, "[A-Za-z]")

      char_share_code  <- ifelse((n_all - n_space) > 0, n_code  / (n_all - n_space), 0)
      char_share_alpha <- ifelse((n_all - n_space) > 0, n_alpha / (n_all - n_space), 0)

      toks <- stringi::stri_split_regex(x_fix, "\\s+", omit_empty = TRUE)
      token_score <- vapply(toks, function(tt) {
        if (!length(tt)) return(0)
        is_code_tok <- function(tk) {
          if (!nzchar(tk)) return(FALSE)
          nonlet <- stringi::stri_count_regex(tk, "[^A-Za-z]") / nchar(tk)
          patmix <- grepl("(?=.*\\d{3,})[A-Za-z0-9][A-Za-z0-9\\-_/]{3,}", tk, perl = TRUE)
          (nonlet >= 0.5) || patmix
        }
        mean(vapply(tt, is_code_tok, logical(1)))
      }, numeric(1))

      has_chain <- grepl("\\b[A-Za-z0-9]{2,}[-_/][A-Za-z0-9\\-_/]{2,}\\b", x_fix, perl = TRUE)
      has_3dig  <- grepl("\\d{3,}", x_fix)

      english_rescue <- char_share_alpha >= 0.70
      char_major <- (char_share_code >= 0.65)
      tok_major  <- (token_score >= 0.60) & (has_chain | has_3dig)

      (!english_rescue) & (char_major | tok_major)
    })
  }

  
  is_multiline_markup_vec <- function(u_fix) {
    stringi::stri_detect_regex(u_fix, "\\b(?:LINE\\s*\\d+|CLIN\\s*[:;\\-]?)\\b")
  }


  is_junky_vec <- function(x_orig) {
    bad <- stringi::stri_count_regex(x_orig, "[^\\p{ASCII}]")
    (bad / pmax.int(1L, nchar(x_orig))) > 0.05
  }


  desc_col <- if ("expanded_description" %in% names(subcontracts)) "expanded_description"
  else if ("subaward_description" %in% names(subcontracts)) "subaward_description"
  else names(subcontracts)[1]

  raw_text <- subcontracts[[desc_col]]
  raw_text[is.na(raw_text)] <- ""


  text_cln <- clean1(raw_text)
  text_fix <- fix_mojibake(text_cln)
  text_fix <- ifelse(nzchar(text_fix), text_fix, text_cln)

  flag_short    <- nchar(text_fix) < 40L
  flag_serial   <- is_serial_like_majority(text_fix)
  u_fix         <- stringi::stri_trans_toupper(text_fix)
  flag_multilin <- is_multiline_markup_vec(u_fix)
  flag_junky    <- is_junky_vec(raw_text)

  out_flags <- dplyr::tibble(
    .row_id = seq_along(raw_text),
    raw_text, text_cln, text_fix,
    flag_serial, flag_multilin, flag_junky, flag_short
  )
  txtdf <- dplyr::bind_cols(subcontracts, out_flags)

  meta <- list(stage="stage1_clean_flags", created_at=Sys.time(),
               input_hash=.hash_input(subcontracts), desc_col=desc_col)
  .save_rds_meta(txtdf, out_path, meta)
}


### Stage 2 


stage2_load_dau <- function(in_path = file.path(here("Data","acronymdict.RDS")), out_path = file.path(proc_dir, "acr_dict_dau.rds")) {
  acr <- readRDS(in_path) %>% as_tibble()
  nm <- names(acr)
  if (!all(c("acr","exp") %in% nm)) {
    colname <- nm[which.max(colSums(!is.na(acr)))]
    lines <- acr[[colname]] |> as.character() |> str_squish()
    lines <- lines[!is.na(lines) & nzchar(lines)]
    split_acr <- function(txt) { m <- regexec("^(\\S{2,15})\\s+(.+)$", txt); r <- regmatches(txt, m)[[1]]; if (length(r) == 3) tibble(acr=toupper(trimws(r[2])), exp=trimws(r[3])) else tibble(acr=NA,exp=NA) }
    acr <- bind_rows(lapply(lines, split_acr))
  }
  acr <- acr %>% filter(!is.na(acr), !is.na(exp), nzchar(acr), nzchar(exp)) %>% mutate(exp = str_split(exp, ";")) %>% unnest(exp) %>% mutate(acr = toupper(str_squish(acr)), exp = str_squish(exp)) %>% filter(nzchar(exp)) %>% distinct(acr, exp)
  meta <- list(stage="stage2_load_dau", created_at=Sys.time(), input_hash=.hash_input(acr))
  .save_rds_meta(acr, out_path, meta)
}



  
  ### Stage 3 
  
stage3_mine_pairs <- function(
  txtdf_path       = file.path(proc_dir, "subcontracts_with_flags.rds"),
  out_path         = file.path(proc_dir, "hi_conf_pairs.rds"),
  INITIALS_SLACK   = 3L,
  use_short_in_mining = TRUE,
  ALLOW_DIGITS     = FALSE,  
  MIN_ACR_LEN      = 3L     
) {
  library(dplyr); library(stringr); library(stringi); library(purrr); library(tibble); library(readr)

  txtdf <- readRDS(txtdf_path)

  
  src_text <- ifelse(nzchar(txtdf$text_fix), txtdf$text_fix, txtdf$text_cln)
  if (!use_short_in_mining) src_text[txtdf$flag_short] <- ""


  ACRONYM_KEEP <- c("DOD","USAF","USN","USMC","USA","AFOTEC","JSF","COTS","MTBF","BOM",
                    "SDRL","NSN","CLIN","NIIN","CAGE","ISR","GPS","RF","IT","AI","UAS","UAV","SATCOM")

  KEEP_UPPER <- unique(toupper(ACRONYM_KEEP))
  STOP_ACR   <- c("TO","FOR","THE","AND","OF","IN","ON","AT","BY","FROM","A","AN","AS","OR")
  CONNECTORS <- c("AND","OR","OF","THE","FOR","IN","ON","WITH","TO","BY","AT","FROM","A","AN","AS")
  CATALOG    <- c("PN","NSN","CLIN","NIIN","SDRL","FAI","MTBF","BOM","EOL","CAGE","P/N","S/N")


  looks_like_acronym <- local({
    `%IN%` <- `%in%`
    re_letters <- "^[A-Z][A-Z&\\.\\-]{1,15}$"
    re_any     <- "^[A-Z0-9][A-Z0-9&\\.\\-]{1,15}$"
    re_shout   <- "^[A-Z]{6,}$"
    function(acr) {
      up <- toupper(as.character(acr))
      ok <- nzchar(up)
      ok <- ok & grepl(if (ALLOW_DIGITS) re_any else re_letters, up)
      ok <- ok & (stringi::stri_count_regex(up, "[A-Z]") >= 2)
      # length gate: allow short only if explicitly kept
      ok <- ok & (nchar(up) >= MIN_ACR_LEN | up %IN% KEEP_UPPER)
     
      ok <- ok & !(up %IN% STOP_ACR)
      shout <- grepl(re_shout, up)
      ok <- ok & (!shout | (up %IN% KEEP_UPPER))
      ok[is.na(ok)] <- FALSE
      ok
    }
  })

  normalize_phrase <- function(x) {
    x %>% toupper() %>%
      str_replace_all("&", " AND ") %>%
      str_replace_all("[ ]*/[ ]*", "/") %>%
      str_replace_all("[^A-Z0-9/ \\-]", " ") %>%
      str_replace_all("\\s+", " ") %>%
      str_squish()
  }
  initials_from_phrase_vec <- function(phr, drop_words = CONNECTORS) {
    p <- normalize_phrase(phr)
    toks <- str_split(p, "\\s+")
    n <- length(toks)
    out_init <- character(n); out_ntok <- integer(n)
    for (i in seq_len(n)) {
      ti <- toks[[i]]
      ti <- ti[!(ti %in% drop_words)]
      ti <- ti[nzchar(ti)]
      out_ntok[i] <- length(ti)
      out_init[i] <- if (length(ti)) paste0(substr(ti, 1, 1), collapse = "") else ""
    }
    list(init = out_init, n_tokens = out_ntok)
  }
  strip_leading_trailing_conn <- function(x) {
    y <- normalize_phrase(x)
    repeat {
      y2 <- str_replace(y, paste0("^(", paste(CONNECTORS, collapse="|"), ")\\s+"), "")
      y2 <- str_replace(y2, paste0("\\s+(", paste(CONNECTORS, collapse="|"), ")$"), "")
      if (identical(y2, y)) break
      y <- y2
    }
    y
  }


  acr_pat <- if (ALLOW_DIGITS) "([A-Z0-9][A-Z0-9&\\.\\-]{1,15})" else "([A-Z][A-Z&\\.\\-]{1,15})"
  pat_exp_acr <- paste0("([A-Za-z][A-Za-z0-9/&\\-\\. ,]+?)\\s*\\(", acr_pat, "\\)")
  pat_acr_exp <- paste0(acr_pat, "\\s*\\(([A-Za-z][A-Za-z0-9/&\\-\\. ,]+?)\\)")
  pat_acr_sep <- paste0(acr_pat, "\\s*[:\\-]\\s*([A-Za-z][A-Za-z0-9/&\\-\\. ,]+?)\\b")
  pat_exp_sep <- paste0("([A-Za-z][A-Za-z0-9/&\\-\\. ,]+?)\\s*[:\\-]\\s*", acr_pat, "\\b")

  
  eligible <- stri_detect_fixed(src_text, "(") |
              stri_detect_regex(src_text, "(?i)[A-Z0-9]\\s*[:\\-]\\s*[A-Za-z]") |
              stri_detect_regex(src_text, "(?i)[A-Za-z]\\s*[:\\-]\\s*[A-Z0-9]")
  if (!any(eligible)) {
    empty <- tibble(acr=character(), exp=character(), pattern=character(), doc_id=integer())
    attr(empty, "meta") <- list(stage="stage3_mine_pairs", created_at=Sys.time(), kept=0L, note="no eligible rows")
    write_rds(empty, out_path)
    return(invisible(empty))
  }
  idx   <- which(eligible)
  texts <- src_text[idx]
  ids   <- txtdf$.row_id[idx]


  extract_pairs_one <- function(txt, doc_id) {
    if (!nzchar(txt)) return(NULL)
    out <- list()
    m1 <- stri_match_all_regex(txt, pat_exp_acr); if (length(m1) && !is.null(m1[[1]]) && nrow(m1[[1]]))
      out[[length(out)+1]] <- tibble(acr = m1[[1]][,3], exp = m1[[1]][,2], pattern = "longform_paren",     doc_id = doc_id)
    m2 <- stri_match_all_regex(txt, pat_acr_exp); if (length(m2) && !is.null(m2[[1]]) && nrow(m2[[1]]))
      out[[length(out)+1]] <- tibble(acr = m2[[1]][,2], exp = m2[[1]][,3], pattern = "acr_paren_longform", doc_id = doc_id)
    m3 <- stri_match_all_regex(txt, pat_acr_sep); if (length(m3) && !is.null(m3[[1]]) && nrow(m3[[1]]))
      out[[length(out)+1]] <- tibble(acr = m3[[1]][,2], exp = m3[[1]][,3], pattern = "acr_sep_longform",   doc_id = doc_id)
    m4 <- stri_match_all_regex(txt, pat_exp_sep); if (length(m4) && !is.null(m4[[1]]) && nrow(m4[[1]]))
      out[[length(out)+1]] <- tibble(acr = m4[[1]][,3], exp = m4[[1]][,2], pattern = "longform_sep_acr",   doc_id = doc_id)
    if (!length(out)) return(NULL)
    bind_rows(out)
  }


  n <- length(texts)
  chunks <- 8L
  cutp <- floor(seq(0, n, length.out = chunks + 1L))
  parts <- lapply(seq_len(chunks), function(i) {
    if (cutp[i] + 1L <= cutp[i + 1L]) (cutp[i] + 1L):cutp[i + 1L] else integer(0)
  })
  parts <- parts[vapply(parts, length, 1L) > 0L]

  worker <- function(ix) {
    res <- vector("list", length(ix))
    for (k in seq_along(ix)) res[[k]] <- extract_pairs_one(texts[ix[k]], ids[ix[k]])
    bind_rows(res)
  }
  outs <- .safe_apply(parts, worker, workers = min(6L, length(parts)))
  raw  <- bind_rows(outs)

  if (is.null(raw) || !nrow(raw)) {
    empty <- tibble(acr=character(), exp=character(), pattern=character(), doc_id=integer())
    attr(empty, "meta") <- list(stage="stage3_mine_pairs", created_at=Sys.time(), kept=0L, note="no raw matches")
    write_rds(empty, out_path); return(invisible(empty))
  }

  
  raw <- raw %>%
    mutate(acr = toupper(str_squish(acr)),
           exp =        str_squish(exp)) %>%
    filter(nchar(acr) >= 2, nchar(exp) >= 3, !(acr %in% CATALOG))


  if (!ALLOW_DIGITS) raw <- filter(raw, !grepl("\\d", acr))

  
  raw <- raw %>%
    mutate(len = nchar(acr)) %>%
    filter(len >= MIN_ACR_LEN | acr %in% KEEP_UPPER) %>%
    select(-len)


  exp_clean <- strip_leading_trailing_conn(raw$exp)
  iv <- initials_from_phrase_vec(exp_clean)

  mined <- raw %>%
    mutate(
      exp_norm = exp_clean,
      init     = iv$init,
      n_tokens = iv$n_tokens,
      acr_key  = gsub("[^A-Z]", "", acr),
      init_key = gsub("[^A-Z]", "", init),
      la       = nchar(acr_key),
      li       = nchar(init_key),
      prefix_ok = li >= la &
                  substr(init_key, 1L, la) == acr_key &
                  (li - la) <= INITIALS_SLACK
    ) %>%
    filter(
      (looks_like_acronym(acr) | init_key == acr_key),
      nzchar(init_key),
      (init_key == acr_key) | prefix_ok,

      (!txtdf$flag_serial[doc_id]) | (pattern %in% c("longform_paren","acr_paren_longform"))
    ) %>%
    transmute(acr, exp = exp_norm, doc_id, pattern) %>%
    distinct()

  attr(mined, "meta") <- list(
    stage      = "stage3_mine_pairs",
    created_at = Sys.time(),
    kept       = nrow(mined),
    allow_digits = ALLOW_DIGITS,
    min_acr_len = MIN_ACR_LEN
  )
  write_rds(mined, out_path)
  invisible(mined)
}



### Stage 4 

stage4_counts_features <- function(pairs_path = file.path(proc_dir,"hi_conf_pairs.rds"),
                                   out_dict_raw = file.path(proc_dir,"dict_all_raw.rds"),
                                   out_feat = file.path(proc_dir,"feat_tbl.rds")) {
  pairs <- readRDS(pairs_path)
  dict_all_raw <- pairs %>% mutate(exp = toupper(exp)) %>% dplyr::count(acr, exp, name="freq", sort=FALSE)
  feat_tbl <- pairs %>% mutate(exp_uc_raw = toupper(exp)) %>% distinct(acr, exp_uc = exp_uc_raw, doc_id, pattern) %>% group_by(acr, exp_uc) %>% summarise(doc_coverage=n_distinct(doc_id), def_hits=sum(pattern %in% c("longform_paren","acr_paren_longform")), .groups="drop")
  .save_rds_meta(dict_all_raw, out_dict_raw, list(stage="stage4_counts_features", created_at=Sys.time(), input_hash=.hash_input(pairs)))
  .save_rds_meta(feat_tbl, out_feat, list(stage="stage4_counts_features", created_at=Sys.time(), input_hash=.hash_input(pairs)))
}



### Stage 5 



stage5_canon_jw <- function(dict_raw_path = file.path(proc_dir,"dict_all_raw.rds"),
                            dau_path = file.path(proc_dir,"acr_dict_dau.rds"),
                            out_dict = file.path(proc_dir,"dict_all.rds"),
                            out_map = file.path(proc_dir,"exp_map.rds"),
                            JW_THRESH = 0.95, block_prefix = 4) {
  dict_all_raw <- readRDS(dict_raw_path)
  acr_dict_dau <- if (file.exists(dau_path)) readRDS(dau_path) else tibble(acr=character(),exp=character())
  ACRONYM_KEEP <- c("DoD","USAF","USN","USMC","USA","AFOTEC","JSF","COTS","F-35","MTBF","BOM","SDRL","NSN","CLIN","NIIN","CAGE","ISR","GPS","RF","IT","AI","UAS","UAV","SATCOM")
  connectors <- c("AND","OR","OF","THE","FOR","IN","ON","WITH","TO","BY","AT","FROM","A","AN","AS")
  strip_leading_trailing_conn <- function(x) {
    y <- toupper(x); y <- str_replace_all(y, "[ ]*/[ ]*", "/"); y <- str_replace_all(y, "\\s+", " "); y <- str_squish(y)
    repeat {
      y2 <- str_replace(y, paste0("^(", paste(connectors, collapse="|"), ")\\s+"), "")
      y2 <- str_replace(y2, paste0("\\s+(", paste(connectors, collapse="|"), ")$"), "")
      if (identical(y2, y)) break
      y <- y2
    }
    y
  }
  has_content_token <- function(x) {
    if (!nzchar(x)) return(FALSE)
    toks <- str_split(x, "\\s+")[[1]]
    any(nchar(toks) >= 3 | str_detect(toks, "[0-9/\\-]"))
  }
  dau_uc <- acr_dict_dau %>% transmute(acr=toupper(acr), exp_uc=toupper(exp))
  dr <- dict_all_raw %>% mutate(exp_uc = strip_leading_trailing_conn(exp)) %>% filter(nzchar(exp_uc)) %>% filter(map_lgl(exp_uc, has_content_token))
  dr <- dr %>% left_join(dau_uc %>% mutate(dau=1L), by=c("acr","exp_uc")) %>% mutate(dau = coalesce(dau, 0L))
  rare_ok <- dr %>% group_by(acr, exp_uc) %>% summarise(freq = sum(freq), dau = max(dau), .groups="drop")
  keep_keys <- rare_ok %>% filter(freq > 1 | dau > 0) %>% transmute(acr, exp_uc) %>% distinct()
  dr <- dr %>% inner_join(keep_keys, by=c("acr","exp_uc")) %>% group_by(acr, exp_uc) %>% summarise(freq=sum(freq), .groups="drop")
  dr <- dr %>% mutate(ntok = str_count(exp_uc, "\\S+"),
                      pref = substr(exp_uc, 1, block_prefix),
                      bucket = cut(ntok, breaks=c(-Inf,2,4,Inf), labels=c("t12","t34","t5p")))
  by_acr <- split(dr, dr$acr)
  worker <- function(df_acr) {
    df_acr <- df_acr %>% arrange(desc(freq), desc(nchar(exp_uc)))
    blocks <- split(df_acr, interaction(df_acr$pref, df_acr$bucket, drop=TRUE))
    dict_list <- list(); map_list <- list()
    for (bl in blocks) {
      if (nrow(bl) == 0) next
      ord <- order(-bl$freq, -nchar(bl$exp_uc))
      bl <- bl[ord, , drop=FALSE]
      n <- nrow(bl)
      if (n == 1) {
        dict_list[[length(dict_list)+1]] <- tibble(acr = bl$acr[1], exp_uc = bl$exp_uc[1], freq = bl$freq[1])
        map_list[[length(map_list)+1]]  <- tibble(acr = bl$acr[1], exp_uc_raw = bl$exp_uc[1], exp_uc_canon = bl$exp_uc[1])
        next
      }
      top_cap <- min(50, n)
      S <- matrix(0, n, n)
      for (i in seq_len(n)) {
        jset <- seq_len(top_cap)
        jw <- 1 - stringdist(bl$exp_uc[i], bl$exp_uc[jset], method="jw")
        S[i, jset] <- jw
      }
      diag(S) <- 1
      comp <- rep(NA_integer_, n); cid <- 0L
      for (i in seq_len(n)) if (is.na(comp[i])) {
        cid <- cid + 1L; q <- i; comp[i] <- cid
        while (length(q)) {
          v <- q[1]; q <- q[-1]
          neigh <- which(S[v, ] >= JW_THRESH & is.na(comp))
          if (length(neigh)) { comp[neigh] <- cid; q <- c(q, neigh) }
        }
      }
      clusters <- split(seq_len(n), comp)
      for (ix in clusters) {
        pick <- ix[order(-bl$freq[ix], -nchar(bl$exp_uc[ix]))][1]
        dict_list[[length(dict_list)+1]] <- tibble(acr=bl$acr[pick], exp_uc=bl$exp_uc[pick], freq=sum(bl$freq[ix]))
        map_list[[length(map_list)+1]]  <- tibble(acr=bl$acr[ix], exp_uc_raw=bl$exp_uc[ix], exp_uc_canon=bl$exp_uc[pick])
      }
    }
    list(dict=bind_rows(dict_list), map=bind_rows(map_list))
  }
  res <- mclapply(by_acr, worker, mc.cores = 6)
  dict_all <- bind_rows(lapply(res, `[[`, "dict")) %>% group_by(acr, exp_uc) %>% summarise(freq=sum(freq), .groups="drop")
  exp_map  <- bind_rows(lapply(res, `[[`, "map"))  %>% distinct(acr, exp_uc_raw, .keep_all=TRUE)
  .save_rds_meta(dict_all, out_dict, list(stage="stage5_canon_jw", created_at=Sys.time(), JW_THRESH=JW_THRESH, block_prefix=block_prefix))
  .save_rds_meta(exp_map,  out_map,  list(stage="stage5_canon_jw", created_at=Sys.time(), JW_THRESH=JW_THRESH, block_prefix=block_prefix))
}



### Stage 6 

stage6_score_candidates <- function(
  dict_path = file.path(proc_dir,"dict_all.rds"),
  feat_path  = file.path(proc_dir,"feat_tbl.rds"),
  dau_path   = file.path(proc_dir,"acr_dict_dau.rds"),
  out_cand   = file.path(proc_dir,"acr_candidates.rds"),
  out_lex    = file.path(proc_dir,"acr_lexicon.rds"),
  w_def = 2.5, w_freq = 1.0, w_doc = 0.8, w_auth = 2.0,
  margin = 0.15, topK_per_acr = 2L
) {
  library(dplyr); library(stringr); library(tidyr); library(tibble); library(purrr)

  norm_exp <- function(x) {
    x %>% toupper() %>%
      stringr::str_replace_all("[ ]*/[ ]*", "/") %>%
      stringr::str_replace_all("\\s+", " ") %>%
      stringr::str_squish()
  }

  # ---------- Inputs ----------
  dict_all <- readRDS(dict_path)
  feat_tbl <- readRDS(feat_path)
  dau_raw  <- if (file.exists(dau_path)) readRDS(dau_path) else tibble(acr=character(), exp=character())

  mined_base <- dict_all %>%
    transmute(acr = toupper(acr), exp_uc = norm_exp(exp_uc), freq = as.integer(freq))

  dau_uc <- dau_raw %>%
    transmute(acr = toupper(str_squish(acr)), exp_uc = norm_exp(exp)) %>%
    filter(nzchar(acr), nzchar(exp_uc)) %>%
    distinct(acr, exp_uc) %>%
    mutate(auth_hits = 1L)

  # Union mined + DAU
  base_union <- full_join(mined_base, select(dau_uc, acr, exp_uc), by = c("acr","exp_uc")) %>%
    mutate(freq = coalesce(freq, 0L))

  # Features + raw composite
  scored <- base_union %>%
    left_join(feat_tbl, by = c("acr","exp_uc")) %>%
    left_join(dau_uc,  by = c("acr","exp_uc")) %>%
    mutate(
      doc_coverage = coalesce(doc_coverage, 0L),
      def_hits     = coalesce(def_hits,     0L),
      auth_hits    = coalesce(auth_hits,    0L)
    ) %>%
    mutate(
      score_raw = w_def*def_hits + w_freq*log1p(freq) + w_doc*log1p(doc_coverage) + w_auth*auth_hits
    )


  scored <- scored %>%
    group_by(acr) %>%
    mutate(
      score = if (max(score_raw, na.rm=TRUE) > min(score_raw, na.rm=TRUE)) {
        (score_raw - min(score_raw, na.rm=TRUE)) /
        (max(score_raw, na.rm=TRUE) - min(score_raw, na.rm=TRUE) + 1e-9)
      } else 1.0  # singleton acronyms: treat as fully confident within-acronym
    ) %>%
    mutate(rank = row_number(desc(score))) %>%
    ungroup()


  scored <- scored %>%
    group_by(acr) %>%
    mutate(
      score_within = if (max(score_raw, na.rm=TRUE) > min(score_raw, na.rm=TRUE)) {
        (score_raw - min(score_raw, na.rm=TRUE)) /
        (max(score_raw, na.rm=TRUE) - min(score_raw, na.rm=TRUE) + 1e-9)
      } else 1.0
    ) %>%
    ungroup()


  q <- stats::quantile(scored$score_raw, c(0.05, 0.95), na.rm = TRUE, names = FALSE)
  q_lo <- q[1]; q_hi <- q[2]
  s_w   <- pmin(pmax(scored$score_raw, q_lo), q_hi)
  g_lin <- (s_w - q_lo) / (q_hi - q_lo + 1e-9)

  mu  <- stats::median(scored$score_raw, na.rm = TRUE)
  sig <- stats::mad(scored$score_raw, center = mu, constant = 1.4826, na.rm = TRUE)
  z   <- (scored$score_raw - mu) / (sig + 1e-9)
  temp <- 1.5
  g_logit <- plogis(z / temp)

  g_w <- 0.6
  prior_global <- g_w * g_lin + (1 - g_w) * g_logit

  w_within <- 0.2
  scored <- scored %>%
    mutate(
      prior_within = score_within,
      prior_global = prior_global,
      prior        = w_within * prior_within + (1 - w_within) * prior_global
    )


  acr_candidates <- scored %>% mutate(exp_disp = exp_uc)

  acr_lexicon <- acr_candidates %>%
    arrange(desc(score), .by_group = TRUE) %>%
    group_by(acr) %>%
    summarise(
      best        = first(exp_disp),
      best_uc     = first(exp_uc),
      confidence  = first(score),
      best_prior  = first(prior),
      alt         = if (n() >= 2) nth(exp_disp, 2) else NA_character_,
      alt_score   = if (n() >= 2) nth(score, 2) else NA_real_,
      alt_prior   = if (n() >= 2) nth(prior, 2) else NA_real_,
      keep_top2   = !is.na(alt_score) & (confidence - alt_score) < margin,
      .groups = "drop"
    )

  .save_rds_meta(
    acr_candidates, out_cand,
    list(stage="stage6_score_candidates", created_at=Sys.time(),
         weights=c(w_def,w_freq,w_doc,w_auth), margin=margin,
         topK_per_acr=topK_per_acr,
         prior_scheme=list(singleton_within=1.0, w_within=w_within, g_w=g_w, temp=temp, winsor=c(0.05,0.95)))
  )
  .save_rds_meta(
    acr_lexicon, out_lex,
    list(stage="stage6_score_candidates", created_at=Sys.time(),
         weights=c(w_def,w_freq,w_doc,w_auth), margin=margin,
         topK_per_acr=topK_per_acr,
         prior_scheme=list(singleton_within=1.0, w_within=w_within, g_w=g_w, temp=temp, winsor=c(0.05,0.95)))
  )
}



### Stage 7 

stage7_embeddings_frozen <- function(
  txtdf_path   = file.path(proc_dir, "subcontracts_with_flags.rds"),
  w2v_mat_path = file.path(proc_dir, "w2v_contracts_matrix.rds"),
  vocab_path   = file.path(proc_dir, "w2v_vocab.rds"),
  dim = 100,
  type = "skip-gram",
  min_count = 5,
  window = 8,
  iter = 10,
  alpha = 0.05,
  use_ns = TRUE,
  ns_size = 5,
  verbose = TRUE
) {

  if (file.exists(w2v_mat_path)) {
    return(invisible(readRDS(w2v_mat_path)))
  }


  txtdf <- readRDS(txtdf_path)

  corp <- quanteda::corpus(txtdf$text_cln)
  toks <- quanteda::tokens(
            corp,
            remove_punct   = TRUE,
            remove_symbols = TRUE
          ) |>
          quanteda::tokens_remove(quanteda::stopwords("en"), padding = TRUE) |>
          quanteda::tokens_tolower()


  w2v <- wordvector::textmodel_word2vec(
    x         = toks,
    dim       = dim,
    type      = type,       # "skip-gram" or "cbow"
    min_count = min_count,
    window    = window,
    iter      = iter,
    alpha     = alpha,
    use_ns    = use_ns,
    ns_size   = ns_size,
    verbose   = verbose
  )


  w2v_mat <- as.matrix(w2v)


  write_rds(rownames(w2v_mat), vocab_path)
  .save_rds_meta(
    w2v_mat,
    w2v_mat_path,
    list(
      stage      = "stage7_embeddings_frozen",
      created_at = Sys.time(),
      source     = "wordvector",
      dim        = dim,
      window     = window,
      min_count  = min_count,
      iter       = iter,
      use_ns     = use_ns,
      ns_size    = ns_size
    )
  )

  invisible(w2v_mat)
}


```

```{r}

stage8_expand <- function(
  mode = c("tune","full"),
  params = list(alpha = 0.8, tau = 0.50, k_window = 12, tau_short = NULL, topK_per_acr = 3L),
  txtdf_path      = file.path(proc_dir, "subcontracts_with_flags.rds"),
  cand_path       = file.path(proc_dir, "acr_candidates.rds"),
  w2v_path        = file.path(proc_dir, "w2v_contracts_matrix.rds"),
  sample_idx_path = file.path(proc_dir, "sample_idx.rds"),
  out_metrics     = file.path(proc_dir, "expanded_sample_metrics.rds"),
  out_preview     = file.path(proc_dir, "expanded_sample_preview.rds"),
  out_full        = file.path(proc_dir, "subcontracts_expanded.rds"),
  out_metrics_full = file.path(proc_dir, "expanded_full_metrics.rds"),
  out_preview_full = file.path(proc_dir, "expanded_full_preview.rds"),
  preview_full_n   = 5000L,
  workers = 6
) {
  `%||%` <- if (exists("%||%", mode = "function")) get("%||%") else function(x, y) if (is.null(x)) y else x

  .safe_apply <- function(X, FUN, workers = 6) {
    workers <- as.integer(max(1L, workers))
    if (.Platform$OS.type == "windows" || workers == 1L) return(lapply(X, FUN))
    ok <- TRUE
    out <- tryCatch(parallel::mclapply(X, FUN, mc.cores = workers),
                    error = function(e) { ok <<- FALSE; e })
    if (!ok || inherits(out, "error") || is.null(out)) out <- lapply(X, FUN)
    out
  }

  normalize_res <- function(x, fallback_txt = "") {
    # Ensure shape for vapply; never return NULL/length-0 fields
    if (is.null(x) || is.null(x$text) || !length(x$text)) {
      return(list(
        text          = as.character(fallback_txt),
        acrs_detected = character(0),
        n             = 0L,
        mean_local    = NA_real_,
        max_local     = NA_real_,
        why           = "normalized_empty"
      ))
    }
    x$text       <- as.character(x$text)[1]
    x$n          <- as.integer(x$n %||% 0L)
    x$mean_local <- if (length(x$mean_local)) as.numeric(x$mean_local[1]) else NA_real_
    x$max_local  <- if (length(x$max_local))  as.numeric(x$max_local[1])  else NA_real_
    x$acrs_detected <- as.character(x$acrs_detected %||% character(0))
    x$why        <- as.character(x$why %||% "ok")[1]
    x
  }

  mode <- match.arg(mode)

  # ---------- Inputs ----------
  txtdf          <- readRDS(txtdf_path)
  acr_candidates <- readRDS(cand_path)
  w2v_mat        <- readRDS(w2v_path)
  texts_for_expansion <- txtdf$text_fix %||% txtdf$text_cln

  alpha     <- params$alpha %||% 0.6
  tau       <- params$tau %||% 0.55
  k_window  <- params$k_window %||% 8
  tau_short <- params$tau_short %||% tau
  topK      <- as.integer(params$topK_per_acr %||% 2L)

  ACRONYM_KEEP <- c("DoD","USAF","USN","USMC","USA","AFOTEC","JSF","COTS","F-35",
                    "MTBF","BOM","SDRL","NSN","CLIN","NIIN","CAGE","ISR","GPS",
                    "RF","IT","AI","UAS","UAV","SATCOM")
  KEEP_UPPER <- toupper(ACRONYM_KEEP)

  # Build acronym universe (candidates ∪ DAU), restricted to short all-caps with &/. allowed
  dau_path <- file.path(proc_dir, "acr_dict_dau.rds")
  dau_raw  <- if (file.exists(dau_path)) readRDS(dau_path) else tibble::tibble(acr=character(), exp=character())

  acr_universe <- sort(unique(c(
    toupper(acr_candidates$acr),
    toupper(dau_raw$acr)
  )))
  acr_universe <- acr_universe[grepl("^[A-Z&\\.]{2,5}$", acr_universe)]

  have_fastmatch <- requireNamespace("fastmatch", quietly = TRUE)
  in_set <- function(x, set) {
    if (have_fastmatch) !is.na(fastmatch::fmatch(x, set)) else x %in% set
  }

  # Top-K per acronym (keep prior if present), distinct rows
  acr_tbl <- acr_candidates %>%
    dplyr::mutate(acr = toupper(acr)) %>%
    dplyr::arrange(dplyr::desc(prior), dplyr::desc(score)) %>%
    dplyr::group_by(acr) %>%
    dplyr::slice_head(n = topK) %>%
    dplyr::ungroup() %>%
    dplyr::distinct(acr, exp_uc, exp_disp, prior, score)

  acr_index <- split(seq_len(nrow(acr_tbl)), acr_tbl$acr)

  # ---------- Embedding helpers ----------
  tokenize_for_w2v <- function(x) {
    if (is.null(x) || is.na(x) || !nzchar(x)) return(character(0))
    toks <- strsplit(tolower(x), "\\s+")[[1]]
    toks[nzchar(toks)]
  }
  embed_phrase_w2v <- function(phrase) {
    toks <- tokenize_for_w2v(phrase)
    if (!length(toks)) return(rep(0, ncol(w2v_mat)))
    idx <- match(toks, rownames(w2v_mat)); idx <- idx[!is.na(idx)]
    if (!length(idx)) return(rep(0, ncol(w2v_mat)))
    colMeans(w2v_mat[idx, , drop = FALSE])
  }
  exp_mat <- if (nrow(acr_tbl)) {
    m <- do.call(rbind, lapply(acr_tbl$exp_uc, embed_phrase_w2v))
    m[is.na(m)] <- 0
    m
  } else matrix(numeric(0), nrow = 0, ncol = ncol(w2v_mat))

  cosine_sim <- function(a, b) {
    na <- sqrt(sum(a*a)); nb <- sqrt(sum(b*b))
    if (na == 0 || nb == 0) return(0)
    sum(a*b)/(na*nb)
  }
  context_string <- function(v, i, k) {
    lo <- max(1L, i - k); hi <- min(length(v), i + k)
    paste(v[lo:hi], collapse = " ")
  }

  # ---------- Acronym detection ----------
  find_acrs <- function(words_vec) {
    if (!length(words_vec)) return(tibble::tibble(pos = integer(0), acr = character(0)))
    out_pos <- integer(0); out_acr <- character(0)

    for (i in seq_along(words_vec)) {
      w <- words_vec[i]
      w1 <- gsub("^[^A-Za-z0-9&\\.]+|[^A-Za-z0-9&\\.]+$", "", w)
      parts <- unlist(strsplit(gsub("[,;:()]+", " ", w1), "[/\\-]"), use.names = FALSE)
      if (!length(parts)) next
      cand <- toupper(parts[nzchar(parts)])
      cand <- cand[grepl("^[A-Z&\\.]{2,5}$", cand)]
      if (!length(cand)) next

      hits <- cand[in_set(cand, acr_universe)]
      if (length(hits)) {
        out_pos <- c(out_pos, rep(i, length(hits)))
        out_acr <- c(out_acr, hits)
      }
    }

    if (!length(out_pos)) return(tibble::tibble(pos = integer(0), acr = character(0)))
    tibble::tibble(pos = out_pos, acr = out_acr)
  }

  # ---------- Scoring ----------
  score_best_expansion <- function(win_txt, acr_here) {
    cand_ix <- acr_index[[acr_here]]
    if (is.null(cand_ix) || !length(cand_ix)) return(NULL)

    ctx_vec <- embed_phrase_w2v(win_txt)
    sims <- vapply(cand_ix, function(j) cosine_sim(ctx_vec, exp_mat[j, ]), numeric(1))
    sims[!is.finite(sims)] <- 0

    pr <- acr_tbl$prior[cand_ix]
    if (is.null(pr) || !length(pr)) pr <- rep(0.5, length(cand_ix))  # fallback prior
    pr[!is.finite(pr)] <- 0

    # downweight alpha on tiny contexts
    tok_cnt <- {
      s <- trimws(gsub("\\s+", " ", win_txt))
      if (!nzchar(s)) 0L else length(strsplit(s, " ", fixed = TRUE)[[1]])
    }
    alpha_eff <- if (is.finite(tok_cnt) && tok_cnt <= 2) 0.20 else alpha

    sims01 <- pmax(0, sims)  # clamp negatives; keep your scale
    s <- alpha_eff * sims01 + (1 - alpha_eff) * pr

    # Example domain nudge
    if (!is.null(win_txt) && length(win_txt) && (acr_here %in% c("R&D","RD"))) {
      if (grepl("\\b(SUPPORT|SVCS|SERVICES|PROGRAM|LABOR|OPS|WORK)\\b", toupper(win_txt))) {
        s <- s + 0.15
      }
    }

    s[!is.finite(s)] <- -Inf
    j <- which.max(s)

    tibble::tibble(
      exp_disp = acr_tbl$exp_disp[cand_ix][j],
      exp_uc   = acr_tbl$exp_uc[cand_ix][j],
      sim      = sims01[j],
      prior    = pr[j],
      local    = s[j]
    )
  }

  # ---------- Expand one line ----------
  expand_one <- function(txt, short_flag = FALSE, serial_flag = FALSE, junky_flag = FALSE) {
    txt <- as.character(txt)
    if (!length(txt) || is.na(txt) || !nzchar(txt)) {
      return(list(text = "", acrs_detected = character(0), n = 0L,
                  mean_local = NA_real_, max_local = NA_real_, why = "empty"))
    }
    if (serial_flag) {
      return(list(text = txt[1], acrs_detected = character(0), n = 0L,
                  mean_local = NA_real_, max_local = NA_real_, why = "serial"))
    }

    words <- strsplit(txt[1], "\\s+")[[1]]
    if (!length(words)) {
      return(list(text = txt[1], acrs_detected = character(0), n = 0L,
                  mean_local = NA_real_, max_local = NA_real_, why = "empty"))
    }

    found <- find_acrs(words)
    if (!nrow(found)) {
      return(list(text = txt[1], acrs_detected = character(0), n = 0L,
                  mean_local = NA_real_, max_local = NA_real_, why = "no_acr_found"))
    }

    keep_mask <- !(found$acr %in% KEEP_UPPER)
    found <- found[keep_mask, , drop = FALSE]
    if (!nrow(found)) {
      return(list(text = txt[1], acrs_detected = character(0), n = 0L,
                  mean_local = NA_real_, max_local = NA_real_, why = "kept_only"))
    }

    can_mask <- vapply(found$acr, function(a) !is.null(acr_index[[a]]), logical(1))
    if (!any(can_mask)) {
      return(list(text = txt[1], acrs_detected = unique(found$acr), n = 0L,
                  mean_local = NA_real_, max_local = NA_real_, why = "no_candidates"))
    }

    fpos <- found$pos[can_mask]
    facr <- found$acr[can_mask]

    picks <- vector("list", length(facr))
    for (r in seq_along(facr)) {
      win <- context_string(words, fpos[r], k_window)
      sc  <- score_best_expansion(win, facr[r])
      if (is.null(sc)) sc <- tibble::tibble(exp_disp = NA_character_, exp_uc = NA_character_, sim = 0, prior = 0, local = -Inf)
      picks[[r]] <- sc
    }
    P <- dplyr::bind_rows(picks)

    gate <- if (isTRUE(short_flag)) tau_short else tau
    replace_ix <- which(!is.na(P$exp_disp) & is.finite(P$local) & P$local > gate)

    if (length(replace_ix)) {
      for (j in replace_ix) {
        i <- fpos[j]; a <- facr[j]
        words[i] <- paste0(P$exp_disp[j], " (", a, ")")
      }
    }

    max_local_any  <- suppressWarnings(ifelse(all(!is.finite(P$local)), NA_real_, max(P$local, na.rm = TRUE)))
    mean_local_rep <- if (length(replace_ix)) mean(P$local[replace_ix], na.rm = TRUE) else NA_real_

    list(
      text          = paste(words, collapse = " "),
      acrs_detected = unique(found$acr),   # includes those without candidates
      n             = length(replace_ix),
      mean_local    = mean_local_rep,      # mean of replacements only
      max_local     = max_local_any,       # best observed (even if < tau)
      why           = if (length(replace_ix)) "replaced" else "below_tau"
    )
  }

  # ---------------------------- TUNE MODE ----------------------------
  if (identical(mode, "tune")) {
    if (file.exists(sample_idx_path)) {
      idx <- readRDS(sample_idx_path)
    } else {
      set.seed(42)
      N <- nrow(txtdf)
      idx <- if (N == 0L) integer(0) else sort(sample.int(N, size = min(10000L, N)))
      readr::write_rds(idx, sample_idx_path)
    }

    if (!length(idx)) {
      metrics <- tibble::tibble(
        alpha = alpha, tau = tau, k_window = k_window, topK = topK,
        repl_per_1k = NA_real_, share_lines = NA_real_,
        median_score = NA_real_, p90_score = NA_real_,
        total_tokens = 0L, sample_n = 0L,
        acr_per_1k = NA_real_, share_lines_with_acr = NA_real_,
        repl_per_1k_given_found = NA_real_
      )
      preview <- tibble::tibble(
        row_id = integer(0), raw = character(0), expanded = character(0),
        num_repl = integer(0), found_acr = integer(0),
        best_local = numeric(0), reason = character(0), raw_nchar = integer(0)
      )
      .save_rds_meta(metrics, out_metrics,
                     list(stage="stage8_expand_tune", created_at=Sys.time(),
                          params=list(alpha=alpha, tau=tau, k_window=k_window, topK=topK)))
      .save_rds_meta(preview, out_preview,
                     list(stage="stage8_expand_tune", created_at=Sys.time(),
                          params=list(alpha=alpha, tau=tau, k_window=k_window, topK=topK)))
      return(invisible(list(metrics = metrics, preview = preview)))
    }

    n_chunks   <- max(1L, min(6L, length(idx)))
    chunk_size <- ceiling(length(idx) / n_chunks)
    shard_ix   <- split(idx, ceiling(seq_along(idx) / chunk_size))

    worker_fun <- function(ix) {
      res <- mapply(function(t, sh, se, ju) expand_one(t, sh, se, ju),
                    t  = texts_for_expansion[ix],
                    sh = txtdf$flag_short[ix],
                    se = txtdf$flag_serial[ix],
                    ju = txtdf$flag_junky[ix],
                    SIMPLIFY = FALSE)
      list(ix = ix, res = res)
    }

    outs    <- .safe_apply(shard_ix, worker_fun, workers = workers)
    ix_all  <- unlist(lapply(outs, `[[`, "ix"))
    res_all <- unlist(lapply(outs, `[[`, "res"), recursive = FALSE)

    ord     <- order(match(ix_all, idx))
    res_all <- res_all[ord]
    res_all <- lapply(seq_along(res_all), function(i) normalize_res(res_all[[i]], texts_for_expansion[idx][i]))

    expanded_text <- vapply(res_all, function(x) x$text, character(1))
    nrep          <- vapply(res_all, function(x) x$n, integer(1))
    found_m       <- vapply(res_all, function(x) length(x$acrs_detected), integer(1))
    max_local     <- vapply(res_all, function(x) x$max_local, numeric(1))
    reason        <- vapply(res_all, function(x) x$why, character(1))

    raw_nchar     <- nchar(texts_for_expansion[idx])
    tok_counts    <- vapply(strsplit(texts_for_expansion[idx], "\\s+"), length, integer(1))
    total_tokens  <- sum(tok_counts)

    repl_per_1k   <- ifelse(total_tokens > 0, 1000 * sum(nrep) / total_tokens, NA_real_)
    share_lines   <- mean(nrep > 0)
    acr_per_1k    <- ifelse(total_tokens > 0, 1000 * sum(found_m) / total_tokens, NA_real_)
    share_lines_with_acr <- mean(found_m > 0)
    repl_per_1k_given_found <- ifelse(sum(found_m) > 0, 1000 * sum(nrep) / sum(found_m), NA_real_)

    med_local_all  <- stats::median(max_local[is.finite(max_local)], na.rm = TRUE)
    p90_local_all  <- stats::quantile(max_local[is.finite(max_local)], 0.9, na.rm = TRUE, names = FALSE)

    metrics <- tibble::tibble(
      alpha = alpha, tau = tau, k_window = k_window, topK = topK,
      repl_per_1k = repl_per_1k, share_lines = share_lines,
      median_score = med_local_all, p90_score = p90_local_all,
      total_tokens = total_tokens, sample_n = length(idx),
      acr_per_1k = acr_per_1k, share_lines_with_acr = share_lines_with_acr,
      repl_per_1k_given_found = repl_per_1k_given_found
    )

    preview <- tibble::tibble(
      row_id     = idx,
      raw        = texts_for_expansion[idx],
      expanded   = expanded_text,
      num_repl   = nrep,
      found_acr  = found_m,
      best_local = max_local,
      reason     = reason,
      raw_nchar  = raw_nchar
    )

    .save_rds_meta(metrics, out_metrics,
                   list(stage="stage8_expand_tune", created_at=Sys.time(),
                        params=list(alpha=alpha, tau=tau, k_window=k_window, topK=topK)))
    .save_rds_meta(preview, out_preview,
                   list(stage="stage8_expand_tune", created_at=Sys.time(),
                        params=list(alpha=alpha, tau=tau, k_window=k_window, topK=topK)))
    return(invisible(list(metrics = metrics, preview = preview)))
  }

  # ---------------------------- FULL MODE ----------------------------
  n <- nrow(txtdf)
  if (n == 0L) {
    warning("No rows in txtdf; nothing to expand.")
    empty <- txtdf %>%
      dplyr::mutate(
        expanded_description = texts_for_expansion,
        acronyms_found       = replicate(0, character(0), simplify = FALSE),
        num_expanded_tokens  = integer(n),
        mean_local_score     = rep(NA_real_, n),
        best_local_score_any = rep(NA_real_, n),
        ambiguity_flag       = rep(FALSE, n)
      ) %>%
      dplyr::select(-.row_id, -raw_text)
    .save_rds_meta(empty, out_full,
                   list(stage="stage8_expand_full", created_at=Sys.time(),
                        params=list(alpha=alpha, tau=tau, k_window=k_window, topK=topK)))
    return(invisible(empty))
  }

  workers <- min(as.integer(workers), max(1L, n))
  cutpoints <- floor(seq(0, n, length.out = workers + 1L))
  splits <- lapply(seq_len(workers), function(i) {
    if (cutpoints[i] + 1L <= cutpoints[i + 1L]) (cutpoints[i] + 1L):cutpoints[i + 1L] else integer(0)
  })
  splits <- splits[vapply(splits, length, integer(1)) > 0]

  worker_fun <- function(ix) {
    if (!length(ix)) return(list(ix = integer(0), res = list()))
    res <- mapply(function(t, sh, se, ju) expand_one(t, sh, se, ju),
                  t  = texts_for_expansion[ix],
                  sh = txtdf$flag_short[ix],
                  se = txtdf$flag_serial[ix],
                  ju = txtdf$flag_junky[ix],
                  SIMPLIFY = FALSE)
    list(ix = ix, res = res)
  }

  outs <- .safe_apply(splits, worker_fun, workers = workers)
  if (is.null(outs) || !length(outs)) outs <- list(worker_fun(seq_len(n)))

  out_list <- vector("list", n)
  for (o in outs) {
    if (is.null(o) || is.null(o$ix) || is.null(o$res)) next
    out_list[o$ix] <- o$res
  }

  missing_ix <- which(vapply(out_list, is.null, logical(1)))
  if (length(missing_ix)) {
    for (i in missing_ix) {
      out_list[[i]] <- list(
        text          = texts_for_expansion[i],
        acrs_detected = character(0),
        n             = 0L,
        mean_local    = NA_real_,
        max_local     = NA_real_,
        why           = "recover_missing"
      )
    }
    warning(sprintf("Recovered %d missing shards; using original text for those rows.", length(missing_ix)))
  }

  # Normalize all results to avoid vapply length errors
  out_list <- lapply(seq_along(out_list), function(i) normalize_res(out_list[[i]], texts_for_expansion[i]))

  expanded_description <- vapply(out_list, function(x) x$text, character(1))
  acronyms_found       <- lapply(out_list, function(x) x$acrs_detected)
  num_expanded_tokens  <- vapply(out_list, function(x) x$n, integer(1))
  mean_local_score     <- vapply(out_list, function(x) x$mean_local, numeric(1))   # replacements only
  best_local_score_any <- vapply(out_list, function(x) x$max_local,  numeric(1))   # observed, even if < tau
  ambiguity_flag       <- rep(FALSE, n)

  # ---- Full-run diagnostics (mirror tune) ----
  nrep_full      <- num_expanded_tokens
  found_m_full   <- vapply(out_list, function(x) length(x$acrs_detected), integer(1))
  max_local_full <- best_local_score_any

  tok_counts_full <- vapply(strsplit(texts_for_expansion, "\\s+"), length, integer(1))
  total_tokens_full <- sum(tok_counts_full)

  repl_per_1k_full             <- ifelse(total_tokens_full > 0, 1000 * sum(nrep_full) / total_tokens_full, NA_real_)
  share_lines_full             <- mean(nrep_full > 0)
  acr_per_1k_full              <- ifelse(total_tokens_full > 0, 1000 * sum(found_m_full) / total_tokens_full, NA_real_)
  share_lines_with_acr_full    <- mean(found_m_full > 0)
  repl_per_1k_given_found_full <- ifelse(sum(found_m_full) > 0, 1000 * sum(nrep_full) / sum(found_m_full), NA_real_)
  med_local_all_full           <- stats::median(max_local_full[is.finite(max_local_full)], na.rm = TRUE)
  p90_local_all_full           <- stats::quantile(max_local_full[is.finite(max_local_full)], 0.9, na.rm = TRUE, names = FALSE)

  metrics_full <- tibble::tibble(
    alpha = alpha, tau = tau, k_window = k_window, topK = topK,
    repl_per_1k = repl_per_1k_full, share_lines = share_lines_full,
    median_score = med_local_all_full, p90_score = p90_local_all_full,
    total_tokens = total_tokens_full, sample_n = n,
    acr_per_1k = acr_per_1k_full, share_lines_with_acr = share_lines_with_acr_full,
    repl_per_1k_given_found = repl_per_1k_given_found_full
  )
  .save_rds_meta(metrics_full, out_metrics_full,
                 list(stage="stage8_expand_full_metrics", created_at=Sys.time(),
                      params=list(alpha=alpha, tau=tau, k_window=k_window, topK=topK)))

  # Optional: small preview sample for eyeballing full results
  if (is.finite(preview_full_n) && preview_full_n > 0L) {
    keep <- if (n > preview_full_n) sort(sample.int(n, preview_full_n)) else seq_len(n)
    preview_full <- tibble::tibble(
      row_id     = keep,
      raw        = texts_for_expansion[keep],
      expanded   = expanded_description[keep],
      num_repl   = num_expanded_tokens[keep],
      found_acr  = found_m_full[keep],
      best_local = best_local_score_any[keep]
    )
    .save_rds_meta(preview_full, out_preview_full,
                   list(stage="stage8_expand_full_preview", created_at=Sys.time(),
                        params=list(alpha=alpha, tau=tau, k_window=k_window, topK=topK),
                        sample_n = length(keep)))
  }

  out <- txtdf %>%
    dplyr::mutate(
      expanded_description = expanded_description,
      acronyms_found       = acronyms_found,
      num_expanded_tokens  = num_expanded_tokens,
      mean_local_score     = mean_local_score,      # > tau only
      best_local_score_any = best_local_score_any,  # observed, incl. < tau
      ambiguity_flag       = ambiguity_flag
    ) %>%
    dplyr::select(-.row_id, -raw_text)

  .save_rds_meta(out, out_full,
                 list(stage="stage8_expand_full", created_at=Sys.time(),
                      params=list(alpha=alpha, tau=tau, k_window=k_window, topK=topK),
                      workers_used = workers))
  invisible(out)
}


```

```{r}



proc_dir <- file.path(here(), "Data", "processed")
if (!dir_exists(proc_dir)) dir_create(proc_dir, recurse = TRUE)
.build_log_path <- file.path(proc_dir, "build_log.txt")

.log <- function(...) {
  ts <- format(Sys.time(), "%Y-%m-%d %H:%M:%S")
  msg <- paste(..., collapse = " ")
  line <- paste0(ts, " | ", msg, "\n")
  cat(line)
  try(write(line, file = .build_log_path, append = TRUE), silent = TRUE)
  invisible(NULL)
}

.timeit <- function(label, expr) {
  .log(label, "START")
  t0 <- proc.time()
  val <- force(expr)
  dt <- proc.time() - t0
  sec <- as.numeric(dt["user.self"] + dt["sys.self"])
  .log(label, "DONE", sprintf("(%.1f sec)", sec))
  invisible(val)
}

run_build_once <- function() {
  .timeit("stage1_clean_flags", stage1_clean_flags(subcontracts))
  .timeit("stage2_load_dau",    stage2_load_dau())
  .timeit("stage3_mine_pairs",  stage3_mine_pairs())
  .timeit("stage4_counts_features", stage4_counts_features())
  .timeit("stage5_canon_jw",    stage5_canon_jw())
  .timeit("stage6_score_candidates", stage6_score_candidates())
  .timeit("stage7_embeddings_frozen", stage7_embeddings_frozen())
  .log("BUILD_COMPLETE")
  invisible(TRUE)
}

run_expand_tune <- function(alpha = 0.8, tau = 0.55, k_window = 12, topK = 2L, workers = 6) {
  p <- list(alpha = alpha, tau = tau, k_window = k_window, topK_per_acr = topK)
  .timeit(
    sprintf("stage8_expand[TUNE] a=%.2f tau=%.2f k=%d topK=%d", alpha, tau, k_window, topK),
    stage8_expand(mode = "tune", params = p, workers = workers)
  )
}

run_expand_full <- function(alpha = 0.8, tau = 0.50, k_window = 12, topK = 2L, workers = 6) {
  p <- list(alpha = alpha, tau = tau, k_window = k_window, topK_per_acr = topK)
  .timeit(
    sprintf("stage8_expand[FULL] a=%.2f tau=%.2f k=%d topK=%d", alpha, tau, k_window, topK),
    stage8_expand(mode = "full", params = p, workers = workers)
  )
}


print_stage_sizes <- function() {
  p <- function(x) file.path(proc_dir, x)
  fmt <- function(x) if (file.exists(x)) file_info(x)$size else NA
  sizes <- tibble::tibble(
    file = c(
      "subcontracts_with_flags.rds",
      "acr_dict_dau.rds",
      "hi_conf_pairs.rds",
      "dict_all_raw.rds",
      "feat_tbl.rds",
      "dict_all.rds",
      "exp_map.rds",
      "acr_candidates.rds",
      "acr_lexicon.rds",
      "w2v_contracts_matrix.rds",
      "sample_idx.rds",
      "expanded_sample_metrics.rds",
      "expanded_sample_preview.rds",
      "subcontracts_expanded.rds"
    ),
    size_bytes = vapply(file.path(proc_dir, file), fmt, numeric(1))
  )
  print(sizes, n = nrow(sizes))
  invisible(sizes)
}

progress_expand_tune_seq <- function(alpha = 0.6, tau = 0.55, k_window = 8, topK = 2L) {
  txtdf <- readRDS(file.path(proc_dir,"subcontracts_with_flags.rds"))
  if (file.exists(file.path(proc_dir,"sample_idx.rds"))) {
    idx <- readRDS(file.path(proc_dir,"sample_idx.rds"))
  } else {
    set.seed(42)
    idx <- sort(sample(seq_len(nrow(txtdf)), min(10000L, nrow(txtdf))))
    write_rds(idx, file.path(proc_dir,"sample_idx.rds"))
  }
  chunks <- split(idx, cut(seq_along(idx), 20, labels = FALSE))
  p <- list(alpha = alpha, tau = tau, k_window = k_window, topK_per_acr = topK)
  .log("TUNE_SEQ START", sprintf("chunks=%d", length(chunks)))
  for (i in seq_along(chunks)) {
    .log(sprintf("chunk %d/%d", i, length(chunks)))
    invisible(
      stage8_expand(
        mode = "tune",
        params = p,
        workers = 1,
        sample_idx_path = file.path(proc_dir, sprintf("sample_idx_chunk%02d.rds", i))
      )
    )
  }
  .log("TUNE_SEQ DONE")
  invisible(TRUE)
}

options(wordvector_threads = 6) 

stage8_debug_snapshot <- function(
  cand_path = file.path(proc_dir, "acr_candidates.rds"),
  K = 2L, head_n = 25L
) {
  cand <- readRDS(cand_path)
  acr_tbl <- cand %>%
    dplyr::arrange(dplyr::desc(score)) %>%
    dplyr::group_by(acr) %>%
    dplyr::slice_head(n = K) %>%
    dplyr::ungroup()

  list(
    n_unique_acr = dplyr::n_distinct(acr_tbl$acr),
    example = dplyr::as_tibble(acr_tbl)[1:min(nrow(acr_tbl), head_n), c("acr","exp_uc","score")]
  )
}



```

```{r}


.stage_outputs <- list(
  stage1     = c("subcontracts_with_flags.rds"),
  stage2     = c("acr_dict_dau.rds"),
  stage3     = c("hi_conf_pairs.rds"),
  stage4     = c("dict_all_raw.rds","feat_tbl.rds"),
  stage5     = c("dict_all.rds","exp_map.rds"),
  stage6     = c("acr_candidates.rds","acr_lexicon.rds"),
  stage7     = c("w2v_contracts_matrix.rds","w2v_vocab.rds"),
  stage8_tune= c("expanded_sample_metrics.rds","expanded_sample_preview.rds"),
  stage8_full= c("subcontracts_expanded.rds")
)
.stage_order <- c("stage1","stage2","stage3","stage4","stage5","stage6","stage7","stage8_tune","stage8_full")


.stage_funcs <- list(
  stage1      = function(...) stage1_clean_flags(subcontracts),
  stage2      = function(...) stage2_load_dau(...),
  stage3      = function(...) stage3_mine_pairs(...),
  stage4      = function(...) stage4_counts_features(...),
  stage5      = function(...) stage5_canon_jw(...),
  stage6      = function(...) stage6_score_candidates(...),
  stage7      = function(...) stage7_embeddings_frozen(...),
  stage8_tune = function(...) stage8_expand(mode="tune", ...),
  stage8_full = function(...) stage8_expand(mode="full", ...)
)


.timeit_safe <- get(".timeit", envir=.GlobalEnv, inherits=TRUE)
if (!is.function(.timeit_safe)) {
  .timeit_safe <- function(label, expr) { message(sprintf("[%s] ...", label)); force(expr) }
}


stage_status <- function() {
  stopifnot(exists("proc_dir", inherits = TRUE))
  tibble::tibble(
    stage = .stage_order,
    files = vapply(.stage_outputs[.stage_order], function(v) paste(v, collapse=", "), character(1)),
    present = vapply(.stage_outputs[.stage_order], function(v) all(file.exists(file.path(proc_dir, v))), logical(1))
  )
}


reset_from <- function(from = "stage3") {
  stopifnot(exists("proc_dir", inherits = TRUE))
  i <- match(from, .stage_order); if (is.na(i)) stop("Unknown stage: ", from)
  files <- unlist(.stage_outputs[.stage_order[i:length(.stage_order)]], use.names = FALSE)
  paths <- file.path(proc_dir, files)
  existed <- file.exists(paths)
  if (any(existed)) unlink(paths[existed])
  invisible(paths[existed])
}


run_from <- function(from = "stage3", thru = "stage7", mode8 = c("none","tune","full"), ...) {
  mode8 <- match.arg(mode8)
  i <- match(from, .stage_order); if (is.na(i)) stop("Unknown 'from' stage: ", from)
  j <- match(thru, .stage_order); if (is.na(j)) stop("Unknown 'thru' stage: ", thru)
  if (j < i) stop("'thru' must be >= 'from'")

  for (k in i:j) {
    st <- .stage_order[k]
    fn <- .stage_funcs[[st]]
    if (is.function(fn)) .timeit_safe(st, fn(...))
  }
  if (mode8 == "tune") .timeit_safe("stage8_tune", .stage_funcs$stage8_tune(...))
  if (mode8 == "full") .timeit_safe("stage8_full", .stage_funcs$stage8_full(...))
  invisible(TRUE)
}


```

```{r}
# Run Example 

# reset_from("stage5")
# run_from("stage5", thru = "stage6")   
# run_expand_full(alpha = 0.6, tau = 0.55, k_window = 8, topK = 2L, workers = 6)


reset_from("stage1")
run_build_once()
run_expand_full(alpha = 0.8, tau = 0.53, k_window = 12, topK = 2L, workers = 6)



```

```{r}
library(tidyverse)
# pick tau 

proc_dir <- file.path(here(), "Data", "processed")
prev <- readRDS(file.path(proc_dir, "expanded_full_preview.rds"))

stopifnot("best_local" %in% names(prev))


taus <- seq(0, 1, by = 0.01)
curve_df <- tibble(
  tau = taus,
  share_above = map_dbl(taus, ~ mean(prev$best_local > .x, na.rm = TRUE))
)


tau_current <- 0.53

ggplot(curve_df, aes(tau, share_above)) +
  geom_line() +
  geom_vline(xintercept = tau_current, linetype = 2) +
  labs(title = "Operating curve: share of candidates above τ",
       x = "τ (gate)", y = "Pr(score > τ)")


pick_tau_for_share <- function(target_share = 0.05) {
  i <- which.min(abs(curve_df$share_above - target_share))
  curve_df$tau[i]
}


tok_counts <- strsplit(prev$raw, "\\s+") |> lengths()
total_tokens <- sum(tok_counts, na.rm = TRUE)
repl_per_1k_given_tau <- function(tau) {
  # we approximate 'will replace' as (best_local > tau)
  reps <- sum(prev$best_local > tau, na.rm = TRUE)
  if (total_tokens == 0) return(NA_real_)
  1000 * reps / total_tokens
}


grid <- tibble(
  tau = seq(0.35, 0.75, by = 0.05)
) |>
  mutate(
    share_lines = map_dbl(tau, ~ mean(prev$best_local > .x, na.rm = TRUE)),
    repl_per_1k = map_dbl(tau, repl_per_1k_given_tau)
  )
print(grid)


pick_tau_for_repl_per_1k <- function(target_rpk = 5) {
  taus <- seq(0, 1, by = 0.005)
  vals <- vapply(taus, repl_per_1k_given_tau, numeric(1))
  taus[which.min(abs(vals - target_rpk))]
}

suggest_tau_by_share <- pick_tau_for_share(0.05)         # 
suggest_tau_by_rpk   <- pick_tau_for_repl_per_1k(5)      # 

cat(sprintf("Suggested τ (5%% lines): %.3f\n", suggest_tau_by_share))
cat(sprintf("Suggested τ (~5 repl/1k): %.3f\n", suggest_tau_by_rpk))




```

```{r}

```
