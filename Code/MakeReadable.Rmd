---
title: "MakeReadable"
output: html_document
date: "2025-08-27"
---

```{r}


library(digest)
library(dplyr)
library(fs)
library(here)
library(httr2)
library(jsonlite)
library(purrr)
library(readr)
library(readxl)
library(stringdist)
library(stringr)
library(tibble)
library(tidyr)

options(scipen = 999)

```

DAU Dictionary

```{r}

dict_path <- here("Data", "acronymdict.RDS")
xlsx_path <- here("Data", "acronymdict.xlsx")

if (!file.exists(dict_path)) {
  raw <- read_xlsx(xlsx_path, col_names = FALSE)
  colname <- names(raw)[which.max(colSums(!is.na(raw)))]
  lines <- raw[[colname]] |> as.character() |> str_squish()
  lines <- lines[!is.na(lines) & nzchar(lines)]

  split_acr <- function(txt) {
    m <- regexec("^(\\S{2,15})\\s+(.+)$", txt); r <- regmatches(txt, m)[[1]]
    if (length(r) == 3) data.frame(acr = toupper(trimws(r[2])), exp = trimws(r[3])) 
    else data.frame(acr = NA, exp = NA)
  }

  acr_dict_dau <- bind_rows(lapply(lines, split_acr)) |>
    filter(!is.na(acr), !is.na(exp), nzchar(acr), nzchar(exp)) |>
    mutate(exp = str_split(exp, ";")) |>
    unnest(exp) |>
    mutate(exp = str_squish(exp)) |>
    filter(exp != "") |>
    distinct(acr, exp)

  saveRDS(acr_dict_dau, dict_path)
} else {
  acr_dict_dau <- readRDS(dict_path)
}

```
 
Mined Dictionary

```{r}

texts <- subcontract$subaward_description

# ---- Builder functions ----
catalog_markers <- c("PN","NSN","CLIN","NIIN","SDRL","FAI","MTBF","BOM","EOL","CAGE","P/N","S/N")
connectors <- c("AND","OF","THE","FOR","IN","ON","WITH","TO","BY","AT","FROM","A","AN")
JW_THRESH <- 0.93

looks_like_acronym <- function(acr) {
  acr <- toupper(as.character(acr))
  ok_shape       <- grepl("^[A-Z0-9][A-Z0-9&\\.\\-]{1,9}$", acr)
  enough_letters <- stringr::str_count(acr, "[A-Z]") >= 2
  ok <- ok_shape & enough_letters
  ok[is.na(ok)] <- FALSE
  ok
}

normalize_phrase <- function(x) {
  x %>%
    toupper() %>%
    str_replace_all("&", " AND ") %>%
    str_replace_all("[ ]*/[ ]*", "/") %>%
    str_replace_all("[^A-Z0-9/ \\-]", " ") %>%
    str_replace_all("\\s+", " ") %>%
    str_squish()
}

initials_from_phrase_vec <- function(phr, drop_words = connectors) {
  p <- normalize_phrase(phr)
  toks <- str_split(p, "\\s+")
  n <- length(toks)
  out_init <- character(n); out_ntok <- integer(n)
  for (i in seq_len(n)) {
    ti <- toks[[i]]
    ti <- ti[!(ti %in% drop_words)]
    ti <- ti[nzchar(ti)]
    out_ntok[i] <- length(ti)
    out_init[i] <- if (length(ti)) paste0(substr(ti, 1, 1), collapse = "") else ""
  }
  list(init = out_init, n_tokens = out_ntok)
}

strip_leading_connectors <- function(x) {
  x %>% toupper() %>% str_squish() %>% str_replace("^(?:AND THE|AND|OR|THE)\\s+", "")
}

normalize_exp_display <- function(x) {
  x %>%
    toupper() %>%
    str_replace_all("[ ]*/[ ]*", " / ") %>%
    str_replace_all("[ ]*-[ ]*", "-") %>%
    str_replace_all("\\s+", " ") %>%
    str_squish()
}


pat_exp_acr <- "(?<exp>[A-Za-z][A-Za-z0-9/&\\-\\. ,]+?)\\s*\\((?<acr>[A-Z0-9][A-Z0-9&\\.\\-]{1,9})\\)"
pat_acr_exp <- "(?<acr>[A-Z0-9][A-Z0-9&\\.\\-]{1,9})\\s*\\((?<exp>[A-Za-z][A-Za-z0-9/&\\-\\. ,]+?)\\)"

extract_pairs_one <- function(txt, doc_id) {
  m1 <- str_match_all(txt, pat_exp_acr)[[1]]
  df1 <- if (nrow(m1)) tibble(acr = m1[, "acr"], exp = m1[, "exp"], pattern = "longform_paren", doc_id = doc_id) else tibble(acr=character(),exp=character(),pattern=character(),doc_id=integer())
  m2 <- str_match_all(txt, pat_acr_exp)[[1]]
  df2 <- if (nrow(m2)) tibble(acr = m2[, "acr"], exp = m2[, "exp"], pattern = "acr_paren_longform", doc_id = doc_id) else tibble(acr=character(),exp=character(),pattern=character(),doc_id=integer())
  bind_rows(df1, df2)
}

mine_acronym_pairs <- function(texts) {
  cand <- map2_dfr(texts, seq_along(texts), extract_pairs_one) %>%
    mutate(acr = toupper(str_squish(acr)),
           exp = str_squish(exp)) %>%
    filter(nchar(acr) >= 2, nchar(exp) >= 3, !acr %in% catalog_markers)

  if (nrow(cand) == 0) return(cand)

  iv <- initials_from_phrase_vec(cand$exp)
  cand$init     <- iv$init
  cand$n_tokens <- iv$n_tokens

  cand %>%
    filter(
      looks_like_acronym(acr),
      n_tokens >= nchar(acr),
      init == acr
    ) %>%
    distinct(acr, exp, doc_id, pattern)
}

# Aggregating collapse
collapse_within_acr_jw_agg <- function(df, jw_thresh = JW_THRESH) {
  if (!nrow(df)) return(list(
    dict = tibble(acr=character(), exp_uc=character(), freq=integer()),
    map  = tibble(acr=character(), exp_uc_raw=character(), exp_uc_canon=character())
  ))
  df2 <- df %>%
    mutate(
      exp    = strip_leading_connectors(as.character(exp)),
      exp    = normalize_exp_display(exp),
      exp_uc = toupper(exp),
      freq   = coalesce(as.integer(freq), 1L)
    ) %>%
    filter(nzchar(exp_uc)) %>%
    arrange(desc(freq), desc(nchar(exp_uc))) %>%
    distinct(exp_uc, .keep_all = TRUE)

  n <- nrow(df2); if (n <= 1) return(list(
    dict = tibble(acr=df2$acr, exp_uc=df2$exp_uc, freq=df2$freq),
    map  = tibble(acr=df2$acr, exp_uc_raw=df2$exp_uc, exp_uc_canon=df2$exp_uc)
  ))

  S <- 1 - stringdistmatrix(df2$exp_uc, df2$exp_uc, method="jw"); diag(S) <- 1
  comp <- rep(NA_integer_, n); cid <- 0L
  for (i in seq_len(n)) if (is.na(comp[i])) {
    cid <- cid + 1L; q <- i; comp[i] <- cid
    while (length(q)) {
      v <- q[1]; q <- q[-1]
      neigh <- which(S[v, ] >= jw_thresh & is.na(comp))
      if (length(neigh)) { comp[neigh] <- cid; q <- c(q, neigh) }
    }
  }
  clusters <- split(seq_len(n), comp)

  dict <- map_dfr(clusters, function(ix){
    pick <- ix[order(-df2$freq[ix], -nchar(df2$exp_uc[ix]))][1]
    tibble(acr=df2$acr[pick], exp_uc=df2$exp_uc[pick], freq=sum(df2$freq[ix]))
  })
  map <- map_dfr(clusters, function(ix){
    pick <- ix[order(-df2$freq[ix], -nchar(df2$exp_uc[ix]))][1]
    tibble(acr=df2$acr[ix], exp_uc_raw=df2$exp_uc[ix], exp_uc_canon=df2$exp_uc[pick])
  })
  list(dict=dict, map=map)
}




# Run mining 
hi_conf_pairs <- mine_acronym_pairs(texts)

dict_all_raw <- hi_conf_pairs %>%
  mutate(exp = normalize_exp_display(exp)) %>%
  dplyr::count(acr, exp, name = "freq", sort = TRUE)

# Collapse within acronym, aggregating counts + create raw->canonical map
by_acr <- split(dict_all_raw, dict_all_raw$acr)
res    <- map(by_acr, ~ collapse_within_acr_jw_agg(.x, jw_thresh = JW_THRESH))
dict_all <- bind_rows(map(res, "dict"))    # (acr, exp_uc, freq) canonical with SUMMED freq
exp_map  <- bind_rows(map(res, "map"))     # (acr, exp_uc_raw -> exp_uc_canon)

# Canonical pair features from mined pairs (doc coverage + definitional hits)
feat_tbl <- hi_conf_pairs %>%
  mutate(exp_uc_raw = toupper(normalize_exp_display(exp))) %>%
  left_join(exp_map, by = c("acr","exp_uc_raw")) %>%
  mutate(exp_uc = coalesce(exp_uc_canon, exp_uc_raw)) %>%
  distinct(acr, exp_uc, doc_id, pattern) %>%
  group_by(acr, exp_uc) %>%
  summarise(
    doc_coverage = n_distinct(doc_id),
    def_hits     = sum(!is.na(pattern) & pattern %in% c("longform_paren","acr_paren_longform")),
    .groups = "drop"
  )

```

Merge

```{r}


ACRONYM_KEEP <- c("DoD","USAF","USN","USMC","USA","AFOTEC","JSF","COTS","F-35","MTBF","BOM",
                  "SDRL","NSN","CLIN","NIIN","CAGE","ISR","GPS","RF","IT","AI","UAS","UAV","SATCOM")
SMALL_WORDS <- c("and","or","of","the","for","in","on","with","to","by","at","from","a","an")

normalize_spaces <- function(x) { x %>% str_replace_all("\\s+", " ") %>% str_squish() }
title_token <- function(tok) {
  if (tok %in% ACRONYM_KEEP) return(tok)
  if (grepl("\\d", tok)) return(toupper(tok))
  if (nchar(tok) <= 4 && grepl("^[A-Z]+$", tok)) return(tok)
  split_and_cap <- function(s, sep) {
    parts <- strsplit(s, sep, fixed = TRUE)[[1]]
    parts <- ifelse(nchar(parts),
                    paste0(str_to_upper(substr(parts,1,1)), str_to_lower(substr(parts,2,nchar(parts)))),
                    parts)
    paste(parts, collapse = sep)
  }
  tok <- split_and_cap(tok, "-"); tok <- split_and_cap(tok, "/"); tok
}
pretty_case <- function(x) {
  if (is.na(x) || !nzchar(x)) return(x)
  x0 <- normalize_spaces(x); toks <- strsplit(x0, " +")[[1]]
  if (!length(toks)) return(x0)
  out <- character(length(toks))
  for (i in seq_along(toks)) {
    b <- toks[i]
    if (tolower(b) %in% SMALL_WORDS && i != 1 && i != length(toks)) out[i] <- tolower(b) else out[i] <- title_token(b)
  }
  out <- ifelse(toupper(out) %in% toupper(ACRONYM_KEEP),
                ACRONYM_KEEP[match(toupper(out), toupper(ACRONYM_KEEP))],
                out)
  paste(out, collapse = " ")
}
pretty_case_vec <- function(x) vapply(x, pretty_case, character(1))


acr_mined_all <- dict_all %>%
  mutate(
    exp_disp = pretty_case_vec(exp_uc),
    source   = "mined"
  ) %>%
  left_join(feat_tbl, by = c("acr","exp_uc")) %>%
  mutate(
    freq         = coalesce(freq, 0L),   # canonical pair frequency across corpus
    doc_coverage = coalesce(doc_coverage, 0L),
    def_hits     = coalesce(def_hits, 0L)
  )


if (exists("acr_dict_dau")) {
  acr_dau_enriched <- acr_dict_dau %>%
    mutate(
      acr      = toupper(str_squish(acr)),
      exp_uc   = toupper(normalize_exp_display(exp)),
      exp_disp = pretty_case_vec(exp),
      source   = "DAU"
    ) %>%
    filter(nzchar(acr), nzchar(exp_uc)) %>%
    distinct(acr, exp_uc, .keep_all = TRUE) %>%
    left_join(select(dict_all, acr, exp_uc, freq), by = c("acr","exp_uc")) %>%     # canonical pair freq
    left_join(feat_tbl, by = c("acr","exp_uc")) %>%                                # canonical features
    mutate(
      freq         = coalesce(freq, 0L),
      doc_coverage = coalesce(doc_coverage, 0L),
      def_hits     = coalesce(def_hits, 0L)
    )

  acr_all_sources <- bind_rows(acr_mined_all, acr_dau_enriched) %>%
    distinct(acr, exp_uc, .keep_all = TRUE)
} else {
  acr_all_sources <- acr_mined_all
}

# Combined dictionary (for meta)
dict_mined <- dict_all %>% select(acr, exp_uc) %>% distinct()
dict_dau <- if (exists("acr_dict_dau")) {
  acr_dict_dau %>%
    mutate(acr = toupper(str_squish(acr)),
           exp_uc = toupper(normalize_exp_display(exp))) %>%
    filter(nzchar(acr), nzchar(exp_uc)) %>%
    distinct(acr, exp_uc)
} else tibble(acr = character(), exp_uc = character())
dict_union <- bind_rows(dict_mined, dict_dau) %>% distinct(acr, exp_uc)



```

Scoring

```{r}


w_def <- 2.5; w_freq <- 1.0; w_doc <- 0.8; w_auth <- 1.2
acr_candidates <- acr_all_sources %>%
  mutate(auth_hits = if_else(source == "DAU", 1L, 0L)) %>%
  group_by(acr) %>%
  mutate(
    score_raw = w_def*def_hits + w_freq*log1p(freq) + w_doc*log1p(doc_coverage) + w_auth*auth_hits,
    score     = (score_raw - min(score_raw, na.rm = TRUE)) /
                (max(score_raw, na.rm = TRUE) - min(score_raw, na.rm = TRUE) + 1e-9)
  ) %>%
  arrange(desc(score), .by_group = TRUE) %>%
  mutate(rank = row_number()) %>%
  ungroup()

margin <- 0.15
acr_lexicon <- acr_candidates %>%
  group_by(acr) %>%
  summarise(
    best        = first(exp_disp),
    best_uc     = first(exp_uc),
    confidence  = first(score),
    alt         = if (n() >= 2) nth(exp_disp, 2) else NA_character_,
    alt_score   = if (n() >= 2) nth(score, 2) else NA_real_,
    keep_top2   = !is.na(alt_score) & (confidence - alt_score) < margin,
    .groups = "drop"
  )

# META VARS

# (1) # unique canonical expansions per acronym in combined dictionary (dictionary-only)
acr_dict_freq <- dict_union %>% dplyr::count(acr, name = "acr_dict_freq")

# (2) & (3) corpus counts per acronym
texts_upper <- toupper(texts)
regex_escape <- function(x) gsub("([\\^$.|?*+(){}\\[\\]\\\\])", "\\\\\\1", x)
make_pat <- function(acr) paste0("(?<![A-Z0-9])", regex_escape(acr), "(?![A-Z0-9])")

acrs <- sort(unique(dict_union$acr))
counts <- lapply(acrs, function(a){
  pat  <- make_pat(a)
  occ  <- sum(str_count(texts_upper, pat))   # total occurrences in corpus
  docs <- sum(str_detect(texts_upper, pat))  # # of docs containing it
  c(acr = a, acr_corpus_freq = occ, doc_cov_acr = docs)
})
acr_counts <- as_tibble(do.call(rbind, counts)) %>%
  mutate(acr_corpus_freq = as.integer(acr_corpus_freq),
         doc_cov_acr     = as.integer(doc_cov_acr))

acr_meta <- acr_dict_freq %>%
  full_join(acr_counts, by = "acr") %>%
  mutate(
    acr_dict_freq   = coalesce(acr_dict_freq, 0L),
    acr_corpus_freq = coalesce(acr_corpus_freq, 0L),
    doc_cov_acr     = coalesce(doc_cov_acr, 0L)
  )

# Attach meta to pair-level dictionary and 1-per-acr lexicon
acr_all_sources <- acr_all_sources %>% left_join(acr_meta, by = "acr")
acr_lexicon     <- acr_lexicon     %>% left_join(acr_meta, by = "acr")




# EXPANDER

expand_with_lexicon <- function(text, lex, tau = 0.4){
  if (!nrow(lex)) return(text)
  vapply(text, function(t){
    parts <- strsplit(t, "\\b")[[1]]
    for (i in seq_along(parts)){
      w <- parts[i]
      if (grepl("^[A-Z]{2,10}$", w) && !grepl("^\\d+$", w)){
        j <- match(w, lex$acr)
        if (!is.na(j) && is.finite(lex$confidence[j]) && lex$confidence[j] >= tau){
          parts[i] <- paste0(lex$best[j], " (", w, ")")
        }
      }
    }
    paste(parts, collapse = "")
  }, character(1))
}


texts_expanded <- expand_with_lexicon(texts, acr_lexicon, tau = 0.4)


glimpse(subcontract)
```




```{r}


make_psc_query <- function(df,
                           subawardee_col = "subawardee_name",
                           raw_desc_col  = "subaward_description",
                           expanded_col  = NULL,
                           max_chars     = 250) {
  
  desc_col <- if (!is.null(expanded_col) && expanded_col %in% names(df)) {
    expanded_col
  } else if ("desc_expanded" %in% names(df)) {
    "desc_expanded"
  } else {
    raw_desc_col
  }
  
  base <- df %>%
    mutate(.doc_id = row_number(),
           .subawardee = .data[[subawardee_col]] %>% as.character() %>% coalesce(""),
           .desc = .data[[desc_col]] %>% as.character() %>% coalesce("")) %>%
    mutate(.desc = str_squish(.desc))
  
  # Break into sentences
  sent_df <- base %>%
    mutate(.sent_list = str_split(.desc, boundary("sentence"))) %>%
    select(.doc_id, .subawardee, .sent_list) %>%
    unnest_longer(.sent_list, values_to = ".sentence", keep_empty = TRUE) %>%
    mutate(.sentence = str_squish(.sentence))
  
  # Tokenize and compute tf-idf
  word_df <- sent_df %>%
    filter(nchar(.sentence) > 0) %>%
    unnest_tokens(word, .sentence, token = "words", drop = FALSE) %>%
    filter(!word %in% stop_words$word, !str_detect(word, "^[0-9]+$")) %>%
    group_by(.doc_id, .sentence) %>%
    dplyr::count(word, name = "term_n") %>%
    ungroup() %>%
    bind_tf_idf(word, .doc_id, term_n)
  
  # Sentence scores
  sent_scores <- word_df %>%
    group_by(.doc_id, .sentence) %>%
    summarise(score = sum(tf_idf, na.rm = TRUE), .groups = "drop")
  
  # Pick top text per doc
  best_text <- sent_df %>%
    left_join(sent_scores, by = c(".doc_id", ".sentence")) %>%
    mutate(score = coalesce(score, 0)) %>%
    group_by(.doc_id) %>%
    arrange(desc(score), .sentence, .by_group = TRUE) %>%
    summarise(.best = paste(.sentence[nchar(.sentence) > 0], collapse = " "),
              .subawardee = first(.subawardee),
              .groups = "drop") %>%
    mutate(.best = ifelse(is.na(.best), "", str_squish(.best)))
  
  out <- best_text %>%
    mutate(.prefix = ifelse(nchar(.subawardee) > 0, paste0(.subawardee, ": "), ""),
           .room   = pmax(0, max_chars - nchar(.prefix)),
           .body   = ifelse(nchar(.best) > 0, str_sub(.best, 1L, .room), ""),
           psc_query_250 = str_squish(paste0(.prefix, .body))) %>%
    select(.doc_id, psc_query_250)
  
  # Fallback: truncate raw concat
  fallback <- base %>%
    transmute(.doc_id,
              fallback_text = str_squish(paste0(.subawardee,
                                                ifelse(nchar(.subawardee) > 0, ": ", ""),
                                                .desc)) %>% str_sub(1L, max_chars))
  
  final <- out %>%
    left_join(fallback, by = ".doc_id") %>%
    mutate(psc_query_250 = ifelse(nchar(psc_query_250) == 0, fallback_text, psc_query_250)) %>%
    select(.doc_id, psc_query_250)
  
  df %>%
    mutate(.doc_id = row_number()) %>%
    left_join(final, by = ".doc_id") %>%
    select(-.doc_id)
}


dat <- make_psc_query(subcontract,
                      subawardee_col = "subawardee_name",
                      raw_desc_col   = "subaward_description")
head(dat$psc_query_250)


vec <- dat %>% 
    select(psc_query_250)

```


API Queries

```{r}


`%||%` <- function(a, b) { if (is.null(a)) return(b); if (length(a)==0L) return(b); if (is.atomic(a) && length(a)==1L && is.na(a)) return(b); a }

FSCPSC_API  <- "https://api.fscpsc.com/searches"
FSCPSC_MIME <- "application/vnd.api+json"


.rel_score_map_from_search <- function(jj, rel_key = "product-service-codes") {
  rel <- jj$data$relationships[[rel_key]]
  if (!is.list(rel)) return(tibble(code=character(), score=double()))
  dat <- rel$data %||% list()
  if (!length(dat)) return(tibble(code=character(), score=double()))
  rows <- lapply(dat, function(x) {
    meta <- x$meta %||% list()
    assoc <- if (!is.null(meta$association)) suppressWarnings(as.numeric(meta$association)) else NA_real_
    tibble(code = toupper(as.character(x$id %||% "")), score = assoc)
  })
  bind_rows(rows) %>% distinct(code, .keep_all = TRUE)
}

.rel_score_map_from_included <- function(jj, want_type = "product-service-codes", back_rel = "searches") {
  inc <- jj$included %||% list()
  if (!length(inc)) return(tibble(code=character(), score=double()))
  items <- inc[vapply(inc, function(z) is.list(z) && identical(z$type, want_type), logical(1))]
  if (!length(items)) return(tibble(code=character(), score=double()))
  rows <- lapply(items, function(x) {
    code <- toupper(as.character(x$id %||% ""))
    rel  <- x$relationships[[back_rel]]
    if (!is.list(rel)) return(tibble(code=character(), score=double()))
    dat  <- rel$data %||% list()
    if (!length(dat)) return(tibble(code=character(), score=double()))
    m <- dat[[1]]$meta %||% list()
    assoc <- if (!is.null(m$association)) suppressWarnings(as.numeric(m$association)) else NA_real_
    tibble(code = code, score = assoc)
  })
  bind_rows(rows) %>% distinct(code, .keep_all = TRUE)
}

parse_fscpsc_jsonapi <- function(jj, want_type = "product-service-codes") {
  inc <- jj$included %||% list()
  items <- inc[vapply(inc, function(x) is.list(x) && identical(x$type, want_type), logical(1))]
  if (!length(items)) return(tibble(code=character(), name=character(), score=double(), type=character()))
  rows <- lapply(items, function(x) {
    code <- toupper(as.character(x$id %||% x$attributes$id %||% ""))
    name <- as.character(x$attributes$name %||% x$attributes$title %||% x$attributes$`full-name` %||% NA_character_)
    tibble(code = code, name = name, type = want_type)
  }) %>% bind_rows() %>% filter(nchar(code) == 4) %>% distinct(code, .keep_all = TRUE)

  s1 <- .rel_score_map_from_search(jj, rel_key = want_type)
  s2 <- .rel_score_map_from_included(jj, want_type = want_type, back_rel = "searches")
  smap <- s1 %>% full_join(s2, by = "code", suffix = c("_a","_b")) %>% transmute(code, score = coalesce(score_a, score_b))

  rows %>% left_join(smap, by = "code") %>% arrange(desc(score), code)
}

# ---- single-call with retry, backoff, & 429 handling ----
fscpsc_search_psc <- function(query, top_k = 5, timeout_sec = 25, max_retries = 4) {
  q <- as.character(query %||% "")
  if (!nzchar(q)) return(tibble(code=character(), name=character(), score=double(), rank=integer()))
  attempt <- 0
  repeat {
    attempt <- attempt + 1
    req <- request(FSCPSC_API) |>
      req_url_query(include = "product-service-codes") |>
      req_headers("Accept" = FSCPSC_MIME, "Content-Type" = FSCPSC_MIME) |>
      req_body_json(list(data=list(type="searches", attributes=list(`search-string`=q))), auto_unbox=TRUE) |>
      req_timeout(timeout_sec)
    resp <- try(req_perform(req), silent = TRUE)

    # success
    if (inherits(resp,"httr2_response") && resp_status(resp) %in% c(200,201)) {
      jj  <- resp_body_json(resp, simplifyVector = FALSE)
      out <- parse_fscpsc_jsonapi(jj, want_type = "product-service-codes")
      if (nrow(out)) out <- out %>% mutate(rank = row_number()) %>% slice_head(n = top_k)
      return(out)
    }

    # 429 rate limit: wait according to headers or exponential backoff
    if (inherits(resp,"httr2_response") && resp_status(resp) == 429) {
      reset <- as.numeric(resp_header(resp, "X-RateLimit-Reset") %||% NA_real_)
      # if header exists and is reasonable, sleep that many seconds; else exponential backoff
      wait_s <- if (is.finite(reset) && reset > 0 && reset < 60) reset else min(2^(attempt-1), 30)
      Sys.sleep(wait_s + runif(1, 0, 0.5))
    } else {
      # other errors: backoff and retry
      if (attempt >= max_retries) return(tibble(code=character(), name=character(), score=double(), rank=integer()))
      Sys.sleep(min(2^(attempt-1), 8) + runif(1, 0, 0.2))
    }
  }
}

# Build a stable key for each row (hash of key columns, or fallback to row_number)
make_psc_keys <- function(df, key_cols = NULL, query_col = NULL) {
  if (is.null(query_col)) stop("Provide query_col (the 250-char string you send to the API).")
  if (!query_col %in% names(df)) stop("query_col not found in df.")
  if (is.null(key_cols)) {
    df %>% mutate(.psc_key = sprintf("ROW%07d", dplyr::row_number()))
  } else {
    df %>% mutate(
      .psc_key = digest::digest(do.call(paste, c(.[key_cols], sep="|")), algo = "xxhash64")
    )
  }
}




make_run_paths <- function(base_dir = "psc_api_runs",
                           dataset_tag = "default",
                           create = TRUE) {
  base_dir <- fs::path_abs(base_dir)
  cache_dir <- fs::path(base_dir, "cache")
  out_dir   <- fs::path(base_dir, "outputs")
  snap_dir  <- fs::path(base_dir, "snapshots")
  if (isTRUE(create)) fs::dir_create(c(cache_dir, out_dir, snap_dir), recurse = TRUE)
  list(
    base      = base_dir,
    cache_rds = fs::path(cache_dir, paste0("cache_", dataset_tag, ".rds")),
    preds_rds = fs::path(out_dir,   paste0("preds_topk_", dataset_tag, ".rds")),
    top1_rds  = fs::path(out_dir,   paste0("top1_", dataset_tag, ".rds")),
    snap_dir  = snap_dir
  )
}

# atomic writers
safe_write_rds <- function(obj, path) {
  tmp <- paste0(path, ".tmp_", sprintf("%06d", sample.int(1e6,1)))
  saveRDS(obj, tmp); fs::file_move(tmp, path)
}
safe_write_csv <- function(df, path) {
  tmp <- paste0(path, ".tmp_", sprintf("%06d", sample.int(1e6,1)))
  readr::write_csv(df, tmp); fs::file_move(tmp, path)
}



# Ensure a directory exists
ensure_dir <- function(path) { if (!fs::dir_exists(path)) fs::dir_create(path, recurse = TRUE); path }


# permanent row keys 
make_psc_keys <- function(df, key_cols = NULL, query_col) {
  stopifnot(query_col %in% names(df))
  if (is.null(key_cols)) {
    df %>% mutate(.psc_key = sprintf("ROW%07d", dplyr::row_number()))
  } else {
    df %>%
      mutate(.psc_key = digest::digest(do.call(paste, c(.[key_cols], sep = "|")), algo = "xxhash64"))
  }
}

run_fscpsc_psc <- function(df,
                           query_col     = "psc_query_250",
                           key_cols      = NULL,
                           paths         = make_run_paths("psc_api_runs", "default"),
                           snapshot_csv  = TRUE,
                           top_k         = 5,
                           batch_size    = 80,      # < 100 req/min
                           pause_between = 1.0,     # seconds between batches
                           per_call_sleep= 0.25     # seconds between calls
                           ) {

  # prepare keys + query col
  keyed <- make_psc_keys(df, key_cols = key_cols, query_col = query_col) %>%
    transmute(.psc_key, !!query_col := .data[[query_col]])

  # load or init cache (unique by query string)
  cache <- if (fs::file_exists(paths$cache_rds)) readRDS(paths$cache_rds)
           else tibble(psc_query_250 = character(), .pred = list())
  if (!all(c("psc_query_250",".pred") %in% names(cache))) {
    cache <- tibble(psc_query_250 = character(), .pred = list())
  }

  # find new queries
  unique_queries <- keyed %>% distinct(.data[[query_col]])
  need <- unique_queries %>% anti_join(cache, by = setNames("psc_query_250", query_col))
  message(sprintf("Unique queries: %d | new: %d", nrow(unique_queries), nrow(need)))

  # fetch in batches
  if (nrow(need) > 0) {
    qvec <- need[[query_col]]
    batches <- split(qvec, ceiling(seq_along(qvec)/batch_size))
    for (bi in seq_along(batches)) {
      batch <- batches[[bi]]
      message(sprintf("[Batch %d/%d] size=%d", bi, length(batches), length(batch)))
      batch_rows <- vector("list", length(batch))
      for (i in seq_along(batch)) {
        q <- as.character(batch[[i]])
        preds <- fscpsc_search_psc(q, top_k = top_k)   # your API call + parser
        if (per_call_sleep > 0) Sys.sleep(per_call_sleep)
        batch_rows[[i]] <- tibble(psc_query_250 = q, .pred = list(preds))
      }
      # update cache + checkpoint
      cache <- bind_rows(cache, bind_rows(batch_rows)) %>%
        distinct(psc_query_250, .keep_all = TRUE)
      safe_write_rds(cache, paths$cache_rds)
      message(sprintf("  cache saved → %s (n=%d)", paths$cache_rds, nrow(cache)))
      if (pause_between > 0 && bi < length(batches)) Sys.sleep(pause_between)
    }
  } else {
    message("No new queries; using existing cache.")
  }

  # expand top-k predictions and attach permanent keys
  preds_long <- keyed %>%
    left_join(cache, by = setNames("psc_query_250", query_col)) %>%
    unnest(.pred, keep_empty = TRUE) %>%
    transmute(.psc_key,
              !!query_col := .data[[query_col]],
              code = .data$code, name = .data$name, score = .data$score, rank = .data$rank)

  # save outputs
  safe_write_rds(preds_long, paths$preds_rds)
  message(sprintf("preds saved → %s  (rows=%d)", paths$preds_rds, nrow(preds_long)))

  if (isTRUE(snapshot_csv)) {
    snap <- fs::path(paths$snap_dir, paste0("preds_", format(Sys.time(), "%Y%m%d_%H%M%S"), ".csv"))
    safe_write_csv(preds_long, snap)
    message(sprintf("snapshot csv → %s", snap))
  }

  # top-1 per row for easy join-back
  top1 <- preds_long %>%
    group_by(.psc_key) %>%
    arrange(rank, .by_group = TRUE) %>%
    slice_head(n = 1) %>%
    ungroup() %>%
    transmute(.psc_key, psc_code = code, psc_name = name, psc_score = score)

  safe_write_rds(top1, paths$top1_rds)
  message(sprintf("top1 saved → %s  (rows=%d)", paths$top1_rds, nrow(top1)))

  list(paths = paths, cache = cache, preds_topk = preds_long, top1 = top1)
}


```

to run 
```{r}


build_psc_query <- function(df, vendor_col = "subawardee_name", desc_col = "subaward_description") {
  df %>% mutate(psc_query_250 = str_sub(str_squish(paste0(.data[[vendor_col]], ": ", .data[[desc_col]])), 1L, 250))
}
sub_q <- subcontract %>% build_psc_query()


paths <- make_run_paths(
  base_dir = here::here("runs","psc"),   
  dataset_tag = "subset400",            
  create = TRUE
)

# run the batcher
res <- run_fscpsc_psc(
  df            = sub_q,
  query_col     = "psc_query_250",
  key_cols      = c("prime_id", "sub_id", "subaward_number"),
  paths         = paths,
  top_k         = 5,
  batch_size    = 80,
  pause_between = 1.0,
  per_call_sleep= 0.25
)

# join with permanent key
sub_with_keys <- make_psc_keys(sub_q, key_cols = c("prime_id","sub_id","subaward_number"), query_col = "psc_query_250")
sub_with_api  <- sub_with_keys %>% left_join(res$top1, by = ".psc_key")


```


