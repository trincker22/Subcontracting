---
title: "Subcontracting"
output: html_document
date: "2025-08-15"
---


To do: 

READABLE
First, sort out subaward descriptions that do not need further processing 
Then, make a dictionary for abbreviations. Look for DoD specific, or use method to find definitions within corpus. 
Can also make a list to identify problem ones to search up if it is relatively short. 
Third, use NLP on remaining unreadable subaward description. 

LDA- write a quick LDA + textplot to visualize unsupervised groupings. 

CATEGORIZATION PROBLEM
There are 183 categories that the 400k distinct subaward descriptions may fall into. Handcode, then run a categorization model.

```{r}
rm(list = ls())


```

```{r setup, include=FALSE}

library(tidyverse)
library(haven)
library(readxl)
library(readr)
library(jsonlite)
library(janitor)
library(purrr)
library(lubridate)
library(quanteda)  
library(quanteda.textmodels)
library(here)
library(tokenizers)
library(topicmodels)
library(syuzhet)
library(caret)
library(irr)
library(SnowballC)
library(ellmer)
library(data.table)
library(e1071)
library(randomForest)
library(glmnet)
library(stringr)
library(glue)
library(digest)
library(tibble)
library(uuid)
library(scales)

options(scipen = 999)


# full set of obs to run data exploration. sliced so the observation level is primary contract. 
# Data exploration primary df name is "sub" 
# sub <- read_dta("M:/Xiaoqing/Subcontracting/forNLP.dta")

# sub <- sub %>%
#   group_by(prime_id) %>%
#   slice_head(n=1) %>%
#   ungroup()
# sub <- sub %>%
#   mutate(year = year(date(prime_date)))



# Deduped, this is for text processing on the sub description 
# the primary df name for text processing is "subcontract"
# subcontract <- read_rds(here("subcontracts.RDS"))

```

PSC code helpers

```{r psc_setup}
normalize_psc_code <- function(x) {
  x <- as.character(x)
  x <- str_squish(x)
  str_replace(x, "^(\\d+)\\.0+$", "\\1")
}

norm_title <- function(x) str_squish(str_to_upper(as.character(x)))

load_psc_lookup <- function(path) {
  read_dta(path) %>%
    clean_names() %>%
    transmute(
      psc_code  = normalize_psc_code(psc_code),
      psc_title = norm_title(coalesce(product_and_service_code_name, psc_name, dservicecodename))
    ) %>%
    filter(!is.na(psc_code) | !is.na(psc_title)) %>%
    distinct()
}

attach_psc <- function(df, lookup, code_col = NULL, title_col = NULL) {
  if (!is.null(code_col)) {
    df <- df %>%
      mutate(psc_code = normalize_psc_code(.data[[code_col]])) %>%
      left_join(lookup, by = "psc_code")
  } else if (!is.null(title_col)) {
    df <- df %>%
      mutate(psc_title = norm_title(.data[[title_col]])) %>%
      left_join(lookup, by = "psc_title")
  } else {
    stop("provide code_col or title_col")
  }
  df
}

psc_path <- "M:/Xiaoqing/Subcontracting/psc_code.dta" # update to local PSC code file
psc_lookup <- load_psc_lookup(psc_path)
```

Get standardized 2022 NAICS code


```{r}


rev_for_year <- function(y) dplyr::case_when(y <= 2011 ~ 2007L,
                                             y <= 2016 ~ 2012L,
                                             y <= 2021 ~ 2017L,
                                             TRUE ~ 2022L)

prep_crosswalk <- function(cw, from_col, to_col, weight_col = NULL) {
  cw %>%
    transmute(from = str_pad(as.character(.data[[from_col]]), 6, "left", "0"),
              to   = str_pad(as.character(.data[[to_col]]),   6, "left", "0"),
              w    = if (is.null(weight_col)) NA_real_ else suppressWarnings(as.numeric(.data[[weight_col]]))) %>%
    filter(nzchar(from), nzchar(to)) %>%
    distinct(from, to, .keep_all = TRUE) %>%
    group_by(from) %>%
    mutate(w = ifelse(is.na(w), 1/dplyr::n(), w / sum(w, na.rm = TRUE))) %>%
    ungroup()
}

map_step <- function(df, cw, rev_from, rev_to) {
  a <- df %>% filter(rev == rev_from)
  b <- df %>% filter(rev != rev_from)
  if (nrow(a) == 0) return(df)
  a %>%
    mutate(code = str_pad(as.character(code), 6, "left", "0")) %>%
    left_join(cw, by = c("code" = "from"), relationship = "many-to-many") %>%
    mutate(code = dplyr::coalesce(to, code),
           weight = weight * dplyr::coalesce(w, 1),
           rev = ifelse(is.na(to), rev, rev_to),
           map_path = ifelse(is.na(to), map_path, paste0(map_path, "|", rev_from, "→", rev_to))) %>%
    select(-to, -w) %>%
    bind_rows(b, .)
}

standardize_naics_to_2022 <- function(df, naics_col = "naics_code", year_col = "year",
                                      cw_07_12_p, cw_12_17_p, cw_17_22_p) {
  base <- df %>%
    mutate(.row_id = row_number(),
           code = str_pad(as.character(.data[[naics_col]]), 6, "left", "0"),
           rev = rev_for_year(as.integer(.data[[year_col]])),
           weight = 1.0,
           map_path = "orig")
  s1 <- map_step(base, cw_07_12_p, 2007L, 2012L)
  s2 <- map_step(s1,   cw_12_17_p, 2012L, 2017L)
  s3 <- map_step(s2,   cw_17_22_p, 2017L, 2022L)
  s3 %>% mutate(naics_2022 = code)
}

normalize_weights <- function(df) {
  df %>%
    group_by(.row_id) %>%
    mutate(weight = ifelse(sum(weight, na.rm = TRUE) > 0, weight / sum(weight, na.rm = TRUE), weight)) %>%
    ungroup()
}

collapse_to_single_naics <- function(df_std) {
  df_std %>%
    group_by(.row_id) %>%
    slice_max(order_by = weight, n = 1, with_ties = FALSE) %>%
    ungroup()
}

split_merge_stats <- function(df_std) {
  df_std %>% dplyr::count(.row_id, name = "k") %>%
    summarise(no_split = sum(k == 1), split = sum(k > 1), max_branches = max(k))
}

build_naics_prior_2022 <- function(df_std, psc_col = "product_or_service_code_descript",
                                   naics_col = "naics_2022", weight_col = "weight") {
  df_std %>%
    filter(!is.na(.data[[naics_col]]), !is.na(.data[[psc_col]])) %>%
    group_by(.data[[naics_col]], .data[[psc_col]]) %>%
    summarise(n = sum(.data[[weight_col]]), .groups = "drop_last") %>%
    group_by(.data[[naics_col]]) %>%
    mutate(p = n / sum(n)) %>%
    ungroup()
}

prep_naics2022_key_from_clean <- function(df_clean) {
  df_clean %>%
    clean_names() %>%
    transmute(code  = as.character(x2022_naics_us_code),
              title = as.character(x2022_naics_us_title)) %>%
    mutate(code = trimws(code), title = trimws(title)) %>%
    filter(!is.na(code), !is.na(title), code != "", title != "") %>%
    distinct(code, .keep_all = TRUE)
}

attach_naics_titles_2022 <- function(df, code_col = "naics_2022", key_df, levels = c(6,3,2), keep_codes = FALSE) {
  if (!all(c("code","title") %in% names(key_df))) {
    key_df <- prep_naics2022_key_from_clean(key_df)
  }
  key_df <- key_df %>% mutate(code = as.character(code), n = nchar(code)) %>% filter(n %in% levels) %>% distinct(code, .keep_all = TRUE)
  out <- df %>% mutate(`._code6` = substr(as.character(.data[[code_col]]), 1, 6))
  for (L in levels) {
    look <- key_df %>% filter(n == L) %>% select(code, title)
    col_code  <- paste0("naics", L, "_code")
    col_title <- paste0("naics", L, "_title")
    out <- out %>%
      mutate(!!col_code := substr(`._code6`, 1, L)) %>%
      left_join(look, by = setNames("code", col_code)) %>%
      rename(!!col_title := title)
    if (!keep_codes) out <- out %>% select(-all_of(col_code))
  }
  out %>% select(-`._code6`)
}

cw_07_12 <- read_xls(here("Data","NAICSCrosswalks","2012_to_2007_NAICS.xls"))
cw_12_17 <- read_xlsx(here("Data","NAICSCrosswalks","2012_to_2017_NAICS.xlsx"))
cw_17_22 <- read_xlsx(here("Data","NAICSCrosswalks","2022_to_2017_NAICS.xlsx"))

cw_07_12_p <- prep_crosswalk(cw_07_12, from_col = "2007 NAICS Code", to_col = "2012 NAICS Code")
cw_12_17_p <- prep_crosswalk(cw_12_17, from_col = "2012 NAICS Code", to_col = "2017 NAICS Code")
cw_17_22_p <- prep_crosswalk(cw_17_22, from_col = "2017 NAICS Code", to_col = "2022 NAICS Code")

codes2022 <- read_xlsx(here("Data","NAICSCrosswalks","2022_codes.xlsx")) %>% clean_names()
codes22_key <- prep_naics2022_key_from_clean(codes2022)

apply_naics <- function(df, naics_col = "naics_code", year_col = "year") {
  std <- standardize_naics_to_2022(df, naics_col, year_col, cw_07_12_p, cw_12_17_p, cw_17_22_p)
  std <- normalize_weights(std)
  std_one <- collapse_to_single_naics(std)
  df_out <- df %>%
    mutate(.row_id = row_number()) %>%
    left_join(std_one %>% select(.row_id, naics_2022), by = ".row_id") %>%
    select(-.row_id)
  df_out <- attach_naics_titles_2022(df_out, code_col = "naics_2022", key_df = codes22_key, levels = c(6,3,2))
  list(df = df_out, std = std)
}


```


```{r apply_codes}
sub_path <- here("Data","sub_enriched.rds")
subcontract_path <- here("Data","subcontract_enriched.rds")

if (file.exists(sub_path) && file.exists(subcontract_path)) {
  sub <- read_rds(sub_path)
  subcontract <- read_rds(subcontract_path)
} else {
  naics_sub <- apply_naics(sub)
  sub <- naics_sub$df %>% attach_psc(psc_lookup, code_col = "psc_code")
  naics_prior_2022 <- build_naics_prior_2022(naics_sub$std,
                                             psc_col = "product_or_service_code_descript",
                                             naics_col = "naics_2022", weight_col = "weight")
  naics_subcontract <- apply_naics(subcontract)
  subcontract <- naics_subcontract$df %>%
    attach_psc(psc_lookup, title_col = "product_or_service_code_descript")
  write_rds(sub, sub_path)
  write_rds(subcontract, subcontract_path)
}
```
```{r}



summary_data_all <- sub %>%
  group_by(year, naics_2022) %>%
  summarise(
    num_contracts = n(),
    avg_contract_value = mean(nom_g, na.rm = TRUE),
    total_contract_value = sum(nom_g, na.rm = TRUE),
    .groups = "drop") %>%
  left_join(sub %>% select(naics_2022, naics6_title), by = "naics_2022")

top_5_naics_value <- summary_data_all %>%
  group_by(naics6_title) %>%
  summarise(total_value = sum(total_contract_value), .groups = "drop") %>%
  arrange(desc(total_value)) %>%
  slice(1:5) %>%
  pull(naics6_title)

filtered_summary_top5_value <- summary_data_all %>%
  filter(naics6_title %in% top_5_naics_value)

grouped_data_for_shares <- summary_data_all %>%
  mutate(naics_group = ifelse(naics6_title %in% top_5_naics_value, naics6_title, "Other")) %>%
  group_by(year, naics_group) %>%
  summarise(
    num_contracts_grouped = sum(num_contracts),
    total_contract_value_grouped = sum(total_contract_value),
    .groups = "drop"
  )

yoy_change_data <- filtered_summary_top5_value %>%
  group_by(naics6_title) %>%
  arrange(year, .by_group = TRUE) %>%
  mutate(
    yoy_num_contracts = 100 * (num_contracts / lag(num_contracts) - 1),
    yoy_total_contract_value = 100 * (total_contract_value / lag(total_contract_value) - 1)
  ) %>%
  ungroup()

overall <- sub %>%
  group_by(year) %>%
  summarise(
    num_contracts = n(),
    total_value   = sum(nom_g, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(year) %>%
  mutate(
    yoy_num = 100 * (num_contracts / lag(num_contracts) - 1),
    yoy_val = 100 * (total_value   / lag(total_value)   - 1)
  )

wrap_legend <- function(x, width = 28) stringr::str_wrap(x, width = width)
legend_guide_bottom <- guides(color = guide_legend(nrow = 2, byrow = TRUE),
                              fill  = guide_legend(nrow = 2, byrow = TRUE))
theme_clean <- theme_minimal(base_size = 12) +
  theme(legend.position = "bottom",
        legend.title = element_text(size = 10),
        legend.text  = element_text(size = 9),
        axis.text.x  = element_text(angle = 45, hjust = 1))
scale_color_wrapped <- scale_color_discrete(labels = ~ wrap_legend(.x, 28))
scale_fill_wrapped  <- scale_fill_discrete(labels  = ~ wrap_legend(.x, 28))

```

##Data Visualization

```{r}

overall %>%
  ggplot(aes(x = factor(year), y = num_contracts)) +
  geom_col() +
  labs(title = "Total Number of Contracts Per Year", x = "Year", y = "Number of Contracts") +
  theme_clean


```


```{r}

overall %>%
  ggplot(aes(x = factor(year), y = total_value)) +
  geom_col() +
  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +
  labs(title = "Total Value of Contracts Per Year", x = "Year", y = "Total Contract Value ($)") +
  theme_clean


```


```{r}

overall %>%
  filter(!is.na(yoy_num), year >2011, year <2024) %>%
  ggplot(aes(x = year, y = yoy_num)) +
  geom_line() + geom_point() +
  scale_y_continuous(labels = label_percent(scale = 0.01)) +
  scale_x_continuous(breaks = sort(unique(overall$year))) +
  labs(title = "YoY Change: Number of Contracts", x = "Year", y = "YoY %") +
  theme_clean

```


```{r}

overall %>%
  filter(!is.na(yoy_val), year >2011, year <2024) %>%
  ggplot(aes(x = year, y = yoy_val)) +
  geom_line() + geom_point() +
  scale_y_continuous(labels = label_percent(scale = 0.01)) +
  scale_x_continuous(breaks = sort(unique(overall$year))) +
  labs(title = "YoY Change: Total Contract Value", x = "Year", y = "YoY %") +
  theme_clean

```

```{r}

yoy_change_data %>%
  filter(year > 2012) %>%
  ggplot(aes(x = year, y = yoy_num_contracts, color = naics6_title, group = naics6_title)) +
  geom_line() + geom_point() +
  scale_y_continuous(labels = label_percent(scale = 0.01)) +
  scale_x_continuous(breaks = sort(unique(yoy_change_data$year))) +
  labs(title = "YoY % Change in Number of Contracts, Top 5 NAICS",
       x = "Year", y = "YoY %", color = "NAICS Industry Title") +
  scale_color_wrapped + theme_clean + legend_guide_bottom

```

```{r}

yoy_change_data %>%
  filter(year > 2012) %>%
  ggplot(aes(x = year, y = yoy_total_contract_value, color = naics6_title, group = naics6_title)) +
  geom_line() + geom_point() +
  scale_y_continuous(labels = label_percent(scale = 0.01)) +
  scale_x_continuous(breaks = sort(unique(yoy_change_data$year))) +
  labs(title = "YoY % Change in Total Contract Value, Top 5 NAICS",
       x = "Year", y = "YoY %", color = "NAICS Industry Title") +
  scale_color_wrapped + theme_clean + legend_guide_bottom


```

```{r}

ggplot(filtered_summary_top5_value,
       aes(x = year, y = num_contracts, color = naics6_title, group = naics6_title)) +
  geom_line() + geom_point() +
  scale_x_continuous(breaks = sort(unique(filtered_summary_top5_value$year))) +
  labs(title = "Number of Contracts Over Time, Top 5 NAICS",
       x = "Year", y = "Number of Contracts", color = "NAICS Industry Title") +
  scale_color_wrapped + theme_clean + legend_guide_bottom


```

```{r}

ggplot(filtered_summary_top5_value,
       aes(x = year, y = avg_contract_value, color = naics6_title, group = naics6_title)) +
  geom_line() + geom_point() +
  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +
  scale_x_continuous(breaks = sort(unique(filtered_summary_top5_value$year))) +
  labs(title = "Average Contract Value Over Time Top 5 NAICS",
       x = "Year", y = "Average Contract Value", color = "NAICS Industry Title") +
  scale_color_wrapped + theme_clean + legend_guide_bottom


```

```{r}

ggplot(grouped_data_for_shares,
       aes(x = factor(year), y = num_contracts_grouped, fill = naics_group)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = percent) +
  labs(title = "Share of Contract Count by Year (Top 5 vs Other)",
       x = "Year", y = "Share", fill = "NAICS Group") +
  scale_fill_wrapped + theme_clean + legend_guide_bottom

```

```{r}

ggplot(grouped_data_for_shares,
       aes(x = factor(year), y = total_contract_value_grouped, fill = naics_group)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = percent) +
  labs(title = "Share of Contract Value by Year (Top 5 vs Other)",
       x = "Year", y = "Share", fill = "NAICS Group") +
  scale_fill_wrapped + theme_clean + legend_guide_bottom

```

```{r}

theme_clean <- theme_minimal(base_size = 12) +
  theme(legend.position = "bottom",
        legend.title = element_text(size = 10),
        legend.text  = element_text(size = 9),
        axis.text.x  = element_text(angle = 45, hjust = 1))

sub_prep <- sub %>%
  mutate(prime_date = as.Date(prime_date),
         year = year(prime_date))

summary_data_all <- sub_prep %>%
  group_by(year, naics6_title) %>%
  summarise(num_contracts = n(),
            avg_contract_value = mean(nom_g, na.rm = TRUE),
            total_contract_value = sum(nom_g, na.rm = TRUE),
            .groups = "drop")

hhi_by_year <- summary_data_all %>%
  group_by(year, naics6_title) %>%
  summarise(val = sum(total_contract_value), .groups = "drop_last") %>%
  mutate(year_total = sum(val),
         share_pct = ifelse(year_total > 0, 100 * val / year_total, NA_real_)) %>%
  summarise(HHI = sum(share_pct^2, na.rm = TRUE), .groups = "drop")

ggplot(hhi_by_year, aes(x = year, y = HHI)) +
  geom_line() + geom_point() +
  labs(title = "Market Concentration (HHI) of Contract Value by Year",
       x = "Year", y = "HHI (0–10,000)") +
  theme_clean

```

The HHI (Herfindahl-Hirschman Index) measures market concentration. HHI is the sum of each industry's % share\*100, squared, for each year. If all the shares are small, nearing perfect competition, then the HHI will be close to 0 because the squared shares are very small. If one firm has 100% share, 100\^2 will be 10,000, the max HHI value. Below 1,500 is considered not concentrated, while an HHI above 2500 is considered highly concentrated. We can see here that concentration has decreased since 2010, but largely has not been concentrated. This value only measures the concentration of industries within federal government contracts, it does not say anything about the firm composition within any given industry.

```{r}

monthly_series <- sub_prep %>%
  mutate(month = floor_date(prime_date, "quarter")) %>%
  group_by(month) %>%
  summarise(total_value = sum(nom_g, na.rm = TRUE), .groups = "drop")

ggplot(monthly_series, aes(x = month, y = total_value)) +
  geom_line() +
  geom_point()+
  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +
  labs(title = "Quarterly Total Contract Value Over Time",
       x = "Quarter", y = "Total Contract Value") +
  theme_clean

```

```{r}
size_vs_count <- summary_data_all %>%
  group_by(naics6_title) %>%
  summarise(total_value = sum(total_contract_value, na.rm = TRUE),
            num_contracts = sum(num_contracts, na.rm = TRUE),
            .groups = "drop")

ggplot(size_vs_count, aes(x = num_contracts, y = total_value)) +
  geom_point(alpha = 0.7) +
  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +
  scale_x_continuous(labels = label_number(scale_cut = cut_short_scale())) +
  labs(title = "Contracts: Count vs Total Value by NAICS (All Years)",
       x = "Number of Contracts", y = "Total Contract Value") +
  theme_clean

```

# LLM Section

There are two prompts that follow a similar workflow. The first uses LLM to translate codes in the subcontractors description into readable text. For example, quite often there will just be a description like IGF::OT::IGF, which, if you look in the DoD handbook, signifies "Inherently Governmental Functions: Other functions." The idea is that the LLM will translate these acronyms or codes instead of doing it by hand, which would be quite time intensive. The second prompt classifies the relationship between the primary contract category and the specific description of the contract given by the subcontractors.

First, I construct a safe call function, which limits the frequency of LLM queries so that we do not repeatedly hit quotas. When it fails, it adds sleep time as powers of 2, increasing with the number of failed tries. Next, because we ask the LLM to return output in JSON format, the JSON parser uses regex to crop out only the text that fits the variables we define. (For ex., it will remove "Sure, the readable text is below!" from the output). It gives a try-error if the output cannot be parsed into JSON, which will also be flagged as F under the parsed_ok variable. If any columns of the tibble built off of the JSON structure are missing, another function will fill with NAs. This all helps to identify failures.

Next, the batch runner works for both prompts. It creates batches size 20 (the number of inputs before the output tibble is saved to checkpoint), and runs each row in the batch through all of the functions above. The output has 5 columns: the observation id, the raw JSON output from the LLM, the parsed output as returned_text, a flag indicating whether the parsing was successful, and notes from the LLM about why it made its decision (people report that LLMs have better accuracy when you ask them to justify their decisions).

These functions let you run the data in chunks. So, you generate the needs_readable df, which is (all observations) - (previously processed obs), identified by the observation id, NOT ROW INDEX (which can change). then it will run the llm over the needs_readable, it will join these results in with your existing processed output as readable_all, and these will overwrite your original file with the new LLM variables, matched by observation id. The structure is the same for the relation prompt run, the dfs just have different names. In the end, both runs write into their own checkpoint files, and their final results into contracts.

There are major computational limitations. The Gemini API that I used here only takes 250 queries a day, even with aggressive batching (200 obs per query), it would take 8 days. Alternatively, I could try to run a "local" model using the remote computers or the BigTex computing. The problem is that these have CPUs, which are significantly slower than GPUs at this task. It is difficult to estimate the amount of time it would take on a CPU, but GPU can be estimated (probably a dayish? but would be paid)

The cost of using a GPU depends on the number of tokens used, which determines the amount of time that the computation takes. The number of tokens depends on the length of the prompt, which depends on batch size. The batch size depends on the accuracy level of a given batch size. So in order to know for sure the cost of using a GPU, would have to test on a few batch sizes, which would be paid.

**Alternatives** :

The codes/acronyms issue can probably be solved without an LLM. There are dictionaries and methods for creating dictionaries with a large corpus (somewhere else in the corpus, there will be text like "Acronym Code (AC)"). But the relationship is more difficult. This is because in general, text based models can categorize a set of text in to groups, like in an LDA model, and then with supervised counterparts. So for example, if you treat each seperate word as its own variable and construct a matrix where the rows are the texts and the columns are the frecquences of each distinct word that appears in all of the texts, then you could regress on the words.

So for ex., if you were categorizing news, then the words "football" and "player" would have large coefficients for the sports category. This is frequence based.

```{r}

chat <- ellmer::chat_google_gemini()

contracts <- subcontract %>%
  mutate(
    obs_id        = as.character(prime_id_num),
    broad_cat     = product_or_service_code_descript,
    specific_desc = subaward_description,
    naics         = naics6_title
  ) %>%
  select(obs_id, broad_cat, specific_desc, naics) %>%
  distinct(obs_id, .keep_all = TRUE)

# prompts
prompt_readable <- function(broad, specific) {
  glue(
    "{{\n  \"task\": \"rewrite_to_plain_english\",\n  \"schema\": {{\n    \"returned_text\": \"string\",\n    \"notes\": \"string\"\n  }},\n  \"instructions\": \"You are given a general contract category and a specific subaward description. If the description is already human-readable, return it unchanged. If it is code-like or obscure, return a brief plain-English description. Be conservative: no hallucinated details; if in doubt, summarize functionally.\",\n  \"category\": \"{broad}\",\n  \"description\": \"{specific}\",\n  \"output_format\": \"JSON with fields returned_text, notes\"\n}}"
  )
}

prompt_relation <- function(broad, specific) {
  glue(
    "{{\n  \"task\": \"relation_classification\",\n  \"schema\": {{\n    \"relation\": \"enum[same, subset, unrelated, unclear]\",\n    \"rationale\": \"string\"\n  }},\n  \"instructions\": \"Classify the relationship between the broad category and the subtask. same = equivalent meaning; subset = subtask is a specialization of the category; unrelated = different domains; unclear = insufficient info.\",\n  \"category\": \"{broad}\",\n  \"subtask\": \"{specific}\",\n  \"output_format\": \"JSON with fields relation, rationale\"\n}}"
  )
}


#-------------------------------------------------
# Builder Functions
#-------------------------------------------------

# LLM Caller with exponential sys sleep 
call_chat_safe <- function(prompt, max_tries = 5, base_sleep = 2) {
  for (i in seq_len(max_tries)) {
    txt <- try({
      captured <- paste(utils::capture.output(res <- chat$chat(prompt)), collapse = "\n")
      out <- if (is.character(res) && nzchar(res)) res else captured
      out
    }, silent = TRUE)
    if (!inherits(txt, "try-error") && is.character(txt) && nzchar(txt)) return(txt)
    Sys.sleep(base_sleep * (2^(i - 1)) + runif(1, 0, 0.5))
  }
  NA_character_
}

# JSON Parser
parse_json_relaxed <- function(txt) {
  if (is.na(txt)) return(tibble(parsed_ok = FALSE))

  m1 <- stringr::str_match(txt, "(?s)```(?:json)?\\s*(\\{[\\s\\S]*?\\})\\s*```")
  if (!is.na(m1[, 2])) {
    json_txt <- m1[, 2]
  } else {
    m2 <- stringr::str_match_all(txt, "\\{[\\s\\S]*?\\}")
    if (length(m2) && nrow(m2[[1]]) > 0) json_txt <- tail(m2[[1]][, 1], 1) else json_txt <- txt
  }

  out <- try(jsonlite::fromJSON(json_txt), silent = TRUE)
  if (inherits(out, "try-error")) tibble(parsed_ok = FALSE)
  else dplyr::bind_cols(tibble(parsed_ok = TRUE), tibble::as_tibble(out))
}

# NA if cols missing
ensure_cols <- function(df, cols) {
  missing <- setdiff(cols, names(df))
  for (nm in missing) df[[nm]] <- NA_character_
  df
}

#  Batch runner
run_llm_over_df <- function(df, broad_col, specific_col,
                            which_prompt = c("readable", "relation"),
                            batch_size = 20,
                            checkpoint_path = NULL,
                            temperature = 0.0, # max determinism parameter in LLM
                            verbose = TRUE) {
  which_prompt <- match.arg(which_prompt)
  df <- df %>% mutate(obs_id = as.character(.data[["obs_id"]]))
  n <- nrow(df)
  if (n == 0) return(tibble())

  idx <- split(seq_len(n), ceiling(seq_along(seq_len(n)) / batch_size))
  results <- vector("list", length(idx))

  for (bi in seq_along(idx)) {
    rows <- df[idx[[bi]], , drop = FALSE]

    rows_small <- tibble::tibble(
      obs_id = rows$obs_id,
      broad  = rows[[broad_col]],
      spec   = rows[[specific_col]]
    )

    prompts <- purrr::pmap(rows_small, function(obs_id, broad, spec) {
      if (which_prompt == "readable") prompt_readable(broad, spec) else prompt_relation(broad, spec)
    })

    raw    <- purrr::map(prompts, ~ call_chat_safe(.x))
    parsed <- purrr::map_dfr(raw, parse_json_relaxed)

    out <- tibble::tibble(
      obs_id = rows$obs_id,
      raw    = unlist(raw, use.names = FALSE)
    ) %>% dplyr::bind_cols(parsed)

    results[[bi]] <- out

    if (!is.null(checkpoint_path)) {
      readr::write_rds(dplyr::bind_rows(results[seq_len(bi)]), checkpoint_path)
      if (verbose) message(glue("[{bi}/{length(idx)}] wrote checkpoint: {checkpoint_path} (rows so far: {nrow(dplyr::bind_rows(results[seq_len(bi)]))})"))
    }
  }

  dplyr::bind_rows(results)
}


#-------------------------------------------------
# Initializing Checkpoint Files
#-------------------------------------------------

readable_ckpt <- here("Checkpoints/readable_checkpoint.rds")
relation_ckpt <- here("Checkpoints/relation_checkpoint.rds")

if (!file.exists(readable_ckpt)) {
  write_rds(
    tibble(
      obs_id        = character(),
      raw           = character(),
      parsed_ok     = logical(),
      returned_text = character(),
      notes         = character()
    ),
    readable_ckpt
  )
}

if (!file.exists(relation_ckpt)) {
  write_rds(
    tibble(
      obs_id    = character(),
      raw       = character(),
      parsed_ok = logical(),
      relation  = character(),
      rationale = character()
    ),
    relation_ckpt
  )
}

prev_readable <- read_rds(readable_ckpt)
prev_rel      <- read_rds(relation_ckpt)

prev_readable <- if (file.exists(readable_ckpt)) readr::read_rds(readable_ckpt) else tibble(obs_id = character())
prev_rel      <- if (file.exists(relation_ckpt)) readr::read_rds(relation_ckpt) else tibble(obs_id = character())


#-------------------------------------------------
# Run Readable, then Relation
#-------------------------------------------------

needs_readable <- dplyr::anti_join(
  contracts %>% dplyr::select(obs_id, broad_cat, specific_desc),
  prev_readable %>% dplyr::select(obs_id),
  by = "obs_id"
)

readable_new <- if (nrow(needs_readable) > 0) {
  run_llm_over_df(
    df = needs_readable,
    broad_col = "broad_cat",
    specific_col = "specific_desc",
    which_prompt = "readable",
    batch_size = 20,
    checkpoint_path = readable_ckpt,
    verbose = TRUE
  )
} else tibble()

readable_all <- dplyr::bind_rows(prev_readable, readable_new) %>%
  dplyr::distinct(obs_id, .keep_all = TRUE) %>%
  ensure_cols(c("returned_text", "notes"))

contracts <- contracts %>%
  dplyr::left_join(
    readable_all %>%
      dplyr::transmute(
        obs_id,
        readable_text  = dplyr::coalesce(returned_text, NA_character_),
        readable_notes = dplyr::coalesce(notes,         NA_character_)
      ),
    by = "obs_id"
  )

# ---- RELATION
needs_rel <- dplyr::anti_join(
  contracts %>% dplyr::select(obs_id, broad_cat, specific_desc),
  prev_rel %>% dplyr::select(obs_id),
  by = "obs_id"
)

rel_new <- if (nrow(needs_rel) > 0) {
  run_llm_over_df(
    df = needs_rel,
    broad_col = "broad_cat",
    specific_col = "specific_desc",
    which_prompt = "relation",
    batch_size = 20,
    checkpoint_path = relation_ckpt,
    verbose = TRUE
  )
} else tibble()

rel_all <- dplyr::bind_rows(prev_rel, rel_new) %>%
  dplyr::distinct(obs_id, .keep_all = TRUE) %>%
  ensure_cols(c("relation", "rationale"))

contracts <- contracts %>%
  dplyr::left_join(
    rel_all %>%
      dplyr::transmute(
        obs_id,
        relation            = ifelse(parsed_ok, relation,  NA_character_),
        relation_rationale  = ifelse(parsed_ok, rationale, NA_character_)
      ),
    by = "obs_id"
  )


```

```{r}


```

```{r}


```
