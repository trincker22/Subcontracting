---
title: "Dictionary Expansion"
output: html_document
date: "2025-08-27"
---

This script creates an acronym dictionary using an authoritative defense procurement source (DAU), and mines a dictionary from the inputted text. Then, it gives the acronym a global prior score (popularity across corpus) and a local semantic similarity score (does the acronym expansion match the context?), and takes the weighted average of those for a composite score. We set a tau parameter for the minimum composite score needed to apply the expansion.

```{r}

library(data.table)
library(digest)
library(dplyr)
library(fs)
library(here)
library(httr2)
library(jsonlite)
library(purrr)
library(readr)
library(readxl)
library(stringdist)
library(stringr)
library(tibble)
library(tidyr)
library(stringi)
library(wordvector)


options(scipen = 999)


```

# DAU Dictionary

```{r}

dict_path <- here("Data", "acronymdict.RDS")
xlsx_path <- here("Data", "acronymdict.xlsx")

if (!file.exists(dict_path)) {
  raw <- read_xlsx(xlsx_path, col_names = FALSE)
  colname <- names(raw)[which.max(colSums(!is.na(raw)))]
  lines <- raw[[colname]] |> as.character() |> str_squish()
  lines <- lines[!is.na(lines) & nzchar(lines)]

  split_acr <- function(txt) {
    m <- regexec("^(\\S{2,15})\\s+(.+)$", txt); r <- regmatches(txt, m)[[1]]
    if (length(r) == 3) data.frame(acr = toupper(trimws(r[2])), exp = trimws(r[3]))
    else data.frame(acr = NA, exp = NA)
  }

  acr_dict_dau <- bind_rows(lapply(lines, split_acr)) |>
    filter(!is.na(acr), !is.na(exp), nzchar(acr), nzchar(exp)) |>
    mutate(exp = str_split(exp, ";")) |>
    unnest(exp) |>
    mutate(exp = str_squish(exp)) |>
    filter(exp != "") |>
    distinct(acr, exp)

  saveRDS(acr_dict_dau, dict_path)
} else {
  acr_dict_dau <- readRDS(dict_path)
}

```

# Mined Dictionary

```{r}
subcontracts <- subcontracts

clean1 <- function(x) {
  x <- stringi::stri_trans_nfkc(x)
  x <- gsub("[\\p{Cc}\\p{Cf}]+", " ", x, perl=TRUE)
  x <- gsub("\\s+", " ", x); str_squish(x)
}
is_serial_like <- function(x) {
  nchar(x) >= 6 &&
    (str_count(x, "[-/]") >= 1 || str_count(x, "\\d") >= 3) &&
    str_count(x, "\\s") <= 2 &&
    !grepl("\\b(PROVIDE|SUPPORT|REPAIR|INSTALL)\\b", toupper(x))
}
is_address_like <- function(x) {
  u <- toupper(x)
  grepl("\\b(PO BOX|P\\.O\\.|SUITE|STE\\.?|BLDG|RM|FLOOR|FL|ATTN)\\b", u) ||
  grepl("\\b(AL|AK|AZ|AR|CA|CO|CT|DE|DC|FL|GA|HI|IA|ID|IL|IN|KS|KY|LA|MA|MD|ME|MI|MN|MO|MS|MT|NC|ND|NE|NH|NJ|NM|NV|NY|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VA|VT|WA|WI|WV)\\b", u) ||
  grepl("\\b\\d{5}(-\\d{4})?\\b", x)
}
is_multiline_markup <- function(x) grepl("\\b(?:LINE\\s*\\d+|CLIN\\s*[:;\\-])", toupper(x))
is_junky  <- function(x) { nzchar(x) && (str_count(x, "[^\\x20-\\x7E]")/nchar(x) > 0.05) }
is_short  <- function(x) nchar(x) < 40

txtdf <- subcontracts %>%
  mutate(
    .row_id = row_number(),
    raw_text = subaward_description %||% "",
    text_cln = clean1(raw_text),
    flag_serial   = vapply(text_cln, is_serial_like,  logical(1)),
    flag_address  = vapply(text_cln, is_address_like, logical(1)),
    flag_multilin = vapply(text_cln, is_multiline_markup, logical(1)),
    flag_junky    = vapply(text_cln, is_junky,        logical(1)),
    flag_short    = vapply(text_cln, is_short,        logical(1))
  )

texts <- txtdf$text_cln

catalog_markers <- c("PN","NSN","CLIN","NIIN","SDRL","FAI","MTBF","BOM","EOL","CAGE","P/N","S/N")
connectors <- c("AND","OF","THE","FOR","IN","ON","WITH","TO","BY","AT","FROM","A","AN")
JW_THRESH <- 0.93

looks_like_acronym <- function(acr) {
  acr <- toupper(as.character(acr))
  ok_shape       <- grepl("^[A-Z0-9][A-Z0-9&\\.\\-]{1,9}$", acr)
  enough_letters <- stringr::str_count(acr, "[A-Z]") >= 2
  ok <- ok_shape & enough_letters
  ok[is.na(ok)] <- FALSE
  ok
}
normalize_phrase <- function(x) {
  x %>%
    toupper() %>%
    str_replace_all("&", " AND ") %>%
    str_replace_all("[ ]*/[ ]*", "/") %>%
    str_replace_all("[^A-Z0-9/ \\-]", " ") %>%
    str_replace_all("\\s+", " ") %>%
    str_squish()
}
initials_from_phrase_vec <- function(phr, drop_words = connectors) {
  p <- normalize_phrase(phr)
  toks <- str_split(p, "\\s+")
  n <- length(toks)
  out_init <- character(n); out_ntok <- integer(n)
  for (i in seq_len(n)) {
    ti <- toks[[i]]
    ti <- ti[!(ti %in% drop_words)]
    ti <- ti[nzchar(ti)]
    out_ntok[i] <- length(ti)
    out_init[i] <- if (length(ti)) paste0(substr(ti, 1, 1), collapse = "") else ""
  }
  list(init = out_init, n_tokens = out_ntok)
}
strip_leading_connectors <- function(x) {
  x %>% toupper() %>% str_squish() %>% str_replace("^(?:AND THE|AND|OR|THE)\\s+", "")
}
normalize_exp_display <- function(x) {
  x %>%
    toupper() %>%
    str_replace_all("[ ]*/[ ]*", " / ") %>%
    str_replace_all("[ ]*-[ ]*", "-") %>%
    str_replace_all("\\s+", " ") %>%
    str_squish()
}

pat_exp_acr <- "(?<exp>[A-Za-z][A-Za-z0-9/&\\-\\. ,]+?)\\s*\\((?<acr>[A-Z0-9][A-Z0-9&\\.\\-]{1,9})\\)"
pat_acr_exp <- "(?<acr>[A-Z0-9][A-Z0-9&\\.\\-]{1,9})\\s*\\((?<exp>[A-Za-z][A-Za-z0-9/&\\-\\. ,]+?)\\)"

extract_pairs_one <- function(txt, doc_id) {
  m1 <- str_match_all(txt, pat_exp_acr)[[1]]
  df1 <- if (nrow(m1)) tibble(acr = m1[, "acr"], exp = m1[, "exp"], pattern = "longform_paren", doc_id = doc_id) else tibble(acr=character(),exp=character(),pattern=character(),doc_id=integer())
  m2 <- str_match_all(txt, pat_acr_exp)[[1]]
  df2 <- if (nrow(m2)) tibble(acr = m2[, "acr"], exp = m2[, "exp"], pattern = "acr_paren_longform", doc_id = doc_id) else tibble(acr=character(),exp=character(),pattern=character(),doc_id=integer())
  bind_rows(df1, df2)
}

mine_acronym_pairs <- function(texts) {
  cand <- map2_dfr(texts, seq_along(texts), extract_pairs_one) %>%
    mutate(acr = toupper(str_squish(acr)),
           exp = str_squish(exp)) %>%
    filter(nchar(acr) >= 2, nchar(exp) >= 3, !acr %in% catalog_markers)

  if (nrow(cand) == 0) return(cand)

  iv <- initials_from_phrase_vec(cand$exp)
  cand$init     <- iv$init
  cand$n_tokens <- iv$n_tokens

  cand %>%
    filter(
      looks_like_acronym(acr),
      n_tokens >= nchar(acr),
      init == acr
    ) %>%
    distinct(acr, exp, doc_id, pattern)
}

collapse_within_acr_jw_agg <- function(df, jw_thresh = JW_THRESH) {
  if (!nrow(df)) return(list(
    dict = tibble(acr=character(), exp_uc=character(), freq=integer()),
    map  = tibble(acr=character(), exp_uc_raw=character(), exp_uc_canon=character())
  ))
  df2 <- df %>%
    mutate(
      exp    = strip_leading_connectors(as.character(exp)),
      exp    = normalize_exp_display(exp),
      exp_uc = toupper(exp),
      freq   = coalesce(as.integer(freq), 1L)
    ) %>%
    filter(nzchar(exp_uc)) %>%
    arrange(desc(freq), desc(nchar(exp_uc))) %>%
    distinct(exp_uc, .keep_all = TRUE)

  n <- nrow(df2); if (n <= 1) return(list(
    dict = tibble(acr=df2$acr, exp_uc=df2$exp_uc, freq=df2$freq),
    map  = tibble(acr=df2$acr, exp_uc_raw=df2$exp_uc, exp_uc_canon=df2$exp_uc)
  ))

  S <- 1 - stringdistmatrix(df2$exp_uc, df2$exp_uc, method="jw"); diag(S) <- 1
  comp <- rep(NA_integer_, n); cid <- 0L
  for (i in seq_len(n)) if (is.na(comp[i])) {
    cid <- cid + 1L; q <- i; comp[i] <- cid
    while (length(q)) {
      v <- q[1]; q <- q[-1]
      neigh <- which(S[v, ] >= jw_thresh & is.na(comp))
      if (length(neigh)) { comp[neigh] <- cid; q <- c(q, neigh) }
    }
  }
  clusters <- split(seq_len(n), comp)

  dict <- map_dfr(clusters, function(ix){
    pick <- ix[order(-df2$freq[ix], -nchar(df2$exp_uc[ix]))][1]
    tibble(acr=df2$acr[pick], exp_uc=df2$exp_uc[pick], freq=sum(df2$freq[ix]))
  })
  map <- map_dfr(clusters, function(ix){
    pick <- ix[order(-df2$freq[ix], -nchar(df2$exp_uc[ix]))][1]
    tibble(acr=df2$acr[ix], exp_uc_raw=df2$exp_uc[ix], exp_uc_canon=df2$exp_uc[pick])
  })
  list(dict=dict, map=map)
}

hi_conf_pairs <- mine_acronym_pairs(texts)

dict_all_raw <- hi_conf_pairs %>%
  mutate(exp = normalize_exp_display(exp)) %>%
  dplyr::count(acr, exp, name = "freq", sort = TRUE)

dict_all_raw <- dict_all_raw %>%
  group_by(acr) %>% slice_head(n = 200L) %>% ungroup()

by_acr <- split(dict_all_raw, dict_all_raw$acr)
res    <- map(by_acr, ~ collapse_within_acr_jw_agg(.x, jw_thresh = JW_THRESH))
dict_all <- bind_rows(map(res, "dict"))
exp_map  <- bind_rows(map(res, "map"))

feat_tbl <- hi_conf_pairs %>%
  mutate(exp_uc_raw = toupper(normalize_exp_display(exp))) %>%
  left_join(exp_map, by = c("acr","exp_uc_raw")) %>%
  mutate(exp_uc = coalesce(exp_uc_canon, exp_uc_raw)) %>%
  distinct(acr, exp_uc, doc_id, pattern) %>%
  group_by(acr, exp_uc) %>%
  summarise(
    doc_coverage = n_distinct(doc_id),
    def_hits     = sum(!is.na(pattern) & pattern %in% c("longform_paren","acr_paren_longform")),
    .groups = "drop"
  )

```

# Merge

```{r}
ACRONYM_KEEP <- c("DoD","USAF","USN","USMC","USA","AFOTEC","JSF","COTS","F-35","MTBF","BOM",
                  "SDRL","NSN","CLIN","NIIN","CAGE","ISR","GPS","RF","IT","AI","UAS","UAV","SATCOM")
SMALL_WORDS <- c("and","or","of","the","for","in","on","with","to","by","at","from","a","an")

normalize_spaces <- function(x) { x %>% str_replace_all("\\s+", " ") %>% str_squish() }
title_token <- function(tok) {
  if (tok %in% ACRONYM_KEEP) return(tok)
  if (grepl("\\d", tok)) return(toupper(tok))
  if (nchar(tok) <= 4 && grepl("^[A-Z]+$", tok)) return(tok)
  split_and_cap <- function(s, sep) {
    parts <- strsplit(s, sep, fixed = TRUE)[[1]]
    parts <- ifelse(nchar(parts),
                    paste0(str_to_upper(substr(parts,1,1)), str_to_lower(substr(parts,2,nchar(parts)))),
                    parts)
    paste(parts, collapse = sep)
  }
  tok <- split_and_cap(tok, "-"); tok <- split_and_cap(tok, "/"); tok
}
pretty_case <- function(x) {
  if (is.na(x) || !nzchar(x)) return(x)
  x0 <- normalize_spaces(x); toks <- strsplit(x0, " +")[[1]]
  if (!length(toks)) return(x0)
  out <- character(length(toks))
  for (i in seq_along(toks)) {
    b <- toks[i]
    if (tolower(b) %in% SMALL_WORDS && i != 1 && i != length(toks)) out[i] <- tolower(b) else out[i] <- title_token(b)
  }
  out <- ifelse(toupper(out) %in% toupper(ACRONYM_KEEP),
                ACRONYM_KEEP[match(toupper(out), toupper(ACRONYM_KEEP))],
                out)
  paste(out, collapse = " ")
}
pretty_case_vec <- function(x) vapply(x, pretty_case, character(1))

acr_mined_all <- dict_all %>%
  mutate(
    exp_disp = pretty_case_vec(exp_uc),
    source   = "mined"
  ) %>%
  left_join(feat_tbl, by = c("acr","exp_uc")) %>%
  mutate(
    freq         = coalesce(freq, 0L),
    doc_coverage = coalesce(doc_coverage, 0L),
    def_hits     = coalesce(def_hits, 0L)
  )

if (exists("acr_dict_dau")) {
  acr_dau_enriched <- acr_dict_dau %>%
    mutate(
      acr      = toupper(str_squish(acr)),
      exp_uc   = toupper(normalize_exp_display(exp)),
      exp_disp = pretty_case_vec(exp),
      source   = "DAU"
    ) %>%
    filter(nzchar(acr), nzchar(exp_uc)) %>%
    distinct(acr, exp_uc, .keep_all = TRUE) %>%
    left_join(select(dict_all, acr, exp_uc, freq), by = c("acr","exp_uc")) %>%
    left_join(feat_tbl, by = c("acr","exp_uc")) %>%
    mutate(
      freq         = coalesce(freq, 0L),
      doc_coverage = coalesce(doc_coverage, 0L),
      def_hits     = coalesce(def_hits, 0L)
    )

  acr_all_sources <- bind_rows(acr_mined_all, acr_dau_enriched) %>%
    distinct(acr, exp_uc, .keep_all = TRUE)
} else {
  acr_all_sources <- acr_mined_all
}

dict_mined <- dict_all %>% select(acr, exp_uc) %>% distinct()
dict_dau <- if (exists("acr_dict_dau")) {
  acr_dict_dau %>%
    mutate(acr = toupper(str_squish(acr)),
           exp_uc = toupper(normalize_exp_display(exp))) %>%
    filter(nzchar(acr), nzchar(exp_uc)) %>%
    distinct(acr, exp_uc)
} else tibble(acr = character(), exp_uc = character())
dict_union <- bind_rows(dict_mined, dict_dau) %>% distinct(acr, exp_uc)


```

# Expansion

This step expands acronyms in context using a combination of semantic similarity from Wikipedia-trained Glove embeddings and global prior scores. 

Each acronym-expansion pair has a global prior score that is a "corpus popularity" measure, taking into account definitional hits, frequency, doc coverage, and whether it matches the DAU authority. Then, looking at acronym occurence in the text, we take a window of the closest 8 nearby words, embed them with Glove, and look at the similarity between possible acronym expansions and the text. This local similarity is blended with the global prior.

Below are the parameters for this model: alpha ranges between [0,1] and represents how we weight the local similarity with the global prior. 1 uses only local similarity k tells us how many words will be considered in semantic similarity surrounding the acronym. tau tells us the minimum blended score (alpha\**similarity + (1-alpha)*\*prior) that an expansion will have to meet to be applied. So, the higher the score, the less expansions we will see. The blended score ranges [0,1].

-   w_def = 2.5 - strong weight for definitional hits (Expansion (ACR) patterns).\
-   w_freq = 1.0 - weight for frequency of the pair in the corpus (diminishing returns with log).\
-   w_doc = 0.8 - weight for number of distinct documents containing the pair.\
-   w_auth = 1.2 - bonus if the pair exists in the DAU authoritative dictionary.

```{r}
# -----------------------------
# Expansion (using domain word2vec)
# -----------------------------

alpha        <- 0.6        # blend of local semantic similarity vs global prior
k_window     <- 8          # +/- tokens around the acronym
tau_local    <- 0.5        # gate for replacements (0..1)
topK_per_acr <- 3L         # limit candidate expansions per acronym (speed/quality)

# ---- Global Score (unchanged) ----
if (!exists("acr_candidates")) {
  w_def  <- 2.5; w_freq <- 1.0; w_doc <- 0.8; w_auth <- 1.2

  acr_candidates <- acr_all_sources %>%
    mutate(auth_hits = if_else(source == "DAU", 1L, 0L)) %>%
    group_by(acr) %>%
    mutate(
      score_raw = w_def*def_hits + w_freq*log1p(freq) + w_doc*log1p(doc_coverage) + w_auth*auth_hits,
      score     = (score_raw - min(score_raw, na.rm = TRUE)) /
                  (max(score_raw, na.rm = TRUE) - min(score_raw, na.rm = TRUE) + 1e-9)
    ) %>%
    arrange(desc(score), .by_group = TRUE) %>%
    mutate(rank = row_number()) %>%
    ungroup()

  margin <- 0.15
  acr_lexicon <- acr_candidates %>%
    group_by(acr) %>%
    summarise(
      best        = first(exp_disp),
      best_uc     = first(exp_uc),
      confidence  = first(score),
      alt         = if (n() >= 2) nth(exp_disp, 2) else NA_character_,
      alt_score   = if (n() >= 2) nth(score, 2) else NA_real_,
      keep_top2   = !is.na(alt_score) & (confidence - alt_score) < margin,
      .groups = "drop"
    )
}



```


```{r}

set.seed(42)


corpus_path <- file.path(here(), "Data", "processed", "contract_corpus.txt")
dir_create(dirname(corpus_path), recurse = TRUE)
corpus_txt <- subcontracts$subaward_description %>%
  as.character() %>%
  str_replace_all("[^A-Za-z0-9/ .\\-]", " ") %>%  # keep / and -
  str_squish()
write_lines(corpus_txt, corpus_path)


if (exists("txtdf") && "text_cln" %in% names(txtdf)) {
  corp <- corpus(txtdf$text_cln)
} else {
  corp <- corpus(subcontracts$subaward_description)
}


toks <- tokens(
  corp,
  remove_punct   = TRUE,
  remove_symbols = TRUE
) %>%
  tokens_remove(stopwords("en"), padding = TRUE) %>%
  tokens_tolower()


w2v <- textmodel_word2vec(
  toks,
  dim       = 100,
  type      = "skip-gram",
  min_count = 5,
  window    = 8,
  iter      = 10,
  alpha     = 0.05,
  use_ns    = TRUE,
  ns_size   = 5,
  verbose   = TRUE
)

w2v_mat <- as.matrix(w2v)  # rows=tokens, cols=dimensions
w2v_dim <- ncol(w2v_mat)
w2v_rds_path <- file.path(here(), "Data", "processed", "w2v_contracts_matrix.rds")
saveRDS(w2v_mat, w2v_rds_path)


tokenize_for_w2v <- function(x) {
  if (is.null(x) || is.na(x) || !nzchar(x)) return(character(0))
  toks <- strsplit(tolower(x), "\\s+")[[1]]
  toks[nzchar(toks)]
}
embed_phrase_w2v <- function(phrase) {
  toks <- tokenize_for_w2v(phrase)
  if (!length(toks)) return(rep(0, w2v_dim))
  idx <- match(toks, rownames(w2v_mat))
  idx <- idx[!is.na(idx)]
  if (!length(idx)) return(rep(0, w2v_dim))
  colMeans(w2v_mat[idx, , drop = FALSE])
}
cosine_sim <- function(a, b) {
  na <- sqrt(sum(a*a)); nb <- sqrt(sum(b*b))
  if (na == 0 || nb == 0) return(0)
  sum(a*b)/(na*nb)
}


topK_per_acr <- 2L
acr_tbl <- acr_candidates %>%
  arrange(desc(score)) %>%
  group_by(acr) %>% slice_head(n = topK_per_acr) %>%
  ungroup() %>%
  distinct(acr, exp_uc, exp_disp, score)

acr_set    <- sort(unique(acr_tbl$acr))
KEEP_UPPER <- toupper(ACRONYM_KEEP)


exp_mat <- do.call(rbind, lapply(acr_tbl$exp_uc, embed_phrase_w2v))
exp_mat[is.na(exp_mat)] <- 0


find_acrs <- function(words_vec) {
  up <- toupper(words_vec)
  ix <- which(up %in% acr_set &
                grepl("^[A-Z0-9&\\.\\-]{2,10}$", words_vec) &
                !(up %in% KEEP_UPPER))
  if (!length(ix)) return(tibble(pos = integer(0), acr = character(0)))
  tibble(pos = ix, acr = up[ix])
}
context_string <- function(v, i, k) {
  lo <- max(1, i - k); hi <- min(length(v), i + k)
  paste(v[lo:hi], collapse = " ")
}
score_best_expansion <- function(win_txt, acr_here) {
  ctx_vec <- embed_phrase_w2v(win_txt)
  cand_ix <- which(acr_tbl$acr == acr_here)
  if (!length(cand_ix)) return(NULL)

  sims  <- vapply(cand_ix, function(j) cosine_sim(ctx_vec, exp_mat[j, ]), numeric(1))
  sims[is.na(sims)] <- 0
  prior <- acr_tbl$score[cand_ix]; prior[is.na(prior)] <- 0

  s <- alpha * sims + (1 - alpha) * prior
  j <- which.max(s)

  tibble(
    exp_disp = acr_tbl$exp_disp[cand_ix][j],
    exp_uc   = acr_tbl$exp_uc[cand_ix][j],
    sim      = sims[j],
    prior    = prior[j],
    local    = s[j]
  )
}


expand_one <- function(txt, short_flag = FALSE, serial_flag = FALSE, address_flag = FALSE, junky_flag = FALSE) {
  if (is.na(txt) || !nzchar(txt) || serial_flag || address_flag || junky_flag) {
    return(list(text = txt, acrs = character(0), n = 0L, mean_local = NA_real_, ambig = FALSE))
  }
  words <- strsplit(txt, "\\s+")[[1]]
  found <- find_acrs(words)
  if (!nrow(found)) {
    return(list(text = txt, acrs = character(0), n = 0L, mean_local = NA_real_, ambig = FALSE))
  }

  picks <- vector("list", nrow(found))
  for (r in seq_len(nrow(found))) {
    i <- found$pos[r]; a <- found$acr[r]
    win <- context_string(words, i, k_window)
    sc  <- score_best_expansion(win, a)
    if (is.null(sc)) sc <- tibble(exp_disp = NA_character_, exp_uc = NA_character_, sim = 0, prior = 0, local = -Inf)
    picks[[r]] <- sc
  }
  P <- bind_rows(picks)

  local_gate <- if (exists("tau_local_short") && short_flag) max(tau_local_short, tau_local) else tau_local

  if (short_flag && nrow(P)) {
    best <- which.max(P$local)
    P$local[-best] <- -Inf
  }

  replace_ix <- which(!is.na(P$exp_disp) & P$local > local_gate)
  if (length(replace_ix)) {
    for (j in replace_ix) {
      i <- found$pos[j]; a <- found$acr[j]
      words[i] <- paste0(P$exp_disp[j], " (", a, ")")
    }
  }

  amb <- FALSE
  if (exists("acr_lexicon") && "keep_top2" %in% names(acr_lexicon)) {
    m <- match(unique(found$acr), acr_lexicon$acr)
    amb <- any(acr_lexicon$keep_top2[m] %in% TRUE, na.rm = TRUE)
  }

  list(
    text       = paste(words, collapse = " "),
    acrs       = unique(found$acr),
    n          = length(replace_ix),
    mean_local = ifelse(length(replace_ix), mean(P$local[replace_ix], na.rm = TRUE), NA_real_),
    ambig      = amb
  )
}


texts <- txtdf$text_cln
res <- mapply(
  FUN = function(t, sh, se, ad, ju) expand_one(
    t,
    short_flag   = sh,
    serial_flag  = se,
    address_flag = ad,
    junky_flag   = ju
  ),
  t  = texts,
  sh = txtdf$flag_short,
  se = txtdf$flag_serial,
  ad = txtdf$flag_address,
  ju = txtdf$flag_junky,
  SIMPLIFY = FALSE
)

stopifnot(length(res) == nrow(txtdf))


expanded_description <- vapply(res, function(x) x$text, character(1))
acronyms_found       <- lapply(res, function(x) x$acrs)
num_expanded_tokens  <- vapply(res, function(x) x$n, integer(1))
mean_local_score     <- vapply(res, function(x) x$mean_local, numeric(1))
ambiguity_flag       <- vapply(res, function(x) x$ambig, logical(1))

subcontracts <- txtdf %>%
  mutate(
    expanded_description = expanded_description,
    acronyms_found       = acronyms_found,
    num_expanded_tokens  = num_expanded_tokens,
    mean_local_score     = mean_local_score,
    ambiguity_flag       = ambiguity_flag
  ) %>%
  select(-.row_id, -raw_text)

names(subcontracts)
small <- subcontracts %>%  
  select(contains("flag"), expanded_description, acronyms_found, mean_local_score )

serial <- small %>%  
  filter(flag_address == T)



proc_dir <- here("Data", "processed")
if (!dir_exists(proc_dir)) dir_create(proc_dir, recurse = TRUE)
write_rds(w2v_mat,        file.path(proc_dir, "w2v_contracts_matrix.rds"))
write_rds(acr_candidates, file.path(proc_dir, "acr_candidates.rds"))
if (exists("acr_lexicon")) write_rds(acr_lexicon, file.path(proc_dir, "acr_lexicon.rds"))
write_rds(dict_all,       file.path(proc_dir, "dict_all.rds"))
write_rds(exp_map,        file.path(proc_dir, "exp_map.rds"))
write_rds(feat_tbl,       file.path(proc_dir, "feat_tbl.rds"))
write_rds(dict_union,     file.path(proc_dir, "dict_union.rds"))
write_rds(txtdf,          file.path(proc_dir, "subcontracts_with_flags.rds"))
write_rds(subcontracts,   file.path(proc_dir, "subcontracts_expanded.rds"))


```

