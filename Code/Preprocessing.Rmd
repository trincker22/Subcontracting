---
title: "Preprocessing"
output: html_document
date: "2025-08-15"
---

This script pulls and processes 1. the primary contractor's NAICS 2. the primary contractor's PSC code 3. the subcontractor's NAICS code (using SAM API). At the end there is an LLM query that will likely go unused. 


```{r}

rm(list = ls())

library(tidyverse)
library(haven)
library(readxl)
library(readr)
library(jsonlite)
library(janitor)
library(purrr)
library(lubridate)
library(quanteda)
library(quanteda.textmodels)
library(here)
library(tokenizers)
library(topicmodels)
library(syuzhet)
library(caret)
library(irr)
library(SnowballC)
library(ellmer)
library(data.table)
library(e1071)
library(randomForest)
library(glmnet)
library(stringr)
library(glue)
library(digest)
library(tibble)
library(uuid)
library(scales)

options(scipen = 999)


```


# Preprocessing Chunk: Add NAICS and PSC Vars, Compute Priors. 



```{r}

# toggles
force_recompute <- T
run_priors      <- T

# ----------------------------
# helpers
# ----------------------------
rev_for_year <- function(y) dplyr::case_when(y <= 2011 ~ 2007L,
                                             y <= 2016 ~ 2012L,
                                             y <= 2021 ~ 2017L,
                                             TRUE ~ 2022L)

prep_crosswalk <- function(cw, from_col, to_col, weight_col = NULL) {
  cw %>%
    transmute(from = str_pad(as.character(.data[[from_col]]), 6, "left", "0"),
              to   = str_pad(as.character(.data[[to_col]]),   6, "left", "0"),
              w    = if (is.null(weight_col)) NA_real_ else suppressWarnings(as.numeric(.data[[weight_col]]))) %>%
    filter(nzchar(from), nzchar(to)) %>%
    distinct(from, to, .keep_all = TRUE) %>%
    group_by(from) %>%
    mutate(w = ifelse(is.na(w), 1/dplyr::n(), w / sum(w, na.rm = TRUE))) %>%
    ungroup()
}

map_step <- function(df, cw, rev_from, rev_to) {
  a <- df %>% filter(rev == rev_from)
  b <- df %>% filter(rev != rev_from)
  if (nrow(a) == 0) return(df)
  a %>%
    mutate(code = str_pad(as.character(code), 6, "left", "0")) %>%
    left_join(cw, by = c("code" = "from"), relationship = "many-to-many") %>%
    mutate(code   = coalesce(to, code),
           weight = weight * coalesce(w, 1),
           rev    = ifelse(is.na(to), rev, rev_to),
           map_path = ifelse(is.na(to), map_path, paste0(map_path, "|", rev_from, "â†’", rev_to))) %>%
    select(-to, -w) %>%
    bind_rows(b, .)
}

standardize_naics_to_2022 <- function(df, naics_col = "naics_code", year_col = "year",
                                      cw_07_12_p, cw_12_17_p, cw_17_22_p) {
  base <- df %>%
    mutate(.row_id = row_number(),
           code = str_pad(as.character(.data[[naics_col]]), 6, "left", "0"),
           rev = rev_for_year(as.integer(.data[[year_col]])),
           weight = 1.0,
           map_path = "orig")
  s1 <- map_step(base, cw_07_12_p, 2007L, 2012L)
  s2 <- map_step(s1,   cw_12_17_p, 2012L, 2017L)
  s3 <- map_step(s2,   cw_17_22_p, 2017L, 2022L)
  s3 %>% mutate(naics_2022 = code)
}

normalize_weights <- function(df) {
  df %>% group_by(.row_id) %>%
    mutate(weight = ifelse(sum(weight, na.rm = TRUE) > 0,
                           weight / sum(weight, na.rm = TRUE), weight)) %>%
    ungroup()
}

collapse_to_single_naics <- function(df_std) {
  df_std %>% group_by(.row_id) %>% slice_max(order_by = weight, n = 1, with_ties = FALSE) %>% ungroup()
}

prep_naics2022_key_from_clean <- function(df_clean) {
  df_clean %>%
    clean_names() %>%
    transmute(code  = as.character(x2022_naics_us_code),
              title = as.character(x2022_naics_us_title)) %>%
    mutate(code = trimws(code), title = trimws(title)) %>%
    filter(!is.na(code), !is.na(title), code != "", title != "") %>%
    distinct(code, .keep_all = TRUE)
}

attach_naics_titles_2022 <- function(df, code_col = "naics_2022", key_df, levels = c(6,3,2), keep_codes = FALSE) {
  if (!all(c("code","title") %in% names(key_df))) key_df <- prep_naics2022_key_from_clean(key_df)
  key_df <- key_df %>% mutate(code = as.character(code), n = nchar(code)) %>% filter(n %in% levels) %>% distinct(code, .keep_all = TRUE)
  out <- df %>% mutate(`._code6` = substr(as.character(.data[[code_col]]), 1, 6))
  for (L in levels) {
    look <- key_df %>% filter(n == L) %>% select(code, title)
    col_code  <- paste0("naics", L, "_code")
    col_title <- paste0("naics", L, "_title")
    out <- out %>%
      mutate(!!col_code := substr(`._code6`, 1, L)) %>%
      left_join(look, by = setNames("code", col_code)) %>%
      rename(!!col_title := title)
    if (!keep_codes) out <- out %>% select(-all_of(col_code))
  }
  out %>% select(-`._code6`)
}

normalize_psc_code <- function(x) {
  x <- as.character(x); x <- str_squish(x); x <- str_replace(x, "^(\\d+)\\.0+$", "\\1"); x
}

read_psc_2025 <- function(path, sheet = 1) {
  hdr <- read_xlsx(path, sheet = sheet, n_max = 0)
  ct  <- rep("guess", length(hdr)); ct[1] <- "text"
  read_xlsx(path, sheet = sheet, col_types = ct, .name_repair = "minimal") %>%
    clean_names() %>%
    mutate(
      psc_code = normalize_psc_code(psc_code),
      product_and_service_code_name = str_squish(as.character(product_and_service_code_name)),
      start_date = suppressWarnings(mdy(as.character(start_date))),
      end_date   = suppressWarnings(mdy(as.character(end_date)))
    )
}

dedupe_psc_title_to_code <- function(PSC,
                                     code_col  = "psc_code",
                                     title_col = "product_and_service_code_name",
                                     start_col = "start_date",
                                     end_col   = "end_date") {
  PSC %>%
    mutate(
      code   = normalize_psc_code(.data[[code_col]]),
      title  = as.character(.data[[title_col]]),
      start  = suppressWarnings(mdy(as.character(.data[[start_col]]))),
      end    = suppressWarnings(mdy(as.character(.data[[end_col]]))),
      title_norm = str_squish(title),
      code_len   = nchar(gsub("[^A-Za-z0-9]", "", code)),
      active     = ifelse(is.na(end), 1L, 0L)
    ) %>%
    distinct(code, title_norm, .keep_all = TRUE) %>%
    group_by(title_norm) %>%
    arrange(desc(code_len), desc(active), desc(start), code, .by_group = TRUE) %>%
    slice(1) %>%
    ungroup() %>%
    transmute(psc_title = title_norm, psc_code = code)
}

norm_title <- function(x) str_squish(str_to_upper(as.character(x)))

ensure_dir <- function(path) { if (!fs::dir_exists(path)) fs::dir_create(path, recurse = TRUE); path }

attach_psc <- function(df, psc_map) {
  if (!"product_or_service_code_descript" %in% names(df)) {
    alt_psc <- intersect(c("product_or_service_code_description","psc_description","psc_title"),
                         names(df))
    if (length(alt_psc) == 1) df <- df %>% rename(product_or_service_code_descript = !!alt_psc)
  }
  if ("product_or_service_code_descript" %in% names(df)) {
    df <- df %>%
      mutate(psc_title = norm_title(product_or_service_code_descript)) %>%
      left_join(psc_map, by = "psc_title") %>%
      select(-psc_title)
  }
  df
}

attach_naics <- function(df, cw_07_12_p, cw_12_17_p, cw_17_22_p, codes22_key) {
  if (!"year" %in% names(df) & "prime_date" %in% names(df)) {
    df <- df %>% mutate(prime_date = suppressWarnings(as.Date(prime_date)),
                        year = year(prime_date))
  }
  if (!"naics_code" %in% names(df)) return(df)
  std <- standardize_naics_to_2022(df, naics_col = "naics_code", year_col = "year",
                                   cw_07_12_p = cw_07_12_p, cw_12_17_p = cw_12_17_p, cw_17_22_p = cw_17_22_p)
  std <- normalize_weights(std)
  std_one <- collapse_to_single_naics(std)
  std_one <- attach_naics_titles_2022(std_one, code_col = "naics_2022",
                                      key_df = codes22_key, levels = c(6,3,2))
  df %>% mutate(.row_id = row_number()) %>%
    left_join(std_one %>% select(.row_id, naics_2022, naics6_title, naics3_title, naics2_title),
              by = ".row_id") %>%
    select(-.row_id)
}

process_one_df <- function(df, source_name = "subcontracts",
                           cw_07_12_p, cw_12_17_p, cw_17_22_p, codes22_key, psc_map) {
  key_candidate <- intersect(c("prime_id_num","prime_id"), names(df))
  if (length(key_candidate) >= 1) {
    best_key <- key_candidate[1]
    df <- df %>% mutate(obs_id = as.character(.data[[best_key]]))
  } else {
    df <- df %>% mutate(obs_id = as.character(row_number()))
  }
  if ("prime_date" %in% names(df)) {
    df <- df %>% mutate(prime_date = suppressWarnings(as.Date(prime_date)),
                        year = if (!"year" %in% names(df)) year(prime_date) else year)
  }
  if (!"nom_g" %in% names(df)) {
    valcand <- intersect(c("obligated_amount","action_obligation","amount","value"),
                         names(df))
    if (length(valcand) == 1) df <- df %>% rename(nom_g = !!valcand)
  }
  df <- attach_naics(df, cw_07_12_p, cw_12_17_p, cw_17_22_p, codes22_key)
  df <- attach_psc(df, psc_map)
  df
}

# ----------------------------
# paths / cache
# ----------------------------
cw_07_12 <- here("Data","NAICSCrosswalks","2012_to_2007_NAICS.xls")
cw_12_17 <- here("Data","NAICSCrosswalks","2012_to_2017_NAICS.xlsx")
cw_17_22 <- here("Data","NAICSCrosswalks","2022_to_2017_NAICS.xlsx")
key_path  <- here("Data","NAICSCrosswalks","2022_codes.xlsx")
psc_path  <- here("Data","PSC0425.xlsx")

cache_dir <- ensure_dir(here("Data","processed"))
cache_primary_path      <- file.path(cache_dir, "primary.rds")
cache_subcontracts_path <- file.path(cache_dir, "subcontracts.rds")

# ----------------------------
# lookups
# ----------------------------
cw_07_12_p <- if (file.exists(cw_07_12)) prep_crosswalk(read_xls(cw_07_12), from_col = "2007 NAICS Code", to_col = "2012 NAICS Code") else NULL
cw_12_17_p <- if (file.exists(cw_12_17)) prep_crosswalk(read_xlsx(cw_12_17), from_col = "2012 NAICS Code", to_col = "2017 NAICS Code") else NULL
cw_17_22_p <- if (file.exists(cw_17_22)) prep_crosswalk(read_xlsx(cw_17_22), from_col = "2017 NAICS Code", to_col = "2022 NAICS Code") else NULL

codes2022  <- if (file.exists(key_path)) read_xlsx(key_path) %>% clean_names() else tibble(code=character(), title=character())
codes22_key <- if (nrow(codes2022)) prep_naics2022_key_from_clean(codes2022) else tibble(code=character(), title=character())

PSC <- if (file.exists(psc_path)) read_psc_2025(psc_path) else tibble()
psc_map <- if (nrow(PSC)) dedupe_psc_title_to_code(PSC) %>%
  mutate(psc_title = norm_title(psc_title)) %>% distinct(psc_title, .keep_all = TRUE) else
  tibble(psc_title=character(), psc_code=character())

# ----------------------------
# process: subcontracts
# ----------------------------
if (file.exists(cache_subcontracts_path) & !force_recompute) {
  subcontracts <- readRDS(cache_subcontracts_path)
} else if (exists("subcontracts")) {
  subcontracts <- process_one_df(subcontracts, "subcontracts",
                                 cw_07_12_p, cw_12_17_p, cw_17_22_p,
                                 codes22_key, psc_map)
  saveRDS(subcontracts, cache_subcontracts_path)
}

# ----------------------------
# process: primary
# ----------------------------
create_primary_from_subcontracts <- function(sc) {
  if ("prime_id" %in% names(sc)) {
    sc %>% group_by(prime_id) %>% slice_head(n = 1) %>% ungroup()
  } else if ("prime_id_num" %in% names(sc)) {
    sc %>% group_by(prime_id_num) %>% slice_head(n = 1) %>% ungroup()
  } else {
    sc %>% distinct(.keep_all = TRUE)
  }
}

if (file.exists(cache_primary_path) & !force_recompute) {
  primary <- readRDS(cache_primary_path)
} else {
  if (!exists("primary") && exists("subcontracts")) {
    primary <- create_primary_from_subcontracts(subcontracts)
  }
  if (exists("primary")) {
    primary <- process_one_df(primary, "primary",
                              cw_07_12_p, cw_12_17_p, cw_17_22_p,
                              codes22_key, psc_map)
    saveRDS(primary, cache_primary_path)
  }
}

# ----------------------------
# priors / audit if run_priors=T
# ----------------------------
if (run_priors) {
  if (exists("subcontracts") && "naics_2022" %in% names(subcontracts) &&
      "product_or_service_code_descript" %in% names(subcontracts)) {
    naics_prior_2022 <- subcontracts %>%
      filter(!is.na(naics_2022), !is.na(product_or_service_code_descript)) %>%
      group_by(naics_2022, product_or_service_code_descript) %>%
      summarise(n = dplyr::n(), .groups = "drop_last") %>%
      group_by(naics_2022) %>%
      mutate(p = n / sum(n)) %>%
      ungroup()
  }
  if (exists("primary") && "naics_code" %in% names(primary) && "naics_2022" %in% names(primary)) {
    audit <- primary %>%
      mutate(code_orig = str_pad(as.character(naics_code), 6, "left", "0"),
             changed  = !is.na(naics_2022) & code_orig != naics_2022) %>%
      select(obs_id, year, code_orig, naics_2022, changed)
  }
}


subcontracts[1,]
glimpse(subcontracts)

```

# Next: Add NAICS Pull given Subcontractor UEI 

```{r}

api_key <- Sys.getenv("SAM_API_KEY")
nchar(api_key)


base_url <- "https://api.sam.gov/entity-information/v3/entities"
page_size <- 10

# --- single-UEI smoke test (prints status + body) ---
uei_one <- "MQTRV3XU9LZ3"
resp <- request(base_url) |>
  req_headers(Accept = "application/json") |>
  req_url_query(api_key = api_key, includeSections = "coreData", size = page_size, page = 1, ueiSAM = uei_one) |>
  req_perform()
print(resp_status(resp))
cat(resp_body_string(resp), "\n")

# --- batched test (up to 100 UEIs per request, with paging) ---
test_ueis <- c("N5SBUSN8R3M9","FEJJGETNAP31","J4NXZFGSEBF4")

build_query <- function(uei_batch, page) {
  base <- list(api_key = api_key, includeSections = "coreData", size = page_size, page = page)
  uei_parts <- rlang::set_names(as.list(unname(uei_batch)), rep("ueiSAM", length(uei_batch)))
  c(base, uei_parts)
}

fetch_batch <- function(uei_batch) {
  page <- 1L
  out <- list()
  repeat {
    q <- build_query(uei_batch, page)
    r <- request(base_url) |>
      req_headers(Accept = "application/json") |>
      req_url_query(!!!q) |>
      req_perform()
    j <- resp_body_json(r, simplifyVector = TRUE)
    items <- if (!is.null(j$entityData)) j$entityData else list()
    if (!length(items)) break
    out <- c(out, items)
    if (length(items) < page_size) break
    page <- page + 1L
  }
  out
}

raw <- fetch_batch(test_ueis)
entities <- map_df(raw, as_tibble)

naics_long <- entities |>
  transmute(ueiSAM, legalBusinessName, naicsList) |>
  mutate(naicsList = map(naicsList, ~{
    if (is.null(.x) || !length(.x)) tibble(naicsCode = character(), naicsDescription = character())
    else as_tibble(.x)[, c("naicsCode","naicsDescription")]
  })) |>
  unnest(naicsList) |>
  distinct(ueiSAM, naicsCode, .keep_all = TRUE) |>
  mutate(naicsCode = str_pad(naicsCode, width = 6, side = "right", pad = "0"))

print(head(entities, 3))
print(head(naics_long, 10))


```

# LLM Section

There are two prompts that follow a similar workflow. The first uses LLM to translate codes in the subcontractors description into readable text. For example, quite often there will just be a description like IGF::OT::IGF, which, if you look in the DoD handbook, signifies "Inherently Governmental Functions: Other functions." The idea is that the LLM will translate these acronyms or codes instead of doing it by hand, which would be quite time intensive. The second prompt classifies the relationship between the primary contract category and the specific description of the contract given by the subcontractors.

First, I construct a safe call function, which limits the frequency of LLM queries so that we do not repeatedly hit quotas. When it fails, it adds sleep time as powers of 2, increasing with the number of failed tries. Next, because we ask the LLM to return output in JSON format, the JSON parser uses regex to crop out only the text that fits the variables we define. (For ex., it will remove "Sure, the readable text is below!" from the output). It gives a try-error if the output cannot be parsed into JSON, which will also be flagged as F under the parsed_ok variable. If any columns of the tibble built off of the JSON structure are missing, another function will fill with NAs. This all helps to identify failures.

Next, the batch runner works for both prompts. It creates batches size 20 (the number of inputs before the output tibble is saved to checkpoint), and runs each row in the batch through all of the functions above. The output has 5 columns: the observation id, the raw JSON output from the LLM, the parsed output as returned_text, a flag indicating whether the parsing was successful, and notes from the LLM about why it made its decision (people report that LLMs have better accuracy when you ask them to justify their decisions).

These functions let you run the data in chunks. So, you generate the needs_readable df, which is (all observations) - (previously processed obs), identified by the observation id, NOT ROW INDEX (which can change). then it will run the llm over the needs_readable, it will join these results in with your existing processed output as readable_all, and these will overwrite your original file with the new LLM variables, matched by observation id. The structure is the same for the relation prompt run, the dfs just have different names. In the end, both runs write into their own checkpoint files, and their final results into contracts.

There are major computational limitations. The Gemini API that I used here only takes 250 queries a day, even with aggressive batching (200 obs per query), it would take 8 days. Alternatively, I could try to run a "local" model using the remote computers or the BigTex computing. The problem is that these have CPUs, which are significantly slower than GPUs at this task. It is difficult to estimate the amount of time it would take on a CPU, but GPU can be estimated (probably a dayish? but would be paid)

The cost of using a GPU depends on the number of tokens used, which determines the amount of time that the computation takes. The number of tokens depends on the length of the prompt, which depends on batch size. The batch size depends on the accuracy level of a given batch size. So in order to know for sure the cost of using a GPU, would have to test on a few batch sizes, which would be paid.

**Alternatives** :

The codes/acronyms issue can probably be solved without an LLM. There are dictionaries and methods for creating dictionaries with a large corpus (somewhere else in the corpus, there will be text like "Acronym Code (AC)"). But the relationship is more difficult. This is because in general, text based models can categorize a set of text in to groups, like in an LDA model, and then with supervised counterparts. So for example, if you treat each seperate word as its own variable and construct a matrix where the rows are the texts and the columns are the frecquences of each distinct word that appears in all of the texts, then you could regress on the words.

So for ex., if you were categorizing news, then the words "football" and "player" would have large coefficients for the sports category. This is frequence based.



```{r}

chat <- ellmer::chat_google_gemini()

contracts <- subcontracts %>%
  mutate(
    obs_id        = as.character(obs_id),
    broad_cat     = product_or_service_code_descript,
    specific_desc = subaward_description,
    naics         = coalesce(naics_title, as.character(naics_code))
  ) %>%
  select(obs_id, broad_cat, specific_desc, naics) %>%
  distinct(obs_id, .keep_all = TRUE)

prompt_readable <- function(broad, specific) {
  glue(
    "{{\n  \"task\": \"rewrite_to_plain_english\",\n  \"schema\": {{\n    \"returned_text\": \"string\",\n    \"notes\": \"string\"\n  }},\n  \"instructions\": \"You are given a general contract category and a specific subaward description. If the description is already human-readable, return it unchanged. If it is code-like or obscure, return a brief plain-English description. Be conservative: no hallucinated details; if in doubt, summarize functionally.\",\n  \"category\": \"{broad}\",\n  \"description\": \"{specific}\",\n  \"output_format\": \"JSON with fields returned_text, notes\"\n}}"
  )
}

prompt_relation <- function(broad, specific) {
  glue(
    "{{\n  \"task\": \"relation_classification\",\n  \"schema\": {{\n    \"relation\": \"enum[same, subset, unrelated, unclear]\",\n    \"rationale\": \"string\"\n  }},\n  \"instructions\": \"Classify the relationship between the broad category and the subtask. same = equivalent meaning; subset = subtask is a specialization of the category; unrelated = different domains; unclear = insufficient info.\",\n  \"category\": \"{broad}\",\n  \"subtask\": \"{specific}\",\n  \"output_format\": \"JSON with fields relation, rationale\"\n}}"
  )
}

call_chat_safe <- function(prompt, max_tries = 5, base_sleep = 2) {
  for (i in seq_len(max_tries)) {
    txt <- try({
      captured <- paste(utils::capture.output(res <- chat$chat(prompt)), collapse = "\n")
      out <- if (is.character(res) && nzchar(res)) res else captured
      out
    }, silent = TRUE)
    if (!inherits(txt, "try-error") && is.character(txt) && nzchar(txt)) return(txt)
    Sys.sleep(base_sleep * (2^(i - 1)) + runif(1, 0, 0.5))
  }
  NA_character_
}

parse_json_relaxed <- function(txt) {
  if (is.na(txt)) return(tibble(parsed_ok = FALSE))
  m1 <- stringr::str_match(txt, "(?s)```(?:json)?\\s*(\\{[\\s\\S]*?\\})\\s*```")
  if (!is.na(m1[, 2])) {
    json_txt <- m1[, 2]
  } else {
    m2 <- stringr::str_match_all(txt, "\\{[\\s\\S]*?\\}")
    if (length(m2) && nrow(m2[[1]]) > 0) json_txt <- tail(m2[[1]][, 1], 1) else json_txt <- txt
  }
  out <- try(jsonlite::fromJSON(json_txt), silent = TRUE)
  if (inherits(out, "try-error")) tibble(parsed_ok = FALSE)
  else dplyr::bind_cols(tibble(parsed_ok = TRUE), tibble::as_tibble(out))
}

ensure_cols <- function(df, cols) {
  missing <- setdiff(cols, names(df))
  for (nm in missing) df[[nm]] <- NA_character_
  df
}

run_llm_over_df <- function(df, broad_col, specific_col,
                            which_prompt = c("readable", "relation"),
                            batch_size = 20,
                            checkpoint_path = NULL,
                            temperature = 0.0,
                            verbose = TRUE) {
  which_prompt <- match.arg(which_prompt)
  df <- df %>% mutate(obs_id = as.character(.data[["obs_id"]]))
  n <- nrow(df)
  if (n == 0) return(tibble())
  idx <- split(seq_len(n), ceiling(seq_along(seq_len(n)) / batch_size))
  results <- vector("list", length(idx))
  for (bi in seq_along(idx)) {
    rows <- df[idx[[bi]], , drop = FALSE]
    rows_small <- tibble::tibble(
      obs_id = rows$obs_id,
      broad  = rows[[broad_col]],
      spec   = rows[[specific_col]]
    )
    prompts <- purrr::pmap(rows_small, function(obs_id, broad, spec) {
      if (which_prompt == "readable") prompt_readable(broad, spec) else prompt_relation(broad, spec)
    })
    raw    <- purrr::map(prompts, ~ call_chat_safe(.x))
    parsed <- purrr::map_dfr(raw, parse_json_relaxed)
    out <- tibble::tibble(
      obs_id = rows$obs_id,
      raw    = unlist(raw, use.names = FALSE)
    ) %>% dplyr::bind_cols(parsed)
    results[[bi]] <- out
    if (!is.null(checkpoint_path)) {
      readr::write_rds(dplyr::bind_rows(results[seq_len(bi)]), checkpoint_path)
      if (verbose) message(glue("[{bi}/{length(idx)}] wrote checkpoint: {checkpoint_path} (rows so far: {nrow(dplyr::bind_rows(results[seq_len(bi)]))})"))
    }
  }
  dplyr::bind_rows(results)
}

readable_ckpt <- here("Checkpoints/readable_checkpoint.rds")
relation_ckpt <- here("Checkpoints/relation_checkpoint.rds")

if (!file.exists(readable_ckpt)) {
  write_rds(
    tibble(
      obs_id        = character(),
      raw           = character(),
      parsed_ok     = logical(),
      returned_text = character(),
      notes         = character()
    ),
    readable_ckpt
  )
}

if (!file.exists(relation_ckpt)) {
  write_rds(
    tibble(
      obs_id    = character(),
      raw       = character(),
      parsed_ok = logical(),
      relation  = character(),
      rationale = character()
    ),
    relation_ckpt
  )
}

prev_readable <- if (file.exists(readable_ckpt)) readr::read_rds(readable_ckpt) else tibble(obs_id = character())
prev_rel      <- if (file.exists(relation_ckpt)) readr::read_rds(relation_ckpt) else tibble(obs_id = character())

needs_readable <- dplyr::anti_join(
  contracts %>% dplyr::select(obs_id, broad_cat, specific_desc),
  prev_readable %>% dplyr::select(obs_id),
  by = "obs_id"
)

readable_new <- if (nrow(needs_readable) > 0) {
  run_llm_over_df(
    df = needs_readable,
    broad_col = "broad_cat",
    specific_col = "specific_desc",
    which_prompt = "readable",
    batch_size = 20,
    checkpoint_path = readable_ckpt,
    verbose = TRUE
  )
} else tibble()

readable_all <- dplyr::bind_rows(prev_readable, readable_new) %>%
  dplyr::distinct(obs_id, .keep_all = TRUE) %>%
  ensure_cols(c("returned_text", "notes"))

contracts <- contracts %>%
  dplyr::left_join(
    readable_all %>%
      dplyr::transmute(
        obs_id,
        readable_text  = dplyr::coalesce(returned_text, NA_character_),
        readable_notes = dplyr::coalesce(notes,         NA_character_)
      ),
    by = "obs_id"
  )

needs_rel <- dplyr::anti_join(
  contracts %>% dplyr::select(obs_id, broad_cat, specific_desc),
  prev_rel %>% dplyr::select(obs_id),
  by = "obs_id"
)

rel_new <- if (nrow(needs_rel) > 0) {
  run_llm_over_df(
    df = needs_rel,
    broad_col = "broad_cat",
    specific_col = "specific_desc",
    which_prompt = "relation",
    batch_size = 20,
    checkpoint_path = relation_ckpt,
    verbose = TRUE
  )
} else tibble()

rel_all <- dplyr::bind_rows(prev_rel, rel_new) %>%
  dplyr::distinct(obs_id, .keep_all = TRUE) %>%
  ensure_cols(c("relation", "rationale"))

contracts <- contracts %>%
  dplyr::left_join(
    rel_all %>%
      dplyr::transmute(
        obs_id,
        relation            = ifelse(parsed_ok, relation,  NA_character_),
        relation_rationale  = ifelse(parsed_ok, rationale, NA_character_)
      ),
    by = "obs_id"
  )


```

```{r}


```
