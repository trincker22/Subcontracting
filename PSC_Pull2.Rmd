---
title: "PSC_Pull2"
output: html_document
date: "2025-09-22"
---


```{r}

library(data.table)
library(digest)
library(dplyr)
library(fs)
library(here)
library(httr2)
library(jsonlite)
library(parallel)
library(purrr)
library(readr)
library(quanteda)
library(readxl)
library(stringdist)
library(stringr)
library(tibble)
library(tidyr)
library(tidytext)
library(spacyr)
library(stringi)
library(ggplot2)
library(scales)

options(scipen = 999)
options(mc.cores = 7)
`%||%` <- function(a,b) if (is.null(a) || (is.atomic(a) && length(a)==1 && is.na(a))) b else a


```


```{r}

to_sentence_case <- function(x){ x2 <- str_to_lower(x); gsub("(^|[\\.\\!\\?]\\s+)([a-z])","\\1\\U\\2",x2,perl=TRUE) }
collapse_phrase_acronym_echo <- function(x){ x <- gsub("\\b([A-Za-z][A-Za-z\\s]{2,}?)(\\s*\\(([A-Z]{2,6})\\))","\\1",x,perl=TRUE); gsub("\\b([A-Z]{2,6})\\s*\\(\\1\\)","\\1",x,perl=TRUE) }
strip_admin_tails <- function(x){ x <- gsub("\\b(PERIOD OF PERFORMANCE|PRIME CONTRACT NO\\.|SUBCONTRACT NO\\.|MODIFICATION NO\\.)\\b[^\\.;\\n]*","",x,perl=TRUE); gsub("\\b(FUNDED BY|REQUESTED BY)\\b[^\\.;\\n]*","",x,perl=TRUE) }
drop_serials_ids_dates <- function(x){ x <- gsub("\\b[A-Z]{2,}\\d[\\w\\-\\.]{4,}\\b","",x,perl=TRUE); x <- gsub("\\b\\d{4}\\b","",x,perl=TRUE); x <- gsub("\\b(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)[A-Z]*\\s+\\d{1,2},?\\s*\\d{4}\\b","",x,perl=TRUE); gsub("\\bfrom\\s+[^\\s]+\\s+to\\s+[^\\s]+","",x,ignore.case=TRUE,perl=TRUE) }
dedup_ngrams <- function(x){ w <- unlist(strsplit(x,"\\s+")); if (length(w)<6) return(str_squish(x)); res <- w[1]; for(i in 2:length(w)){ prev5 <- tail(res,5); cand <- w[i]; if (length(prev5) && tail(prev5,1)==cand) next; res <- c(res,cand) }; str_squish(paste(res,collapse=" ")) }
preclean_text <- function(x){ x %>% to_sentence_case() %>% collapse_phrase_acronym_echo() %>% strip_admin_tails() %>% drop_serials_ids_dates() %>% dedup_ngrams() %>% str_squish() }

split_sentences_df <- function(text_vec, max_tokens=60, max_chars=400){
  txt <- vapply(text_vec, preclean_text, character(1))
  ss <- stringi::stri_split_boundaries(txt, type="sentence")
  row_id <- rep(seq_along(ss), lengths(ss))
  sents <- unlist(ss, use.names=FALSE)
  keep <- nzchar(sents)
  sents <- sents[keep]; row_id <- row_id[keep]
  sents <- vapply(sents, preclean_text, character(1))
  tok_counts <- stringi::stri_count_words(sents)
  keep2 <- (tok_counts <= max_tokens) & (nchar(sents) <= max_chars)
  data.frame(row_id=row_id[keep2], sentence=sents[keep2], stringsAsFactors=FALSE)
}

admin_penalty_vec <- function(x){
  nchar_x <- nchar(x)
  up_ratio <- ifelse(nchar_x>0, vapply(strsplit(x,""), function(cs) sum(cs %in% LETTERS), numeric(1))/nchar_x, 0)
  dig_sym  <- str_count(x,"[0-9]")+str_count(x,"[-_/\\.]")
  dates    <- str_count(x,"\\b(19|20)\\d{2}\\b|\\bjan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec\\b")
  id_like  <- str_count(x,"\\b[A-Z]{2,}\\d[\\w\\-]{3,}\\b")
  enum_p   <- str_count(x,",")+str_count(x,";")
  0.6*pmin(up_ratio,1)+0.1*pmin(dig_sym/6,1)+0.1*pmin(dates,3)/3+0.1*pmin(id_like,3)/3+0.1*pmin(enum_p,5)/5
}
verbiness_spacy <- function(sents){
  parsed <- spacy_parse(sents, pos=TRUE, tag=FALSE, entity=FALSE, dependency=FALSE)
  if (!nrow(parsed)) return(rep(0,length(sents)))
  agg <- parsed |> group_by(doc_id) |> summarise(n_verbs=sum(pos=="VERB",na.rm=TRUE), n_toks=n(), has_obj=any(lag(pos)=="VERB" & pos=="NOUN",na.rm=TRUE), .groups="drop")
  score <- (agg$n_verbs/pmax(agg$n_toks,1))+ifelse(agg$has_obj,0.3,0)
  order_ids <- as.character(seq_along(sents)); idx <- match(order_ids, agg$doc_id)
  res <- numeric(length(sents)); res[!is.na(idx)] <- score[idx[!is.na(idx)]]; res
}
tfidf_sentences <- function(sents, idf_vocab){
  toks <- tokens(sents, remove_punct=TRUE, remove_numbers=TRUE)
  toks <- tokens_tolower(toks)
  toks <- tokens_remove(toks, stopwords("en"))
  dfm_s <- dfm(toks); feats <- featnames(dfm_s)
  common <- intersect(feats, names(idf_vocab)); if (!length(common)) return(rep(0,nrow(dfm_s)))
  dfm_s <- dfm_s[, common, drop=FALSE]; as.numeric(dfm_s %*% idf_vocab[common])
}
assemble_ranked <- function(sentences, scores, max_chars=250, max_units=3){
  if (!length(sentences)) return("")
  ord <- order(scores, decreasing=TRUE); selected <- character(0); used <- 0L
  for (i in ord){ cand <- sentences[i]; if (!nzchar(cand)) next; add <- ifelse(length(selected)>0,1L,0L)+nchar(cand); if (used+add>max_chars) next; selected <- c(selected,cand); used <- used+add; if (length(selected)>=max_units || used>=max_chars-1L) break }
  if (!length(selected)) { longest <- sentences[which.max(nchar(sentences))]; return(str_squish(substr(longest,1L,max_chars))) }
  str_squish(substr(paste(selected,collapse=" "),1L,max_chars))
}
build_global_idf <- function(text_vec){
  text_vec <- as.character(text_vec); text_vec[is.na(text_vec)] <- ""
  text_vec <- vapply(text_vec, preclean_text, character(1))
  toks <- tokens(text_vec, remove_punct=TRUE, remove_numbers=TRUE)
  toks <- tokens_tolower(toks)
  toks <- tokens_remove(toks, stopwords("en"))
  dfm  <- dfm(toks)
  N    <- ndoc(dfm); df <- Matrix::colSums(dfm>0)
  idf  <- log((N+1)/(df+1))
  structure(as.numeric(idf), names = featnames(dfm))
}



```


```{r}


make_psc_query_fast <- function(df, subawardee_col="subawardee_name", expanded_col="expanded_description", max_chars=250, idf_vocab){
  desc <- tidyr::replace_na(as.character(df[[expanded_col]]), "")
  suba <- tidyr::replace_na(as.character(df[[subawardee_col]]), "")
  desc_clean <- vapply(desc, preclean_text, character(1))
  prefixes <- ifelse(nzchar(suba), paste0(suba, ": "), ""); rooms <- pmax(0L, max_chars - nchar(prefixes))
  sent_df <- split_sentences_df(desc)
  if (!nrow(sent_df)) return(mutate(df, psc_query_250=str_squish(substr(paste0(prefixes,desc_clean),1L,max_chars))))
  vscore <- verbiness_spacy(sent_df$sentence)
  apen   <- admin_penalty_vec(sent_df$sentence)
  len    <- nchar(sent_df$sentence); lbon <- scales::rescale(pmax(0, pmin(len,160)-40), to=c(0,0.15))
  tfidf  <- tfidf_sentences(sent_df$sentence, idf_vocab)
  sent_df$score <- 0.8*tfidf + 1.0*vscore - 0.7*apen + 0.3*lbon
  row_splits <- split(seq_len(nrow(sent_df)), sent_df$row_id)
  assemble_row <- function(j){ ix <- row_splits[[j]]; assembled <- assemble_ranked(sent_df$sentence[ix], sent_df$score[ix], max_chars=rooms[j], max_units=3); if (!nzchar(assembled)) str_squish(substr(desc_clean[j],1L,rooms[j])) else assembled }
  assembled <- vapply(seq_along(row_splits), assemble_row, character(1))
  out <- str_squish(substr(paste0(prefixes,assembled),1L,max_chars))
  mutate(df, psc_query_250=out)
}

make_psc_keys_strict <- function(df, key_cols, query_col="psc_query_250"){
  stopifnot(all(key_cols %in% names(df)))
  df2 <- df %>%
    mutate(across(all_of(key_cols), ~ as.character(.))) %>%
    mutate(.any_na = do.call(pmax, c(across(all_of(key_cols), ~ as.numeric(is.na(.))), list(na.rm=TRUE)))) %>%
    mutate(.id_combo = do.call(paste, c(across(all_of(key_cols)), sep="|")))
  if (any(df2$.any_na > 0)) stop("key_cols contain NA; cannot build stable .psc_key")
  key_dups <- df2 %>% count(.id_combo) %>% filter(n>1)
  if (nrow(key_dups)>0) stop("key_cols are not unique; cannot build stable .psc_key")
  df2 %>%
    mutate(.psc_key = vapply(paste0("ID|", .id_combo), digest, character(1), algo="xxhash64")) %>%
    select(-.any_na, -.id_combo)
}



```


```{r}


FSCPSC_API  <- "https://api.fscpsc.com/searches"
FSCPSC_MIME <- "application/vnd.api+json"

parse_fscpsc_jsonapi <- function(jj){
  inc <- jj$included %||% list()
  items <- inc[vapply(inc, function(x) is.list(x) && identical(x$type,"product-service-codes"), logical(1))]
  if (!length(items)) return(tibble(code=character(),name=character(),score=double()))
  rows <- lapply(items, function(x){
    code <- toupper(as.character(x$id %||% x$attributes$id %||% ""))
    name <- as.character(x$attributes$name %||% x$attributes$title %||% x$attributes$`full-name` %||% NA_character_)
    tibble(code=code, name=name)
  }) |> bind_rows() |> filter(nchar(code)==4) |> distinct(code, .keep_all=TRUE)
  rel_a <- {
    rel <- jj$data$relationships[["product-service-codes"]]
    if (!is.list(rel)) tibble(code=character(),score=double()) else {
      dat <- rel$data %||% list()
      if (!length(dat)) tibble(code=character(),score=double()) else bind_rows(lapply(dat, function(x){
        meta <- x$meta %||% list(); assoc <- if (!is.null(meta$association)) suppressWarnings(as.numeric(meta$association)) else NA_real_
        tibble(code=toupper(as.character(x$id %||% "")), score=assoc)
      })) |> distinct(code,.keep_all=TRUE)
    }
  }
  rel_b <- {
    if (!length(items)) tibble(code=character(),score=double()) else bind_rows(lapply(items, function(x){
      rel  <- x$relationships$searches; if (!is.list(rel)) return(tibble(code=character(),score=double()))
      dat  <- rel$data %||% list(); if (!length(dat)) return(tibble(code=character(),score=double()))
      m    <- dat[[1]]$meta %||% list(); assoc <- if (!is.null(m$association)) suppressWarnings(as.numeric(m$association)) else NA_real_
      tibble(code=toupper(as.character(x$id %||% "")), score=assoc)
    })) |> distinct(code,.keep_all=TRUE)
  }
  smap <- full_join(rel_a, rel_b, by="code", suffix=c("_a","_b")) |> transmute(code, score=coalesce(score_a,score_b))
  left_join(rows, smap, by="code") |> arrange(desc(score), code)
}

fscpsc_search_psc <- function(query, top_k=5, timeout_sec=25, max_retries=4){
  q <- as.character(query %||% ""); if (!nzchar(q)) return(tibble(code=character(),name=character(),score=double(),rank=integer()))
  attempt <- 0
  repeat{
    attempt <- attempt+1
    req <- request(FSCPSC_API) |>
      req_url_query(include="product-service-codes") |>
      req_headers("Accept"=FSCPSC_MIME,"Content-Type"=FSCPSC_MIME) |>
      req_body_json(list(data=list(type="searches", attributes=list(`search-string`=q))), auto_unbox=TRUE) |>
      req_timeout(timeout_sec)
    resp <- try(req_perform(req), silent=TRUE)
    if (inherits(resp,"httr2_response") && resp_status(resp) %in% c(200,201)){
      jj <- resp_body_json(resp, simplifyVector=FALSE)
      out <- parse_fscpsc_jsonapi(jj)
      if (nrow(out)) out <- out |> mutate(rank=row_number()) |> slice_head(n=top_k)
      return(out)
    }
    if (inherits(resp,"httr2_response") && resp_status(resp)==429){
      reset <- as.numeric(resp_header(resp,"X-RateLimit-Reset") %||% NA_real_)
      wait_s <- if (is.finite(reset) && reset>0 && reset<60) reset else min(2^(attempt-1),30)
      Sys.sleep(wait_s + runif(1,0,0.5))
    } else {
      if (attempt>=max_retries) return(tibble(code=character(),name=character(),score=double(),rank=integer()))
      Sys.sleep(min(2^(attempt-1),8)+runif(1,0,0.2))
    }
  }
}


chunk_indices <- function(n, chunk_n){ split(seq_len(n), ceiling(seq_len(n)/chunk_n)) }
safe_write_rds <- function(obj,path){ tmp <- paste0(path,".tmp_",sprintf("%06d",sample.int(1e6,1))); saveRDS(obj,tmp); file_move(tmp,path) }
safe_write_csv <- function(df,path){ tmp <- paste0(path,".tmp_",sprintf("%06d",sample.int(1e6,1))); write_csv(df,tmp); file_move(tmp,path) }


```


```{r}
run_pipeline_chunked <- function(df,
                                 key_cols = c("prime_id","sub_id","subaward_number"),
                                 subawardee_col="subawardee_name",
                                 expanded_col="expanded_description",
                                 idf_vocab,
                                 outdir = here("runs","psc"),
                                 tag = "full",
                                 chunk_n = 100000,
                                 n_workers = getOption("mc.cores",1),
                                 top_k = 5,
                                 batch_size = 60,
                                 pause_between = 2.0,
                                 per_call_sleep = 0.8,
                                 fetch_fn = fscpsc_search_psc){

  dir_create(c(file.path(outdir,"outputs"), file.path(outdir,"cache"), file.path(outdir,"snapshots")), recurse=TRUE)
  cache_path <- file.path(outdir,"cache", paste0("cache_",tag,".rds"))

  df_keys <- df %>% select(all_of(key_cols))
  if (any(!complete.cases(df_keys))) stop("key_cols contain NA; fix before running")
  if (n_distinct(df_keys %>% mutate(.id = do.call(paste, c(across(all_of(key_cols)), sep="|"))) %>% pull(.id)) != nrow(df)) stop("key_cols not unique; fix before running")

  df0 <- df %>%
    make_psc_keys_strict(key_cols = key_cols) %>%
    mutate(.row_ix = row_number())

  idx_list <- chunk_indices(nrow(df0), chunk_n)

  if (file_exists(cache_path)) {
    cache <- readRDS(cache_path)
    if (!all(c("psc_query_250",".pred") %in% names(cache))) cache <- tibble(psc_query_250=character(), .pred=list())
  } else {
    cache <- tibble(psc_query_250=character(), .pred=list())
  }

  for (ci in seq_along(idx_list)){
    ii <- idx_list[[ci]]
    chunk <- df0[ii, , drop=FALSE]

    build_one <- function(slice){
      make_psc_query_fast(slice,
                          subawardee_col=subawardee_col,
                          expanded_col=expanded_col,
                          max_chars=250,
                          idf_vocab=idf_vocab)
    }

    if (n_workers > 1L && length(ii) >= 10000L){
      splits <- chunk_indices(nrow(chunk), max(10000L, floor(length(ii)/n_workers)))
      parts <- mclapply(splits, function(jj) build_one(chunk[jj, , drop=FALSE]), mc.cores=n_workers)
      chunk_q <- bind_rows(parts)
    } else {
      chunk_q <- build_one(chunk)
    }

    chunk_q <- chunk_q %>% select(all_of(key_cols), .psc_key, psc_query_250, .row_ix)
    safe_write_rds(chunk_q, file.path(outdir,"outputs", sprintf("crosswalk_%s_%04d.rds", tag, ci)))

    uq <- chunk_q %>% distinct(psc_query_250) %>% filter(nzchar(psc_query_250))
    need <- uq %>% anti_join(cache, by="psc_query_250")
    if (nrow(need) > 0){
      qvec <- need$psc_query_250
      batches <- split(qvec, ceiling(seq_along(qvec)/batch_size))
      for (bi in seq_along(batches)){
        batch <- batches[[bi]]
        rows <- lapply(batch, function(q){ preds <- fetch_fn(q, top_k=top_k); if (per_call_sleep>0) Sys.sleep(per_call_sleep); tibble(psc_query_250=q, .pred=list(preds)) })
        cache <- bind_rows(cache, bind_rows(rows)) %>% distinct(psc_query_250, .keep_all=TRUE)
        safe_write_rds(cache, cache_path)
        if (pause_between>0 && bi<length(batches)) Sys.sleep(pause_between)
      }
    }

    preds_long <- chunk_q %>%
      left_join(cache, by="psc_query_250") %>%
      unnest(.pred, keep_empty=TRUE) %>%
      transmute(.psc_key, !!!syms(key_cols), psc_query_250, psc_code=code, psc_name=name, psc_score=score, rank)

    top1 <- preds_long %>%
      group_by(.psc_key) %>% arrange(rank, .by_group=TRUE) %>% slice_head(n=1) %>% ungroup()

    safe_write_rds(preds_long, file.path(outdir,"outputs", sprintf("preds_topk_%s_%04d.rds", tag, ci)))
    safe_write_rds(top1,      file.path(outdir,"outputs", sprintf("top1_%s_%04d.rds",      tag, ci)))
  }
}




```


```{r}

finalize_outputs <- function(outdir = here("runs","psc"), tag="full"){
  outs <- dir_ls(file.path(outdir,"outputs"), glob = sprintf("*_%s_*.rds", tag))
  crosswalk <- dir_ls(file.path(outdir,"outputs"), glob = sprintf("crosswalk_%s_*.rds", tag)) %>% map(readRDS) %>% bind_rows()
  preds     <- dir_ls(file.path(outdir,"outputs"), glob = sprintf("preds_topk_%s_*.rds", tag)) %>% map(readRDS) %>% bind_rows()
  top1      <- dir_ls(file.path(outdir,"outputs"), glob = sprintf("top1_%s_*.rds", tag)) %>% map(readRDS) %>% bind_rows()

  safe_write_rds(crosswalk, file.path(outdir,"outputs", sprintf("crosswalk_%s_all.rds", tag)))
  safe_write_rds(preds,     file.path(outdir,"outputs", sprintf("preds_topk_%s_all.rds", tag)))
  safe_write_rds(top1,      file.path(outdir,"outputs", sprintf("top1_%s_all.rds", tag)))

  invisible(list(crosswalk=crosswalk, preds_topk=preds, top1=top1))
}



```



```{r}

# key_cols <- c("prime_id","sub_id","subaward_number")
# idf_path <- here("Data","TF_IDF","idf_vocab.rds")
# if (!file_exists(idf_path)) {
#   idf_vocab <- build_global_idf(subcontracts$expanded_description)
#   dir_create(path_dir(idf_path), recurse=TRUE)
#   saveRDS(idf_vocab, idf_path)
# }
# idf_vocab <- readRDS(idf_path)
# 
# run_pipeline_chunked(
#   df           = subcontracts,
#   key_cols     = key_cols,
#   subawardee_col = "subawardee_name",
#   expanded_col = "expanded_description",
#   idf_vocab    = idf_vocab,
#   outdir       = here("runs","psc"),
#   tag          = "subset400",
#   chunk_n      = 100000,
#   n_workers    = getOption("mc.cores", 1),
#   top_k        = 5,
#   batch_size   = 60,
#   pause_between= 2.0,
#   per_call_sleep = 0.8,
#   fetch_fn     = fscpsc_search_psc
# )
# 
# finalize_outputs(outdir = here("runs","psc"), tag="subset400")



```

# run ex

```{r}

dupes <- subcontracts %>%
  count(prime_id_num) %>%
  filter(n > 1)

subset_df <- subcontracts %>% 
  slice(1:5000)

key_cols <- c("prime_id_num")
idf_vocab <- readRDS(here::here("Data","TF_IDF","idf_vocab.rds"))

run_pipeline_chunked(
  df             = subset_df,        
  key_cols       = key_cols,
  subawardee_col = "subawardee_name",
  expanded_col   = "expanded_description",
  idf_vocab      = idf_vocab,
  outdir         = here::here("runs","psc"),
  tag            = "pilot_subset",     # keeps files separate (crosswalk_*, preds_topk_*, top1_*)
  chunk_n        = 100000,             # can be smaller for tiny subsets; not critical
  n_workers      = 4,                  # parallel workers for query building
  top_k          = 5,
  batch_size     = 60,
  pause_between  = 2.0,
  per_call_sleep = 0.8,
  fetch_fn       = fscpsc_search_psc
)

finalize_outputs(outdir = here::here("runs","psc"), tag = "pilot_subset")



```
# ADD 

```{r}




```

